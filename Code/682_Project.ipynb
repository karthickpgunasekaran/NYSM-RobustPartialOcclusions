{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "682_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxm9Sg3-KUlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataloader file path\n",
        "import sys\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import DataLoader as dataloader\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader as tdataloader\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AKBGsquKnSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class  Controller: \n",
        "    def __init__(self):\n",
        "        # track loss and accuracies\n",
        "        self.num_epochs = 30\n",
        "        self.batch_size = 32\n",
        "        self.batch_size_train = 64\n",
        "        self.class_labels = 196  # Subject to change\n",
        "        self.loader_works = 1  # multi-process data loading,#loader worker processes.\n",
        "\n",
        "        self.images_dir = '/content/drive/My Drive/stanford_split/train_all/'\n",
        "        self.images_dir_test = '/content/drive/My Drive/stanford_split/test_all/'\n",
        "        self.store_dir = '/content/drive/My Drive/stanford_split/'\n",
        "        # images_dir='/content/drive/My Drive/25/train/'\n",
        "        # images_dir_test='/content/drive/My Drive/25/test/'\n",
        "        self.check_point_path = self.store_dir+'results/'\n",
        "        self.tr_loss_track, self.val_loss_track, self.tr_acc_track, self.val_acc_track, self.val_acc_history = [], [], [], [], []\n",
        "        self.USE_GPU = True\n",
        "        self.datasets = {}\n",
        "        self.dataloaders = {}\n",
        "        self.learning_rate = 0.01\n",
        "        self.optim_type = \"sgd\"\n",
        "        self.model_type = \"googlenet\"\n",
        "\n",
        "    def setDevice(self):\n",
        "        if self.USE_GPU and torch.cuda.is_available():\n",
        "                self.device = torch.device('cuda')\n",
        "        else:\n",
        "                self.device = torch.device('cpu')\n",
        "        print('device running on:', self.device)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    def checkpoint(self,model, best_loss, epoch, learning_rate):\n",
        "        state = {'model': model, 'best_loss': best_loss, 'epoch': epoch, 'rng_state': torch.get_rng_state(),\n",
        "             'LR': learning_rate}\n",
        "        torch.save(state, self.check_point_path+str(epoch)+\"model.pth\")\n",
        "\n",
        "\n",
        "    def loadmodel(self,model_name):\n",
        "        # load best model weights to return\n",
        "        checkpoint_best = torch.load(self.check_point_path+model_name)\n",
        "        model = checkpoint_best['model']\n",
        "        return model\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    def initialize_data(self):\n",
        "        mean = [0.5, 0.5, 0.5]\n",
        "        std = [0.5, 0.5, 0.5]\n",
        "\n",
        "        transform = {'train': transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)]),\n",
        "        'val': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)]),\n",
        "        'test': transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)])}\n",
        "        # Loading the dataset with train and val as keys\n",
        "        # Data divided in training and val using separate script\n",
        "        datasets = {}\n",
        "        datasets['train'] = dataloader.ImageDataSet(self.images_dir, fold='train', transformation=transform['train'])\n",
        "        datasets['val'] = dataloader.ImageDataSet(self.images_dir, fold='val', transformation=transform['val'])\n",
        "        datasets['test'] = dataloader.ImageDataSet(self.images_dir_test, fold='test', transformation=transform['test'])\n",
        "        print('datasets', len(datasets['train']))\n",
        "        print('datasets', len(datasets['val']))\n",
        "\n",
        "        # Dataloader.util\n",
        "        self.dataloaders['train'] = tdataloader(datasets['train'], batch_size=self.batch_size_train, shuffle=True, num_workers=self.loader_works)\n",
        "        self.dataloaders['val'] = tdataloader(datasets['val'], batch_size=self.batch_size, shuffle=True, num_workers=self.loader_works)\n",
        "        print('dataloaders train ', len(self.dataloaders['train']))\n",
        "        print('dataloaders valida', len(self.dataloaders['val']))\n",
        "\n",
        "\n",
        "\n",
        "        self.dataloaders['test'] = tdataloader(datasets['test'], batch_size=self.batch_size, shuffle=True, num_workers=self.loader_works)\n",
        "        #print('dataloaders', len(dataloaders))\n",
        "\n",
        "\n",
        "    def training(self):\n",
        "        # Defining pretrained model\n",
        "        # model = models.resnet50(pretrained=True)\n",
        "        if self.model_type==\"vgg19\":\n",
        "            model = models.vgg19(pretrained=True)\n",
        "            model.classifier[6] = nn.Linear(4096,self.class_labels)\n",
        "        elif self.model_type==\"resnet50\":\n",
        "            model = models.resnet50(pretrained=True)\n",
        "            num_features = model.fc.in_features\n",
        "            model.fc = nn.Linear(num_features, self.class_labels)\n",
        "        elif self.model_type ==\"googlenet\":\n",
        "            model = models.googlenet(pretrained=True)\n",
        "\n",
        "        model = model.to(self.device)\n",
        "\n",
        "        criterian = nn.CrossEntropyLoss()\n",
        "\n",
        "        if self.optim_type==\"adam\":\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        elif self.optim_type==\"sgd\":\n",
        "            optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.9)\n",
        "\n",
        "        # training\n",
        "        start_time = time.time()\n",
        "        best_loss = float('inf')\n",
        "        best_model = None\n",
        "        best_acc = 0.0\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        for epoch in range(1, self.num_epochs + 1):\n",
        "        #print('epoch {}/{}'.format(epoch, num_epochs))\n",
        "            for t in ['train', 'val']:\n",
        "                    num_correct = 0.0\n",
        "                    num_samples = 0\n",
        "                    r_loss = 0.0\n",
        "                    running_corrects = 0\n",
        "\n",
        "                    if t == 'train':\n",
        "                        # training mode\n",
        "                        model.train()\n",
        "                    else:\n",
        "                        # evaluate model\n",
        "                        model.eval()\n",
        "\n",
        "                    count = 0\n",
        "                    for data in self.dataloaders[t]:\n",
        "                        count += 1\n",
        "                        # data has three types files, labels and filename\n",
        "                        files, labels, filename = data\n",
        "\n",
        "                        files = Variable(files.to(self.device))  # to gpu or cpu\n",
        "                        labels = Variable(labels.to(self.device))\n",
        "\n",
        "                        optimizer.zero_grad()  # clearning old gradient from last step\n",
        "\n",
        "                        with torch.set_grad_enabled(t == 'train'):\n",
        "                            pred = model(files)\n",
        "                            # loss computation\n",
        "                            loss = criterian(pred, labels)\n",
        "\n",
        "                            _, prediction = torch.max(pred, 1)\n",
        "\n",
        "                            # backprop gradients at training time\n",
        "                            if t == 'train':\n",
        "                                loss.backward()\n",
        "                                optimizer.step()\n",
        "\n",
        "                            # print(t +' iteration {}:loss {}  '.format(count,r_loss))\n",
        "                        # statistics\n",
        "                        r_loss += loss.item() * files.size(0)\n",
        "                        print(t + ' iteration {}:loss {}  '.format(count, r_loss))\n",
        "                        running_corrects += torch.sum(prediction == labels.data)\n",
        "                    epoch_loss = r_loss / len(self.dataloaders[t].dataset)\n",
        "                    epoch_acc = running_corrects.double() / len(self.dataloaders[t].dataset)\n",
        "\n",
        "                    print('{} Loss: {:.4f} Acc: {:.4f}'.format(t, epoch_loss, epoch_acc))\n",
        "\n",
        "                    # print(t +' epoch {}:loss {}  '.format(epoch,r_loss))\n",
        "\n",
        "                    # deep copy the model\n",
        "                    if t == 'val' and epoch_acc > best_acc:\n",
        "                        best_acc = epoch_acc\n",
        "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                        self.checkpoint(best_model_wts, best_loss, epoch, self.learning_rate)\n",
        "                    if t == 'val':\n",
        "                        self.val_acc_history.append(epoch_acc.item())\n",
        "                        self.val_loss_track.append(epoch_loss)\n",
        "\n",
        "                    if t == 'train':\n",
        "                        self.tr_loss_track.append(epoch_loss)\n",
        "                        self.tr_acc_track.append(epoch_acc.item())\n",
        "\n",
        "\n",
        "\n",
        "        time_elapsed = time.time() - start_time\n",
        "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "        # load best model weights\n",
        "        model.load_state_dict(best_model_wts)\n",
        "        # updating best model in checkpoint\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self,model):\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "        count = 0\n",
        "        for data in self.dataloaders['test']:\n",
        "            count += 1\n",
        "            files, labels, filename = data\n",
        "            # load the data to GPU / CPU\n",
        "            image_data = Variable(files.to(self.device))\n",
        "            labels = Variable(labels.to(self.device))\n",
        "            output = model(image_data)\n",
        "            # print('output', output)\n",
        "            _, prediction = torch.max(output.data, 1)\n",
        "            # print('filename',filename,'\\nlabels',labels,'\\nprediction',prediction)\n",
        "            num_correct += torch.sum(prediction == labels)\n",
        "            num_samples += prediction.size(0)\n",
        "            print('Iteration', count)\n",
        "        acc = float(num_correct) / num_samples\n",
        "        print('Accuracy of test data :', acc)\n",
        "\n",
        "\n",
        "    def plot_losses_both(loss_track1,loss_track2,track1_label=\"Training Loss\",track2_label=\"Validation loss\",plt_label=\"Loss\"):\n",
        "        ax = plt.subplot(111)\n",
        "        plt.plot(loss_track1, c='b', label=track1_label)\n",
        "        plt.plot(loss_track2, c='g', label=track2_label)\n",
        "        plt.title(plt_label)\n",
        "        ax.legend()\n",
        "        plt.savefig(store_dir+\"train_val.png\")\n",
        "\n",
        "    def plot_loss_Accu(val_acc,xlab=\"Num epochs\",ylab=\"Validation accuracy\"):\n",
        "        plt.plot(val_acc, c='b')\n",
        "        plt.xlabel(xlab)\n",
        "        plt.ylabel(ylab)\n",
        "        plt.savefig(store_dir+\"val_acc.png\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaQcWwkbKpEl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "d7754a1f-c1b6-4ba3-a4bf-3b2e561b11c8"
      },
      "source": [
        "\n",
        "sys.path.insert(1, '/content/drive/My Drive/Colab Notebooks')\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "cont = Controller()\n",
        "cont.setDevice()\n",
        "cont.initialize_data()\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "device running on: cuda\n",
            "################# train #############\n",
            "#######################################\n",
            "################# val #############\n",
            "#######################################\n",
            "################# test #############\n",
            "#######################################\n",
            "datasets 6380\n",
            "datasets 1764\n",
            "dataloaders train  100\n",
            "dataloaders valida 56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r48fM_UyKwTs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cf8b0bfc-0e8f-4eb4-a754-a28708f2cf80"
      },
      "source": [
        "print('Training began....')\n",
        "best_model = cont.training()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training began....\n",
            "train iteration 1:loss 502.53350830078125  \n",
            "train iteration 2:loss 1011.4563903808594  \n",
            "train iteration 3:loss 1498.6766967773438  \n",
            "train iteration 4:loss 1993.1148986816406  \n",
            "train iteration 5:loss 2478.762481689453  \n",
            "train iteration 6:loss 2959.5226440429688  \n",
            "train iteration 7:loss 3441.8888549804688  \n",
            "train iteration 8:loss 3914.212615966797  \n",
            "train iteration 9:loss 4373.225189208984  \n",
            "train iteration 10:loss 4828.6495361328125  \n",
            "train iteration 11:loss 5268.524658203125  \n",
            "train iteration 12:loss 5709.688507080078  \n",
            "train iteration 13:loss 6144.218292236328  \n",
            "train iteration 14:loss 6567.330017089844  \n",
            "train iteration 15:loss 6987.752349853516  \n",
            "train iteration 16:loss 7400.547088623047  \n",
            "train iteration 17:loss 7820.063262939453  \n",
            "train iteration 18:loss 8221.10952758789  \n",
            "train iteration 19:loss 8621.29931640625  \n",
            "train iteration 20:loss 9022.073303222656  \n",
            "train iteration 21:loss 9425.836822509766  \n",
            "train iteration 22:loss 9832.518493652344  \n",
            "train iteration 23:loss 10226.520263671875  \n",
            "train iteration 24:loss 10622.548065185547  \n",
            "train iteration 25:loss 11004.930419921875  \n",
            "train iteration 26:loss 11382.77914428711  \n",
            "train iteration 27:loss 11751.42416381836  \n",
            "train iteration 28:loss 12125.506164550781  \n",
            "train iteration 29:loss 12496.066162109375  \n",
            "train iteration 30:loss 12872.105499267578  \n",
            "train iteration 31:loss 13251.211364746094  \n",
            "train iteration 32:loss 13624.728118896484  \n",
            "train iteration 33:loss 14007.34683227539  \n",
            "train iteration 34:loss 14370.830810546875  \n",
            "train iteration 35:loss 14732.778930664062  \n",
            "train iteration 36:loss 15102.23373413086  \n",
            "train iteration 37:loss 15472.48062133789  \n",
            "train iteration 38:loss 15825.498168945312  \n",
            "train iteration 39:loss 16171.58364868164  \n",
            "train iteration 40:loss 16525.032775878906  \n",
            "train iteration 41:loss 16875.49545288086  \n",
            "train iteration 42:loss 17238.345703125  \n",
            "train iteration 43:loss 17574.43133544922  \n",
            "train iteration 44:loss 17915.60140991211  \n",
            "train iteration 45:loss 18266.32699584961  \n",
            "train iteration 46:loss 18603.638702392578  \n",
            "train iteration 47:loss 18941.59487915039  \n",
            "train iteration 48:loss 19284.007049560547  \n",
            "train iteration 49:loss 19612.96533203125  \n",
            "train iteration 50:loss 19947.523223876953  \n",
            "train iteration 51:loss 20283.14776611328  \n",
            "train iteration 52:loss 20606.644165039062  \n",
            "train iteration 53:loss 20930.888793945312  \n",
            "train iteration 54:loss 21280.940216064453  \n",
            "train iteration 55:loss 21612.303314208984  \n",
            "train iteration 56:loss 21940.515502929688  \n",
            "train iteration 57:loss 22264.529235839844  \n",
            "train iteration 58:loss 22584.3876953125  \n",
            "train iteration 59:loss 22903.10855102539  \n",
            "train iteration 60:loss 23210.33218383789  \n",
            "train iteration 61:loss 23518.664031982422  \n",
            "train iteration 62:loss 23830.312591552734  \n",
            "train iteration 63:loss 24131.944915771484  \n",
            "train iteration 64:loss 24450.08090209961  \n",
            "train iteration 65:loss 24744.54656982422  \n",
            "train iteration 66:loss 25062.327178955078  \n",
            "train iteration 67:loss 25370.47186279297  \n",
            "train iteration 68:loss 25665.923278808594  \n",
            "train iteration 69:loss 25970.935180664062  \n",
            "train iteration 70:loss 26268.150787353516  \n",
            "train iteration 71:loss 26575.868225097656  \n",
            "train iteration 72:loss 26868.265258789062  \n",
            "train iteration 73:loss 27174.177947998047  \n",
            "train iteration 74:loss 27463.319854736328  \n",
            "train iteration 75:loss 27749.801544189453  \n",
            "train iteration 76:loss 28065.67855834961  \n",
            "train iteration 77:loss 28335.31121826172  \n",
            "train iteration 78:loss 28625.649658203125  \n",
            "train iteration 79:loss 28900.607177734375  \n",
            "train iteration 80:loss 29200.690643310547  \n",
            "train iteration 81:loss 29480.151397705078  \n",
            "train iteration 82:loss 29752.38134765625  \n",
            "train iteration 83:loss 30045.44464111328  \n",
            "train iteration 84:loss 30327.95233154297  \n",
            "train iteration 85:loss 30601.606170654297  \n",
            "train iteration 86:loss 30861.279388427734  \n",
            "train iteration 87:loss 31138.344116210938  \n",
            "train iteration 88:loss 31386.977081298828  \n",
            "train iteration 89:loss 31650.44644165039  \n",
            "train iteration 90:loss 31936.931884765625  \n",
            "train iteration 91:loss 32223.539794921875  \n",
            "train iteration 92:loss 32485.630340576172  \n",
            "train iteration 93:loss 32756.22329711914  \n",
            "train iteration 94:loss 33022.71273803711  \n",
            "train iteration 95:loss 33306.39489746094  \n",
            "train iteration 96:loss 33575.80490112305  \n",
            "train iteration 97:loss 33825.17984008789  \n",
            "train iteration 98:loss 34082.290130615234  \n",
            "train iteration 99:loss 34334.05838012695  \n",
            "train iteration 100:loss 34503.26629638672  \n",
            "train Loss: 5.4080 Acc: 0.0534\n",
            "val iteration 1:loss 127.1467056274414  \n",
            "val iteration 2:loss 239.36994171142578  \n",
            "val iteration 3:loss 370.5833053588867  \n",
            "val iteration 4:loss 505.9343032836914  \n",
            "val iteration 5:loss 635.4796829223633  \n",
            "val iteration 6:loss 763.3596572875977  \n",
            "val iteration 7:loss 891.374137878418  \n",
            "val iteration 8:loss 1035.5060501098633  \n",
            "val iteration 9:loss 1160.7008743286133  \n",
            "val iteration 10:loss 1287.0981369018555  \n",
            "val iteration 11:loss 1410.518424987793  \n",
            "val iteration 12:loss 1535.4064483642578  \n",
            "val iteration 13:loss 1671.4927215576172  \n",
            "val iteration 14:loss 1812.4027252197266  \n",
            "val iteration 15:loss 1943.1115112304688  \n",
            "val iteration 16:loss 2060.786491394043  \n",
            "val iteration 17:loss 2194.3134384155273  \n",
            "val iteration 18:loss 2318.6079177856445  \n",
            "val iteration 19:loss 2447.2517623901367  \n",
            "val iteration 20:loss 2579.5976333618164  \n",
            "val iteration 21:loss 2709.439292907715  \n",
            "val iteration 22:loss 2834.298843383789  \n",
            "val iteration 23:loss 2967.2462005615234  \n",
            "val iteration 24:loss 3097.3464965820312  \n",
            "val iteration 25:loss 3215.0568618774414  \n",
            "val iteration 26:loss 3344.6337509155273  \n",
            "val iteration 27:loss 3485.6555709838867  \n",
            "val iteration 28:loss 3624.974723815918  \n",
            "val iteration 29:loss 3745.2563552856445  \n",
            "val iteration 30:loss 3860.650718688965  \n",
            "val iteration 31:loss 3998.21524810791  \n",
            "val iteration 32:loss 4114.14501953125  \n",
            "val iteration 33:loss 4256.268478393555  \n",
            "val iteration 34:loss 4403.476974487305  \n",
            "val iteration 35:loss 4516.893882751465  \n",
            "val iteration 36:loss 4640.5489501953125  \n",
            "val iteration 37:loss 4762.252799987793  \n",
            "val iteration 38:loss 4892.237358093262  \n",
            "val iteration 39:loss 5020.452415466309  \n",
            "val iteration 40:loss 5148.320503234863  \n",
            "val iteration 41:loss 5276.736228942871  \n",
            "val iteration 42:loss 5409.233268737793  \n",
            "val iteration 43:loss 5551.266166687012  \n",
            "val iteration 44:loss 5682.8537673950195  \n",
            "val iteration 45:loss 5803.544731140137  \n",
            "val iteration 46:loss 5924.68586730957  \n",
            "val iteration 47:loss 6042.721298217773  \n",
            "val iteration 48:loss 6180.596237182617  \n",
            "val iteration 49:loss 6308.153282165527  \n",
            "val iteration 50:loss 6433.144104003906  \n",
            "val iteration 51:loss 6575.302734375  \n",
            "val iteration 52:loss 6703.3280029296875  \n",
            "val iteration 53:loss 6823.781425476074  \n",
            "val iteration 54:loss 6962.270149230957  \n",
            "val iteration 55:loss 7091.96883392334  \n",
            "val iteration 56:loss 7108.760623931885  \n",
            "val Loss: 4.0299 Acc: 0.1293\n",
            "train iteration 1:loss 214.32411193847656  \n",
            "train iteration 2:loss 431.815673828125  \n",
            "train iteration 3:loss 659.6800994873047  \n",
            "train iteration 4:loss 847.2919006347656  \n",
            "train iteration 5:loss 1053.5803833007812  \n",
            "train iteration 6:loss 1257.055435180664  \n",
            "train iteration 7:loss 1455.155746459961  \n",
            "train iteration 8:loss 1696.1355743408203  \n",
            "train iteration 9:loss 1899.3009643554688  \n",
            "train iteration 10:loss 2124.8636016845703  \n",
            "train iteration 11:loss 2346.5497131347656  \n",
            "train iteration 12:loss 2569.472702026367  \n",
            "train iteration 13:loss 2750.0444946289062  \n",
            "train iteration 14:loss 2959.9034423828125  \n",
            "train iteration 15:loss 3153.575454711914  \n",
            "train iteration 16:loss 3359.2417755126953  \n",
            "train iteration 17:loss 3550.332229614258  \n",
            "train iteration 18:loss 3756.4063262939453  \n",
            "train iteration 19:loss 3956.9588165283203  \n",
            "train iteration 20:loss 4164.301345825195  \n",
            "train iteration 21:loss 4371.949478149414  \n",
            "train iteration 22:loss 4579.206878662109  \n",
            "train iteration 23:loss 4789.194839477539  \n",
            "train iteration 24:loss 4976.301452636719  \n",
            "train iteration 25:loss 5175.908157348633  \n",
            "train iteration 26:loss 5378.078109741211  \n",
            "train iteration 27:loss 5584.339111328125  \n",
            "train iteration 28:loss 5773.523239135742  \n",
            "train iteration 29:loss 5958.307098388672  \n",
            "train iteration 30:loss 6152.272155761719  \n",
            "train iteration 31:loss 6343.205871582031  \n",
            "train iteration 32:loss 6544.188751220703  \n",
            "train iteration 33:loss 6710.048980712891  \n",
            "train iteration 34:loss 6905.956100463867  \n",
            "train iteration 35:loss 7111.867202758789  \n",
            "train iteration 36:loss 7303.6651611328125  \n",
            "train iteration 37:loss 7496.566146850586  \n",
            "train iteration 38:loss 7692.889236450195  \n",
            "train iteration 39:loss 7871.057479858398  \n",
            "train iteration 40:loss 8057.416458129883  \n",
            "train iteration 41:loss 8246.714126586914  \n",
            "train iteration 42:loss 8428.577941894531  \n",
            "train iteration 43:loss 8609.806121826172  \n",
            "train iteration 44:loss 8798.858993530273  \n",
            "train iteration 45:loss 8968.643005371094  \n",
            "train iteration 46:loss 9142.26040649414  \n",
            "train iteration 47:loss 9326.33903503418  \n",
            "train iteration 48:loss 9498.901550292969  \n",
            "train iteration 49:loss 9679.273956298828  \n",
            "train iteration 50:loss 9874.06462097168  \n",
            "train iteration 51:loss 10069.374603271484  \n",
            "train iteration 52:loss 10243.305786132812  \n",
            "train iteration 53:loss 10419.554397583008  \n",
            "train iteration 54:loss 10577.319869995117  \n",
            "train iteration 55:loss 10745.613067626953  \n",
            "train iteration 56:loss 10924.245193481445  \n",
            "train iteration 57:loss 11077.799835205078  \n",
            "train iteration 58:loss 11259.01513671875  \n",
            "train iteration 59:loss 11425.723022460938  \n",
            "train iteration 60:loss 11611.553253173828  \n",
            "train iteration 61:loss 11794.952194213867  \n",
            "train iteration 62:loss 11970.371322631836  \n",
            "train iteration 63:loss 12145.781600952148  \n",
            "train iteration 64:loss 12297.179016113281  \n",
            "train iteration 65:loss 12479.908584594727  \n",
            "train iteration 66:loss 12621.226577758789  \n",
            "train iteration 67:loss 12788.40348815918  \n",
            "train iteration 68:loss 12960.709457397461  \n",
            "train iteration 69:loss 13112.533599853516  \n",
            "train iteration 70:loss 13274.332611083984  \n",
            "train iteration 71:loss 13445.550186157227  \n",
            "train iteration 72:loss 13612.972030639648  \n",
            "train iteration 73:loss 13751.103378295898  \n",
            "train iteration 74:loss 13924.649780273438  \n",
            "train iteration 75:loss 14104.41342163086  \n",
            "train iteration 76:loss 14248.527572631836  \n",
            "train iteration 77:loss 14388.114639282227  \n",
            "train iteration 78:loss 14562.126342773438  \n",
            "train iteration 79:loss 14718.982391357422  \n",
            "train iteration 80:loss 14883.639892578125  \n",
            "train iteration 81:loss 15056.386611938477  \n",
            "train iteration 82:loss 15193.808532714844  \n",
            "train iteration 83:loss 15372.076263427734  \n",
            "train iteration 84:loss 15526.147918701172  \n",
            "train iteration 85:loss 15695.217315673828  \n",
            "train iteration 86:loss 15831.070831298828  \n",
            "train iteration 87:loss 15972.108520507812  \n",
            "train iteration 88:loss 16125.504043579102  \n",
            "train iteration 89:loss 16277.919692993164  \n",
            "train iteration 90:loss 16404.580711364746  \n",
            "train iteration 91:loss 16565.383598327637  \n",
            "train iteration 92:loss 16712.75005340576  \n",
            "train iteration 93:loss 16855.016746520996  \n",
            "train iteration 94:loss 17006.089073181152  \n",
            "train iteration 95:loss 17172.603172302246  \n",
            "train iteration 96:loss 17316.569190979004  \n",
            "train iteration 97:loss 17465.776191711426  \n",
            "train iteration 98:loss 17603.489738464355  \n",
            "train iteration 99:loss 17733.248695373535  \n",
            "train iteration 100:loss 17848.155210494995  \n",
            "train Loss: 2.7975 Acc: 0.3602\n",
            "val iteration 1:loss 97.98085021972656  \n",
            "val iteration 2:loss 179.47952270507812  \n",
            "val iteration 3:loss 266.500732421875  \n",
            "val iteration 4:loss 353.5272521972656  \n",
            "val iteration 5:loss 443.60870361328125  \n",
            "val iteration 6:loss 535.7674026489258  \n",
            "val iteration 7:loss 607.1700744628906  \n",
            "val iteration 8:loss 706.0602493286133  \n",
            "val iteration 9:loss 796.4045715332031  \n",
            "val iteration 10:loss 904.3395233154297  \n",
            "val iteration 11:loss 982.4800567626953  \n",
            "val iteration 12:loss 1067.427848815918  \n",
            "val iteration 13:loss 1161.2041702270508  \n",
            "val iteration 14:loss 1253.646842956543  \n",
            "val iteration 15:loss 1340.6488494873047  \n",
            "val iteration 16:loss 1443.259017944336  \n",
            "val iteration 17:loss 1513.969985961914  \n",
            "val iteration 18:loss 1595.5522155761719  \n",
            "val iteration 19:loss 1675.708122253418  \n",
            "val iteration 20:loss 1767.161018371582  \n",
            "val iteration 21:loss 1834.1123275756836  \n",
            "val iteration 22:loss 1898.9011535644531  \n",
            "val iteration 23:loss 1988.0067749023438  \n",
            "val iteration 24:loss 2064.013397216797  \n",
            "val iteration 25:loss 2171.1539001464844  \n",
            "val iteration 26:loss 2245.896774291992  \n",
            "val iteration 27:loss 2323.270462036133  \n",
            "val iteration 28:loss 2396.757843017578  \n",
            "val iteration 29:loss 2465.782814025879  \n",
            "val iteration 30:loss 2552.400230407715  \n",
            "val iteration 31:loss 2636.177589416504  \n",
            "val iteration 32:loss 2717.890312194824  \n",
            "val iteration 33:loss 2805.691764831543  \n",
            "val iteration 34:loss 2889.1755981445312  \n",
            "val iteration 35:loss 2973.9487075805664  \n",
            "val iteration 36:loss 3052.867546081543  \n",
            "val iteration 37:loss 3139.9975509643555  \n",
            "val iteration 38:loss 3224.2792434692383  \n",
            "val iteration 39:loss 3307.263641357422  \n",
            "val iteration 40:loss 3392.1054458618164  \n",
            "val iteration 41:loss 3469.992401123047  \n",
            "val iteration 42:loss 3561.7305068969727  \n",
            "val iteration 43:loss 3639.3021850585938  \n",
            "val iteration 44:loss 3720.2530670166016  \n",
            "val iteration 45:loss 3789.241523742676  \n",
            "val iteration 46:loss 3866.8988647460938  \n",
            "val iteration 47:loss 3963.2383880615234  \n",
            "val iteration 48:loss 4030.725028991699  \n",
            "val iteration 49:loss 4109.245620727539  \n",
            "val iteration 50:loss 4193.494522094727  \n",
            "val iteration 51:loss 4269.801063537598  \n",
            "val iteration 52:loss 4364.577278137207  \n",
            "val iteration 53:loss 4449.869850158691  \n",
            "val iteration 54:loss 4537.895233154297  \n",
            "val iteration 55:loss 4620.707962036133  \n",
            "val iteration 56:loss 4631.515205383301  \n",
            "val Loss: 2.6256 Acc: 0.3696\n",
            "train iteration 1:loss 100.4289321899414  \n",
            "train iteration 2:loss 215.22684478759766  \n",
            "train iteration 3:loss 330.78182220458984  \n",
            "train iteration 4:loss 437.41273498535156  \n",
            "train iteration 5:loss 537.9146728515625  \n",
            "train iteration 6:loss 645.4378814697266  \n",
            "train iteration 7:loss 748.1870727539062  \n",
            "train iteration 8:loss 843.1096649169922  \n",
            "train iteration 9:loss 945.3324661254883  \n",
            "train iteration 10:loss 1056.5237350463867  \n",
            "train iteration 11:loss 1168.2859344482422  \n",
            "train iteration 12:loss 1271.4279022216797  \n",
            "train iteration 13:loss 1368.3204956054688  \n",
            "train iteration 14:loss 1472.7570114135742  \n",
            "train iteration 15:loss 1575.253791809082  \n",
            "train iteration 16:loss 1670.7425384521484  \n",
            "train iteration 17:loss 1778.4680862426758  \n",
            "train iteration 18:loss 1876.2384414672852  \n",
            "train iteration 19:loss 1961.495994567871  \n",
            "train iteration 20:loss 2053.4166259765625  \n",
            "train iteration 21:loss 2156.9276123046875  \n",
            "train iteration 22:loss 2246.5761642456055  \n",
            "train iteration 23:loss 2345.4625396728516  \n",
            "train iteration 24:loss 2446.309425354004  \n",
            "train iteration 25:loss 2534.763511657715  \n",
            "train iteration 26:loss 2628.514533996582  \n",
            "train iteration 27:loss 2728.353645324707  \n",
            "train iteration 28:loss 2828.2619857788086  \n",
            "train iteration 29:loss 2918.1777725219727  \n",
            "train iteration 30:loss 3030.0739517211914  \n",
            "train iteration 31:loss 3107.626091003418  \n",
            "train iteration 32:loss 3190.855827331543  \n",
            "train iteration 33:loss 3275.6433639526367  \n",
            "train iteration 34:loss 3367.908233642578  \n",
            "train iteration 35:loss 3448.28572845459  \n",
            "train iteration 36:loss 3534.249313354492  \n",
            "train iteration 37:loss 3630.197639465332  \n",
            "train iteration 38:loss 3728.4773559570312  \n",
            "train iteration 39:loss 3824.5990829467773  \n",
            "train iteration 40:loss 3929.4726943969727  \n",
            "train iteration 41:loss 4032.580146789551  \n",
            "train iteration 42:loss 4131.936347961426  \n",
            "train iteration 43:loss 4227.465431213379  \n",
            "train iteration 44:loss 4338.386489868164  \n",
            "train iteration 45:loss 4421.597747802734  \n",
            "train iteration 46:loss 4504.457458496094  \n",
            "train iteration 47:loss 4593.684967041016  \n",
            "train iteration 48:loss 4675.130554199219  \n",
            "train iteration 49:loss 4756.043434143066  \n",
            "train iteration 50:loss 4852.441581726074  \n",
            "train iteration 51:loss 4936.332221984863  \n",
            "train iteration 52:loss 5011.946723937988  \n",
            "train iteration 53:loss 5092.45637512207  \n",
            "train iteration 54:loss 5170.420593261719  \n",
            "train iteration 55:loss 5260.829566955566  \n",
            "train iteration 56:loss 5357.303009033203  \n",
            "train iteration 57:loss 5456.932106018066  \n",
            "train iteration 58:loss 5558.865394592285  \n",
            "train iteration 59:loss 5631.432472229004  \n",
            "train iteration 60:loss 5732.478469848633  \n",
            "train iteration 61:loss 5815.14714050293  \n",
            "train iteration 62:loss 5893.164321899414  \n",
            "train iteration 63:loss 5988.787879943848  \n",
            "train iteration 64:loss 6081.91251373291  \n",
            "train iteration 65:loss 6184.359443664551  \n",
            "train iteration 66:loss 6265.761901855469  \n",
            "train iteration 67:loss 6337.828971862793  \n",
            "train iteration 68:loss 6424.9467697143555  \n",
            "train iteration 69:loss 6519.594657897949  \n",
            "train iteration 70:loss 6620.786056518555  \n",
            "train iteration 71:loss 6706.851959228516  \n",
            "train iteration 72:loss 6807.543029785156  \n",
            "train iteration 73:loss 6884.241790771484  \n",
            "train iteration 74:loss 6985.4437255859375  \n",
            "train iteration 75:loss 7080.802581787109  \n",
            "train iteration 76:loss 7160.423812866211  \n",
            "train iteration 77:loss 7252.288902282715  \n",
            "train iteration 78:loss 7355.545974731445  \n",
            "train iteration 79:loss 7438.621925354004  \n",
            "train iteration 80:loss 7524.952880859375  \n",
            "train iteration 81:loss 7606.380996704102  \n",
            "train iteration 82:loss 7688.377754211426  \n",
            "train iteration 83:loss 7776.356056213379  \n",
            "train iteration 84:loss 7867.355377197266  \n",
            "train iteration 85:loss 7943.1354904174805  \n",
            "train iteration 86:loss 8029.436073303223  \n",
            "train iteration 87:loss 8090.502410888672  \n",
            "train iteration 88:loss 8176.900367736816  \n",
            "train iteration 89:loss 8253.938346862793  \n",
            "train iteration 90:loss 8320.484855651855  \n",
            "train iteration 91:loss 8398.080917358398  \n",
            "train iteration 92:loss 8477.998252868652  \n",
            "train iteration 93:loss 8560.880180358887  \n",
            "train iteration 94:loss 8654.259803771973  \n",
            "train iteration 95:loss 8749.317085266113  \n",
            "train iteration 96:loss 8823.719230651855  \n",
            "train iteration 97:loss 8918.767616271973  \n",
            "train iteration 98:loss 9006.054214477539  \n",
            "train iteration 99:loss 9086.881965637207  \n",
            "train iteration 100:loss 9155.91982793808  \n",
            "train Loss: 1.4351 Acc: 0.6614\n",
            "val iteration 1:loss 67.22209167480469  \n",
            "val iteration 2:loss 142.83887481689453  \n",
            "val iteration 3:loss 215.9579620361328  \n",
            "val iteration 4:loss 279.8910827636719  \n",
            "val iteration 5:loss 338.9155807495117  \n",
            "val iteration 6:loss 407.6507110595703  \n",
            "val iteration 7:loss 467.5323486328125  \n",
            "val iteration 8:loss 536.7361907958984  \n",
            "val iteration 9:loss 610.3226318359375  \n",
            "val iteration 10:loss 676.8699951171875  \n",
            "val iteration 11:loss 735.335205078125  \n",
            "val iteration 12:loss 789.5600929260254  \n",
            "val iteration 13:loss 847.494083404541  \n",
            "val iteration 14:loss 926.5402946472168  \n",
            "val iteration 15:loss 992.3805122375488  \n",
            "val iteration 16:loss 1059.130702972412  \n",
            "val iteration 17:loss 1118.4926795959473  \n",
            "val iteration 18:loss 1187.2269096374512  \n",
            "val iteration 19:loss 1251.8985481262207  \n",
            "val iteration 20:loss 1318.3083305358887  \n",
            "val iteration 21:loss 1382.356990814209  \n",
            "val iteration 22:loss 1449.8690910339355  \n",
            "val iteration 23:loss 1522.7650032043457  \n",
            "val iteration 24:loss 1594.7487831115723  \n",
            "val iteration 25:loss 1653.8034019470215  \n",
            "val iteration 26:loss 1727.4301948547363  \n",
            "val iteration 27:loss 1788.4255752563477  \n",
            "val iteration 28:loss 1868.775535583496  \n",
            "val iteration 29:loss 1946.1121215820312  \n",
            "val iteration 30:loss 2004.1268043518066  \n",
            "val iteration 31:loss 2081.3707008361816  \n",
            "val iteration 32:loss 2154.8601722717285  \n",
            "val iteration 33:loss 2225.1654167175293  \n",
            "val iteration 34:loss 2311.278995513916  \n",
            "val iteration 35:loss 2388.804843902588  \n",
            "val iteration 36:loss 2448.424087524414  \n",
            "val iteration 37:loss 2517.127685546875  \n",
            "val iteration 38:loss 2588.3863983154297  \n",
            "val iteration 39:loss 2664.074966430664  \n",
            "val iteration 40:loss 2729.7639770507812  \n",
            "val iteration 41:loss 2796.9316024780273  \n",
            "val iteration 42:loss 2865.792549133301  \n",
            "val iteration 43:loss 2939.9614181518555  \n",
            "val iteration 44:loss 3017.6812438964844  \n",
            "val iteration 45:loss 3079.7901878356934  \n",
            "val iteration 46:loss 3154.1010704040527  \n",
            "val iteration 47:loss 3234.0135459899902  \n",
            "val iteration 48:loss 3304.5363960266113  \n",
            "val iteration 49:loss 3356.0412788391113  \n",
            "val iteration 50:loss 3416.3531379699707  \n",
            "val iteration 51:loss 3469.0101737976074  \n",
            "val iteration 52:loss 3530.3700942993164  \n",
            "val iteration 53:loss 3600.361640930176  \n",
            "val iteration 54:loss 3666.1529998779297  \n",
            "val iteration 55:loss 3746.8895263671875  \n",
            "val iteration 56:loss 3758.616991043091  \n",
            "val Loss: 2.1307 Acc: 0.4603\n",
            "train iteration 1:loss 47.93336486816406  \n",
            "train iteration 2:loss 93.34994888305664  \n",
            "train iteration 3:loss 136.9928741455078  \n",
            "train iteration 4:loss 180.92858123779297  \n",
            "train iteration 5:loss 242.02639770507812  \n",
            "train iteration 6:loss 301.6204071044922  \n",
            "train iteration 7:loss 371.7061767578125  \n",
            "train iteration 8:loss 430.09735107421875  \n",
            "train iteration 9:loss 475.9248046875  \n",
            "train iteration 10:loss 526.522388458252  \n",
            "train iteration 11:loss 576.3949356079102  \n",
            "train iteration 12:loss 636.2005195617676  \n",
            "train iteration 13:loss 692.8655700683594  \n",
            "train iteration 14:loss 737.7231330871582  \n",
            "train iteration 15:loss 797.8019218444824  \n",
            "train iteration 16:loss 849.3589553833008  \n",
            "train iteration 17:loss 891.8267517089844  \n",
            "train iteration 18:loss 943.6451721191406  \n",
            "train iteration 19:loss 985.2293930053711  \n",
            "train iteration 20:loss 1023.5035057067871  \n",
            "train iteration 21:loss 1076.4107055664062  \n",
            "train iteration 22:loss 1119.6130332946777  \n",
            "train iteration 23:loss 1168.6301651000977  \n",
            "train iteration 24:loss 1223.2079849243164  \n",
            "train iteration 25:loss 1283.0031967163086  \n",
            "train iteration 26:loss 1332.476547241211  \n",
            "train iteration 27:loss 1394.027946472168  \n",
            "train iteration 28:loss 1437.255039215088  \n",
            "train iteration 29:loss 1493.441333770752  \n",
            "train iteration 30:loss 1544.6079292297363  \n",
            "train iteration 31:loss 1586.9987449645996  \n",
            "train iteration 32:loss 1637.9661865234375  \n",
            "train iteration 33:loss 1695.486759185791  \n",
            "train iteration 34:loss 1748.589900970459  \n",
            "train iteration 35:loss 1798.478988647461  \n",
            "train iteration 36:loss 1851.6511535644531  \n",
            "train iteration 37:loss 1907.1002578735352  \n",
            "train iteration 38:loss 1939.4465866088867  \n",
            "train iteration 39:loss 1986.0322151184082  \n",
            "train iteration 40:loss 2042.9504737854004  \n",
            "train iteration 41:loss 2079.07914352417  \n",
            "train iteration 42:loss 2127.316764831543  \n",
            "train iteration 43:loss 2181.3736686706543  \n",
            "train iteration 44:loss 2234.008888244629  \n",
            "train iteration 45:loss 2280.8015174865723  \n",
            "train iteration 46:loss 2340.077136993408  \n",
            "train iteration 47:loss 2398.6313285827637  \n",
            "train iteration 48:loss 2449.7450065612793  \n",
            "train iteration 49:loss 2506.329662322998  \n",
            "train iteration 50:loss 2549.6002082824707  \n",
            "train iteration 51:loss 2598.1582984924316  \n",
            "train iteration 52:loss 2644.77783203125  \n",
            "train iteration 53:loss 2685.244129180908  \n",
            "train iteration 54:loss 2739.0615425109863  \n",
            "train iteration 55:loss 2778.3228912353516  \n",
            "train iteration 56:loss 2830.2255210876465  \n",
            "train iteration 57:loss 2871.496654510498  \n",
            "train iteration 58:loss 2915.161159515381  \n",
            "train iteration 59:loss 2953.3337173461914  \n",
            "train iteration 60:loss 3003.3994522094727  \n",
            "train iteration 61:loss 3050.951717376709  \n",
            "train iteration 62:loss 3089.7467498779297  \n",
            "train iteration 63:loss 3145.4136543273926  \n",
            "train iteration 64:loss 3191.1429176330566  \n",
            "train iteration 65:loss 3244.451858520508  \n",
            "train iteration 66:loss 3293.3713455200195  \n",
            "train iteration 67:loss 3338.7313194274902  \n",
            "train iteration 68:loss 3384.863872528076  \n",
            "train iteration 69:loss 3438.6709747314453  \n",
            "train iteration 70:loss 3478.68514251709  \n",
            "train iteration 71:loss 3525.7886848449707  \n",
            "train iteration 72:loss 3567.4056396484375  \n",
            "train iteration 73:loss 3614.3775634765625  \n",
            "train iteration 74:loss 3650.8467330932617  \n",
            "train iteration 75:loss 3693.8332901000977  \n",
            "train iteration 76:loss 3742.1770057678223  \n",
            "train iteration 77:loss 3793.4759559631348  \n",
            "train iteration 78:loss 3842.2815856933594  \n",
            "train iteration 79:loss 3882.388729095459  \n",
            "train iteration 80:loss 3939.948215484619  \n",
            "train iteration 81:loss 3992.615451812744  \n",
            "train iteration 82:loss 4045.3702354431152  \n",
            "train iteration 83:loss 4100.973133087158  \n",
            "train iteration 84:loss 4153.405860900879  \n",
            "train iteration 85:loss 4210.881744384766  \n",
            "train iteration 86:loss 4254.062759399414  \n",
            "train iteration 87:loss 4296.640609741211  \n",
            "train iteration 88:loss 4348.473388671875  \n",
            "train iteration 89:loss 4404.417900085449  \n",
            "train iteration 90:loss 4450.791694641113  \n",
            "train iteration 91:loss 4514.410526275635  \n",
            "train iteration 92:loss 4554.429347991943  \n",
            "train iteration 93:loss 4621.5249671936035  \n",
            "train iteration 94:loss 4665.543422698975  \n",
            "train iteration 95:loss 4724.092029571533  \n",
            "train iteration 96:loss 4770.876747131348  \n",
            "train iteration 97:loss 4830.427585601807  \n",
            "train iteration 98:loss 4874.929870605469  \n",
            "train iteration 99:loss 4920.300666809082  \n",
            "train iteration 100:loss 4952.615524530411  \n",
            "train Loss: 0.7763 Acc: 0.8193\n",
            "val iteration 1:loss 56.78166580200195  \n",
            "val iteration 2:loss 115.51033401489258  \n",
            "val iteration 3:loss 171.81271743774414  \n",
            "val iteration 4:loss 225.63095474243164  \n",
            "val iteration 5:loss 275.92418670654297  \n",
            "val iteration 6:loss 323.78579330444336  \n",
            "val iteration 7:loss 376.02573013305664  \n",
            "val iteration 8:loss 414.93974685668945  \n",
            "val iteration 9:loss 459.28760528564453  \n",
            "val iteration 10:loss 518.9913177490234  \n",
            "val iteration 11:loss 568.3705520629883  \n",
            "val iteration 12:loss 616.2902870178223  \n",
            "val iteration 13:loss 675.3197441101074  \n",
            "val iteration 14:loss 715.5615196228027  \n",
            "val iteration 15:loss 763.871395111084  \n",
            "val iteration 16:loss 821.5217628479004  \n",
            "val iteration 17:loss 874.0627098083496  \n",
            "val iteration 18:loss 922.8522987365723  \n",
            "val iteration 19:loss 972.0700759887695  \n",
            "val iteration 20:loss 1015.8938026428223  \n",
            "val iteration 21:loss 1077.5620574951172  \n",
            "val iteration 22:loss 1127.3272323608398  \n",
            "val iteration 23:loss 1172.2948150634766  \n",
            "val iteration 24:loss 1207.6330947875977  \n",
            "val iteration 25:loss 1239.587947845459  \n",
            "val iteration 26:loss 1303.7294883728027  \n",
            "val iteration 27:loss 1351.6884269714355  \n",
            "val iteration 28:loss 1395.910488128662  \n",
            "val iteration 29:loss 1438.6834182739258  \n",
            "val iteration 30:loss 1484.3498191833496  \n",
            "val iteration 31:loss 1535.9836807250977  \n",
            "val iteration 32:loss 1570.5994186401367  \n",
            "val iteration 33:loss 1628.5498161315918  \n",
            "val iteration 34:loss 1665.1111526489258  \n",
            "val iteration 35:loss 1717.4333305358887  \n",
            "val iteration 36:loss 1761.759609222412  \n",
            "val iteration 37:loss 1812.2421035766602  \n",
            "val iteration 38:loss 1845.5351638793945  \n",
            "val iteration 39:loss 1886.4039726257324  \n",
            "val iteration 40:loss 1937.1777229309082  \n",
            "val iteration 41:loss 2001.3208045959473  \n",
            "val iteration 42:loss 2062.261875152588  \n",
            "val iteration 43:loss 2112.829502105713  \n",
            "val iteration 44:loss 2165.9990463256836  \n",
            "val iteration 45:loss 2225.9225997924805  \n",
            "val iteration 46:loss 2282.876796722412  \n",
            "val iteration 47:loss 2341.2851753234863  \n",
            "val iteration 48:loss 2388.2647743225098  \n",
            "val iteration 49:loss 2433.788372039795  \n",
            "val iteration 50:loss 2504.4074211120605  \n",
            "val iteration 51:loss 2563.523277282715  \n",
            "val iteration 52:loss 2626.1993141174316  \n",
            "val iteration 53:loss 2665.8468132019043  \n",
            "val iteration 54:loss 2714.8200340270996  \n",
            "val iteration 55:loss 2752.6889572143555  \n",
            "val iteration 56:loss 2759.0162949562073  \n",
            "val Loss: 1.5641 Acc: 0.5964\n",
            "train iteration 1:loss 34.06086349487305  \n",
            "train iteration 2:loss 70.30378341674805  \n",
            "train iteration 3:loss 92.86314010620117  \n",
            "train iteration 4:loss 122.42814636230469  \n",
            "train iteration 5:loss 145.03686714172363  \n",
            "train iteration 6:loss 173.84447288513184  \n",
            "train iteration 7:loss 196.8222255706787  \n",
            "train iteration 8:loss 222.28643989562988  \n",
            "train iteration 9:loss 256.7243175506592  \n",
            "train iteration 10:loss 284.14561653137207  \n",
            "train iteration 11:loss 310.89696884155273  \n",
            "train iteration 12:loss 341.4268569946289  \n",
            "train iteration 13:loss 369.91773986816406  \n",
            "train iteration 14:loss 395.88978004455566  \n",
            "train iteration 15:loss 420.6510238647461  \n",
            "train iteration 16:loss 449.4169921875  \n",
            "train iteration 17:loss 474.29314613342285  \n",
            "train iteration 18:loss 500.6602077484131  \n",
            "train iteration 19:loss 520.3294429779053  \n",
            "train iteration 20:loss 547.5896663665771  \n",
            "train iteration 21:loss 578.8432388305664  \n",
            "train iteration 22:loss 606.4900779724121  \n",
            "train iteration 23:loss 632.8536071777344  \n",
            "train iteration 24:loss 662.2950191497803  \n",
            "train iteration 25:loss 690.973237991333  \n",
            "train iteration 26:loss 716.2918949127197  \n",
            "train iteration 27:loss 746.7851448059082  \n",
            "train iteration 28:loss 765.1983890533447  \n",
            "train iteration 29:loss 804.6934642791748  \n",
            "train iteration 30:loss 829.90380859375  \n",
            "train iteration 31:loss 853.0997352600098  \n",
            "train iteration 32:loss 877.0828437805176  \n",
            "train iteration 33:loss 897.874080657959  \n",
            "train iteration 34:loss 926.6694793701172  \n",
            "train iteration 35:loss 953.9662933349609  \n",
            "train iteration 36:loss 976.9479866027832  \n",
            "train iteration 37:loss 1002.8019428253174  \n",
            "train iteration 38:loss 1031.4194736480713  \n",
            "train iteration 39:loss 1055.1159400939941  \n",
            "train iteration 40:loss 1081.7188987731934  \n",
            "train iteration 41:loss 1114.7474899291992  \n",
            "train iteration 42:loss 1135.3193626403809  \n",
            "train iteration 43:loss 1174.6444282531738  \n",
            "train iteration 44:loss 1193.5369243621826  \n",
            "train iteration 45:loss 1224.9330463409424  \n",
            "train iteration 46:loss 1248.9666080474854  \n",
            "train iteration 47:loss 1274.0198574066162  \n",
            "train iteration 48:loss 1303.78129196167  \n",
            "train iteration 49:loss 1328.241891860962  \n",
            "train iteration 50:loss 1358.7201232910156  \n",
            "train iteration 51:loss 1382.4099197387695  \n",
            "train iteration 52:loss 1413.286937713623  \n",
            "train iteration 53:loss 1444.0180625915527  \n",
            "train iteration 54:loss 1472.5941638946533  \n",
            "train iteration 55:loss 1496.362024307251  \n",
            "train iteration 56:loss 1520.5042400360107  \n",
            "train iteration 57:loss 1555.1050701141357  \n",
            "train iteration 58:loss 1575.2562656402588  \n",
            "train iteration 59:loss 1602.3632144927979  \n",
            "train iteration 60:loss 1626.92897605896  \n",
            "train iteration 61:loss 1657.2601432800293  \n",
            "train iteration 62:loss 1680.9009609222412  \n",
            "train iteration 63:loss 1702.2840213775635  \n",
            "train iteration 64:loss 1724.9163494110107  \n",
            "train iteration 65:loss 1745.6701736450195  \n",
            "train iteration 66:loss 1766.2198104858398  \n",
            "train iteration 67:loss 1792.4789600372314  \n",
            "train iteration 68:loss 1814.6579113006592  \n",
            "train iteration 69:loss 1841.7999439239502  \n",
            "train iteration 70:loss 1873.5764408111572  \n",
            "train iteration 71:loss 1900.4175071716309  \n",
            "train iteration 72:loss 1918.9852180480957  \n",
            "train iteration 73:loss 1941.244716644287  \n",
            "train iteration 74:loss 1962.8455696105957  \n",
            "train iteration 75:loss 1985.1499862670898  \n",
            "train iteration 76:loss 2014.2962837219238  \n",
            "train iteration 77:loss 2028.432448387146  \n",
            "train iteration 78:loss 2047.9166631698608  \n",
            "train iteration 79:loss 2068.4875020980835  \n",
            "train iteration 80:loss 2088.2862272262573  \n",
            "train iteration 81:loss 2116.06063747406  \n",
            "train iteration 82:loss 2140.842677116394  \n",
            "train iteration 83:loss 2162.2859411239624  \n",
            "train iteration 84:loss 2185.7811250686646  \n",
            "train iteration 85:loss 2225.032872200012  \n",
            "train iteration 86:loss 2248.5653219223022  \n",
            "train iteration 87:loss 2279.031355857849  \n",
            "train iteration 88:loss 2314.3909406661987  \n",
            "train iteration 89:loss 2333.4706983566284  \n",
            "train iteration 90:loss 2362.492777824402  \n",
            "train iteration 91:loss 2380.949927330017  \n",
            "train iteration 92:loss 2411.790690422058  \n",
            "train iteration 93:loss 2436.2663068771362  \n",
            "train iteration 94:loss 2457.6835432052612  \n",
            "train iteration 95:loss 2493.2109518051147  \n",
            "train iteration 96:loss 2517.226809501648  \n",
            "train iteration 97:loss 2545.5902433395386  \n",
            "train iteration 98:loss 2570.841673851013  \n",
            "train iteration 99:loss 2599.782057762146  \n",
            "train iteration 100:loss 2613.815500974655  \n",
            "train Loss: 0.4097 Acc: 0.9230\n",
            "val iteration 1:loss 47.558319091796875  \n",
            "val iteration 2:loss 85.33689498901367  \n",
            "val iteration 3:loss 128.90557479858398  \n",
            "val iteration 4:loss 178.65163803100586  \n",
            "val iteration 5:loss 226.20689392089844  \n",
            "val iteration 6:loss 274.55467987060547  \n",
            "val iteration 7:loss 309.4985580444336  \n",
            "val iteration 8:loss 347.70399475097656  \n",
            "val iteration 9:loss 398.5577163696289  \n",
            "val iteration 10:loss 443.6921920776367  \n",
            "val iteration 11:loss 494.7953071594238  \n",
            "val iteration 12:loss 539.7903099060059  \n",
            "val iteration 13:loss 575.9608573913574  \n",
            "val iteration 14:loss 636.7756881713867  \n",
            "val iteration 15:loss 678.9914703369141  \n",
            "val iteration 16:loss 726.2039566040039  \n",
            "val iteration 17:loss 763.1567878723145  \n",
            "val iteration 18:loss 794.4128379821777  \n",
            "val iteration 19:loss 838.1889419555664  \n",
            "val iteration 20:loss 897.7101516723633  \n",
            "val iteration 21:loss 936.3913879394531  \n",
            "val iteration 22:loss 985.9852447509766  \n",
            "val iteration 23:loss 1025.370994567871  \n",
            "val iteration 24:loss 1062.827377319336  \n",
            "val iteration 25:loss 1115.2672653198242  \n",
            "val iteration 26:loss 1164.6373405456543  \n",
            "val iteration 27:loss 1207.6883735656738  \n",
            "val iteration 28:loss 1259.2715797424316  \n",
            "val iteration 29:loss 1313.4239234924316  \n",
            "val iteration 30:loss 1354.4877166748047  \n",
            "val iteration 31:loss 1402.4898719787598  \n",
            "val iteration 32:loss 1451.456974029541  \n",
            "val iteration 33:loss 1491.6876029968262  \n",
            "val iteration 34:loss 1537.6038513183594  \n",
            "val iteration 35:loss 1573.095947265625  \n",
            "val iteration 36:loss 1629.601806640625  \n",
            "val iteration 37:loss 1682.7304878234863  \n",
            "val iteration 38:loss 1730.3489112854004  \n",
            "val iteration 39:loss 1776.8454627990723  \n",
            "val iteration 40:loss 1827.7226257324219  \n",
            "val iteration 41:loss 1878.33540725708  \n",
            "val iteration 42:loss 1914.1014709472656  \n",
            "val iteration 43:loss 1967.8395690917969  \n",
            "val iteration 44:loss 2004.529224395752  \n",
            "val iteration 45:loss 2045.7949409484863  \n",
            "val iteration 46:loss 2091.6986351013184  \n",
            "val iteration 47:loss 2139.0947761535645  \n",
            "val iteration 48:loss 2174.8992080688477  \n",
            "val iteration 49:loss 2209.014190673828  \n",
            "val iteration 50:loss 2264.7777519226074  \n",
            "val iteration 51:loss 2309.9311180114746  \n",
            "val iteration 52:loss 2366.5944023132324  \n",
            "val iteration 53:loss 2421.4383125305176  \n",
            "val iteration 54:loss 2456.615566253662  \n",
            "val iteration 55:loss 2521.9127769470215  \n",
            "val iteration 56:loss 2525.877727508545  \n",
            "val Loss: 1.4319 Acc: 0.6417\n",
            "train iteration 1:loss 17.493511199951172  \n",
            "train iteration 2:loss 32.26551532745361  \n",
            "train iteration 3:loss 45.31546878814697  \n",
            "train iteration 4:loss 58.19171905517578  \n",
            "train iteration 5:loss 68.2260103225708  \n",
            "train iteration 6:loss 83.87994956970215  \n",
            "train iteration 7:loss 97.46106052398682  \n",
            "train iteration 8:loss 111.98989582061768  \n",
            "train iteration 9:loss 125.3386583328247  \n",
            "train iteration 10:loss 143.06837558746338  \n",
            "train iteration 11:loss 161.31568050384521  \n",
            "train iteration 12:loss 177.76863384246826  \n",
            "train iteration 13:loss 193.87366390228271  \n",
            "train iteration 14:loss 213.66995906829834  \n",
            "train iteration 15:loss 238.24967098236084  \n",
            "train iteration 16:loss 257.0166540145874  \n",
            "train iteration 17:loss 267.94847106933594  \n",
            "train iteration 18:loss 280.98942375183105  \n",
            "train iteration 19:loss 305.849796295166  \n",
            "train iteration 20:loss 317.84795665740967  \n",
            "train iteration 21:loss 334.0936613082886  \n",
            "train iteration 22:loss 348.6875009536743  \n",
            "train iteration 23:loss 363.77597999572754  \n",
            "train iteration 24:loss 388.83522605895996  \n",
            "train iteration 25:loss 407.928674697876  \n",
            "train iteration 26:loss 420.3944606781006  \n",
            "train iteration 27:loss 435.4785432815552  \n",
            "train iteration 28:loss 452.2046728134155  \n",
            "train iteration 29:loss 471.0064916610718  \n",
            "train iteration 30:loss 492.06746196746826  \n",
            "train iteration 31:loss 501.6693115234375  \n",
            "train iteration 32:loss 514.2354393005371  \n",
            "train iteration 33:loss 532.3028945922852  \n",
            "train iteration 34:loss 553.4791641235352  \n",
            "train iteration 35:loss 568.4157009124756  \n",
            "train iteration 36:loss 582.0268239974976  \n",
            "train iteration 37:loss 598.9501523971558  \n",
            "train iteration 38:loss 618.9116697311401  \n",
            "train iteration 39:loss 630.037971496582  \n",
            "train iteration 40:loss 643.6481018066406  \n",
            "train iteration 41:loss 654.2654733657837  \n",
            "train iteration 42:loss 668.2231035232544  \n",
            "train iteration 43:loss 684.1570062637329  \n",
            "train iteration 44:loss 698.232587814331  \n",
            "train iteration 45:loss 711.8174619674683  \n",
            "train iteration 46:loss 729.4599256515503  \n",
            "train iteration 47:loss 747.8460645675659  \n",
            "train iteration 48:loss 761.3749980926514  \n",
            "train iteration 49:loss 772.3241777420044  \n",
            "train iteration 50:loss 789.0853300094604  \n",
            "train iteration 51:loss 804.3440761566162  \n",
            "train iteration 52:loss 821.7155704498291  \n",
            "train iteration 53:loss 839.2944583892822  \n",
            "train iteration 54:loss 853.3156270980835  \n",
            "train iteration 55:loss 872.6051244735718  \n",
            "train iteration 56:loss 884.1177749633789  \n",
            "train iteration 57:loss 897.5563697814941  \n",
            "train iteration 58:loss 915.9761047363281  \n",
            "train iteration 59:loss 927.3001289367676  \n",
            "train iteration 60:loss 941.6742496490479  \n",
            "train iteration 61:loss 948.4869565963745  \n",
            "train iteration 62:loss 967.5504179000854  \n",
            "train iteration 63:loss 984.4981832504272  \n",
            "train iteration 64:loss 991.5208072662354  \n",
            "train iteration 65:loss 1004.8647556304932  \n",
            "train iteration 66:loss 1020.3272066116333  \n",
            "train iteration 67:loss 1037.5142908096313  \n",
            "train iteration 68:loss 1046.6848096847534  \n",
            "train iteration 69:loss 1064.5574598312378  \n",
            "train iteration 70:loss 1078.8428792953491  \n",
            "train iteration 71:loss 1091.2559928894043  \n",
            "train iteration 72:loss 1107.1220512390137  \n",
            "train iteration 73:loss 1127.030330657959  \n",
            "train iteration 74:loss 1139.717523574829  \n",
            "train iteration 75:loss 1152.9715185165405  \n",
            "train iteration 76:loss 1168.3107833862305  \n",
            "train iteration 77:loss 1182.7213344573975  \n",
            "train iteration 78:loss 1191.4985466003418  \n",
            "train iteration 79:loss 1205.3516693115234  \n",
            "train iteration 80:loss 1220.327642440796  \n",
            "train iteration 81:loss 1234.993685722351  \n",
            "train iteration 82:loss 1247.9553289413452  \n",
            "train iteration 83:loss 1260.1978273391724  \n",
            "train iteration 84:loss 1280.7479085922241  \n",
            "train iteration 85:loss 1290.8997812271118  \n",
            "train iteration 86:loss 1301.1748495101929  \n",
            "train iteration 87:loss 1313.3792057037354  \n",
            "train iteration 88:loss 1326.875672340393  \n",
            "train iteration 89:loss 1340.6570253372192  \n",
            "train iteration 90:loss 1355.566819190979  \n",
            "train iteration 91:loss 1370.6655473709106  \n",
            "train iteration 92:loss 1381.1636323928833  \n",
            "train iteration 93:loss 1390.7468633651733  \n",
            "train iteration 94:loss 1405.4602451324463  \n",
            "train iteration 95:loss 1422.0985202789307  \n",
            "train iteration 96:loss 1435.3325309753418  \n",
            "train iteration 97:loss 1450.4753198623657  \n",
            "train iteration 98:loss 1460.7483386993408  \n",
            "train iteration 99:loss 1471.5597858428955  \n",
            "train iteration 100:loss 1480.5448408722878  \n",
            "train Loss: 0.2321 Acc: 0.9683\n",
            "val iteration 1:loss 32.78972244262695  \n",
            "val iteration 2:loss 79.27947616577148  \n",
            "val iteration 3:loss 117.55569076538086  \n",
            "val iteration 4:loss 146.27914428710938  \n",
            "val iteration 5:loss 185.560546875  \n",
            "val iteration 6:loss 241.8631477355957  \n",
            "val iteration 7:loss 288.1332778930664  \n",
            "val iteration 8:loss 325.310546875  \n",
            "val iteration 9:loss 370.8161735534668  \n",
            "val iteration 10:loss 413.3696403503418  \n",
            "val iteration 11:loss 454.6035270690918  \n",
            "val iteration 12:loss 479.36885833740234  \n",
            "val iteration 13:loss 515.8865242004395  \n",
            "val iteration 14:loss 562.2394676208496  \n",
            "val iteration 15:loss 606.671516418457  \n",
            "val iteration 16:loss 637.9620819091797  \n",
            "val iteration 17:loss 676.1594429016113  \n",
            "val iteration 18:loss 708.3448295593262  \n",
            "val iteration 19:loss 744.017463684082  \n",
            "val iteration 20:loss 782.7255897521973  \n",
            "val iteration 21:loss 828.108211517334  \n",
            "val iteration 22:loss 861.4880905151367  \n",
            "val iteration 23:loss 897.2374420166016  \n",
            "val iteration 24:loss 939.533576965332  \n",
            "val iteration 25:loss 979.117561340332  \n",
            "val iteration 26:loss 1038.2624435424805  \n",
            "val iteration 27:loss 1081.2127838134766  \n",
            "val iteration 28:loss 1100.4520282745361  \n",
            "val iteration 29:loss 1147.9764156341553  \n",
            "val iteration 30:loss 1185.5776348114014  \n",
            "val iteration 31:loss 1237.4173908233643  \n",
            "val iteration 32:loss 1290.011281967163  \n",
            "val iteration 33:loss 1332.0679683685303  \n",
            "val iteration 34:loss 1385.2923412322998  \n",
            "val iteration 35:loss 1408.7982902526855  \n",
            "val iteration 36:loss 1443.5486907958984  \n",
            "val iteration 37:loss 1465.4631004333496  \n",
            "val iteration 38:loss 1495.909351348877  \n",
            "val iteration 39:loss 1542.3179779052734  \n",
            "val iteration 40:loss 1587.02779006958  \n",
            "val iteration 41:loss 1623.737419128418  \n",
            "val iteration 42:loss 1663.3735656738281  \n",
            "val iteration 43:loss 1713.8964653015137  \n",
            "val iteration 44:loss 1744.3925075531006  \n",
            "val iteration 45:loss 1773.596824645996  \n",
            "val iteration 46:loss 1809.5090446472168  \n",
            "val iteration 47:loss 1847.7819938659668  \n",
            "val iteration 48:loss 1889.5242958068848  \n",
            "val iteration 49:loss 1937.5967979431152  \n",
            "val iteration 50:loss 1981.974910736084  \n",
            "val iteration 51:loss 2025.346622467041  \n",
            "val iteration 52:loss 2065.5592918395996  \n",
            "val iteration 53:loss 2104.1170234680176  \n",
            "val iteration 54:loss 2123.330066680908  \n",
            "val iteration 55:loss 2162.474452972412  \n",
            "val iteration 56:loss 2164.1328229904175  \n",
            "val Loss: 1.2268 Acc: 0.7001\n",
            "train iteration 1:loss 9.882573127746582  \n",
            "train iteration 2:loss 17.728768348693848  \n",
            "train iteration 3:loss 29.01077365875244  \n",
            "train iteration 4:loss 37.32218933105469  \n",
            "train iteration 5:loss 44.09354782104492  \n",
            "train iteration 6:loss 51.25562763214111  \n",
            "train iteration 7:loss 57.8388090133667  \n",
            "train iteration 8:loss 65.90422916412354  \n",
            "train iteration 9:loss 75.92690181732178  \n",
            "train iteration 10:loss 85.25262260437012  \n",
            "train iteration 11:loss 96.17617511749268  \n",
            "train iteration 12:loss 106.73981285095215  \n",
            "train iteration 13:loss 112.1476993560791  \n",
            "train iteration 14:loss 119.7012414932251  \n",
            "train iteration 15:loss 127.769118309021  \n",
            "train iteration 16:loss 134.9427671432495  \n",
            "train iteration 17:loss 141.11792659759521  \n",
            "train iteration 18:loss 150.19910717010498  \n",
            "train iteration 19:loss 156.85328197479248  \n",
            "train iteration 20:loss 165.6009340286255  \n",
            "train iteration 21:loss 173.3253870010376  \n",
            "train iteration 22:loss 183.1010332107544  \n",
            "train iteration 23:loss 189.99116230010986  \n",
            "train iteration 24:loss 199.412823677063  \n",
            "train iteration 25:loss 210.13592147827148  \n",
            "train iteration 26:loss 219.97003173828125  \n",
            "train iteration 27:loss 227.20351314544678  \n",
            "train iteration 28:loss 233.87521743774414  \n",
            "train iteration 29:loss 241.77878761291504  \n",
            "train iteration 30:loss 252.23459243774414  \n",
            "train iteration 31:loss 256.47373390197754  \n",
            "train iteration 32:loss 268.56412506103516  \n",
            "train iteration 33:loss 277.994589805603  \n",
            "train iteration 34:loss 290.1764440536499  \n",
            "train iteration 35:loss 299.9078073501587  \n",
            "train iteration 36:loss 306.75365924835205  \n",
            "train iteration 37:loss 312.0504446029663  \n",
            "train iteration 38:loss 319.5453681945801  \n",
            "train iteration 39:loss 326.4344983100891  \n",
            "train iteration 40:loss 336.71102476119995  \n",
            "train iteration 41:loss 344.76511907577515  \n",
            "train iteration 42:loss 350.83244943618774  \n",
            "train iteration 43:loss 357.80793046951294  \n",
            "train iteration 44:loss 363.51337003707886  \n",
            "train iteration 45:loss 370.6611828804016  \n",
            "train iteration 46:loss 376.90910291671753  \n",
            "train iteration 47:loss 385.2732491493225  \n",
            "train iteration 48:loss 394.64646196365356  \n",
            "train iteration 49:loss 399.6599555015564  \n",
            "train iteration 50:loss 413.4029583930969  \n",
            "train iteration 51:loss 424.88610792160034  \n",
            "train iteration 52:loss 431.04678201675415  \n",
            "train iteration 53:loss 442.7126317024231  \n",
            "train iteration 54:loss 448.93983125686646  \n",
            "train iteration 55:loss 461.9840598106384  \n",
            "train iteration 56:loss 471.0775475502014  \n",
            "train iteration 57:loss 479.0799593925476  \n",
            "train iteration 58:loss 486.2540383338928  \n",
            "train iteration 59:loss 491.01392221450806  \n",
            "train iteration 60:loss 500.1959881782532  \n",
            "train iteration 61:loss 511.04148626327515  \n",
            "train iteration 62:loss 516.2017893791199  \n",
            "train iteration 63:loss 527.0243334770203  \n",
            "train iteration 64:loss 531.8415608406067  \n",
            "train iteration 65:loss 538.5140433311462  \n",
            "train iteration 66:loss 547.5190653800964  \n",
            "train iteration 67:loss 552.6429886817932  \n",
            "train iteration 68:loss 558.7218012809753  \n",
            "train iteration 69:loss 565.5472054481506  \n",
            "train iteration 70:loss 576.9198594093323  \n",
            "train iteration 71:loss 589.0051922798157  \n",
            "train iteration 72:loss 597.2275109291077  \n",
            "train iteration 73:loss 608.1851687431335  \n",
            "train iteration 74:loss 615.3915114402771  \n",
            "train iteration 75:loss 621.1751036643982  \n",
            "train iteration 76:loss 626.0304341316223  \n",
            "train iteration 77:loss 635.6745409965515  \n",
            "train iteration 78:loss 640.6252646446228  \n",
            "train iteration 79:loss 649.851725101471  \n",
            "train iteration 80:loss 657.3713059425354  \n",
            "train iteration 81:loss 665.6496472358704  \n",
            "train iteration 82:loss 674.9404149055481  \n",
            "train iteration 83:loss 685.967954158783  \n",
            "train iteration 84:loss 692.739248752594  \n",
            "train iteration 85:loss 700.6167044639587  \n",
            "train iteration 86:loss 712.2088150978088  \n",
            "train iteration 87:loss 724.5011219978333  \n",
            "train iteration 88:loss 733.3572363853455  \n",
            "train iteration 89:loss 740.801992893219  \n",
            "train iteration 90:loss 746.2101197242737  \n",
            "train iteration 91:loss 755.5096316337585  \n",
            "train iteration 92:loss 764.4202418327332  \n",
            "train iteration 93:loss 773.9720149040222  \n",
            "train iteration 94:loss 782.3249802589417  \n",
            "train iteration 95:loss 788.3955750465393  \n",
            "train iteration 96:loss 795.8541951179504  \n",
            "train iteration 97:loss 804.3692011833191  \n",
            "train iteration 98:loss 812.7445559501648  \n",
            "train iteration 99:loss 827.149516582489  \n",
            "train iteration 100:loss 839.3038244247437  \n",
            "train Loss: 0.1316 Acc: 0.9842\n",
            "val iteration 1:loss 28.557186126708984  \n",
            "val iteration 2:loss 56.858421325683594  \n",
            "val iteration 3:loss 97.13386535644531  \n",
            "val iteration 4:loss 140.25342559814453  \n",
            "val iteration 5:loss 181.81475830078125  \n",
            "val iteration 6:loss 220.4666290283203  \n",
            "val iteration 7:loss 246.58041954040527  \n",
            "val iteration 8:loss 288.3839054107666  \n",
            "val iteration 9:loss 338.12191581726074  \n",
            "val iteration 10:loss 381.62074851989746  \n",
            "val iteration 11:loss 407.0980701446533  \n",
            "val iteration 12:loss 442.9052333831787  \n",
            "val iteration 13:loss 493.27783393859863  \n",
            "val iteration 14:loss 527.2067737579346  \n",
            "val iteration 15:loss 555.3838520050049  \n",
            "val iteration 16:loss 580.4580364227295  \n",
            "val iteration 17:loss 611.1000652313232  \n",
            "val iteration 18:loss 652.5045299530029  \n",
            "val iteration 19:loss 697.7223949432373  \n",
            "val iteration 20:loss 744.2350788116455  \n",
            "val iteration 21:loss 798.3246631622314  \n",
            "val iteration 22:loss 827.9086055755615  \n",
            "val iteration 23:loss 863.5799922943115  \n",
            "val iteration 24:loss 889.1977443695068  \n",
            "val iteration 25:loss 957.5394039154053  \n",
            "val iteration 26:loss 994.8841915130615  \n",
            "val iteration 27:loss 1029.990400314331  \n",
            "val iteration 28:loss 1049.853708267212  \n",
            "val iteration 29:loss 1089.3035068511963  \n",
            "val iteration 30:loss 1132.8805561065674  \n",
            "val iteration 31:loss 1164.6950778961182  \n",
            "val iteration 32:loss 1203.8629131317139  \n",
            "val iteration 33:loss 1252.653600692749  \n",
            "val iteration 34:loss 1310.9366207122803  \n",
            "val iteration 35:loss 1340.8182735443115  \n",
            "val iteration 36:loss 1380.9408473968506  \n",
            "val iteration 37:loss 1417.6737728118896  \n",
            "val iteration 38:loss 1446.3778247833252  \n",
            "val iteration 39:loss 1499.0745372772217  \n",
            "val iteration 40:loss 1539.4732837677002  \n",
            "val iteration 41:loss 1571.0992050170898  \n",
            "val iteration 42:loss 1615.5409965515137  \n",
            "val iteration 43:loss 1664.0535202026367  \n",
            "val iteration 44:loss 1699.4064102172852  \n",
            "val iteration 45:loss 1741.4298057556152  \n",
            "val iteration 46:loss 1778.8459358215332  \n",
            "val iteration 47:loss 1801.5431785583496  \n",
            "val iteration 48:loss 1833.6087608337402  \n",
            "val iteration 49:loss 1865.8964309692383  \n",
            "val iteration 50:loss 1921.5322799682617  \n",
            "val iteration 51:loss 1957.7982749938965  \n",
            "val iteration 52:loss 1991.3372077941895  \n",
            "val iteration 53:loss 2037.3421516418457  \n",
            "val iteration 54:loss 2079.386962890625  \n",
            "val iteration 55:loss 2125.6017684936523  \n",
            "val iteration 56:loss 2127.180284500122  \n",
            "val Loss: 1.2059 Acc: 0.6961\n",
            "train iteration 1:loss 5.851133346557617  \n",
            "train iteration 2:loss 10.562908172607422  \n",
            "train iteration 3:loss 15.900343894958496  \n",
            "train iteration 4:loss 19.489428520202637  \n",
            "train iteration 5:loss 25.459306716918945  \n",
            "train iteration 6:loss 30.670166015625  \n",
            "train iteration 7:loss 34.786932945251465  \n",
            "train iteration 8:loss 41.80014133453369  \n",
            "train iteration 9:loss 45.3135404586792  \n",
            "train iteration 10:loss 50.98495578765869  \n",
            "train iteration 11:loss 55.97800636291504  \n",
            "train iteration 12:loss 64.26790809631348  \n",
            "train iteration 13:loss 69.92267894744873  \n",
            "train iteration 14:loss 73.42556571960449  \n",
            "train iteration 15:loss 79.16621494293213  \n",
            "train iteration 16:loss 83.4822416305542  \n",
            "train iteration 17:loss 87.38073348999023  \n",
            "train iteration 18:loss 92.68404293060303  \n",
            "train iteration 19:loss 98.01836585998535  \n",
            "train iteration 20:loss 105.70377731323242  \n",
            "train iteration 21:loss 110.15833854675293  \n",
            "train iteration 22:loss 115.39100360870361  \n",
            "train iteration 23:loss 122.40626525878906  \n",
            "train iteration 24:loss 127.25636005401611  \n",
            "train iteration 25:loss 132.0204849243164  \n",
            "train iteration 26:loss 142.0792932510376  \n",
            "train iteration 27:loss 149.6842851638794  \n",
            "train iteration 28:loss 153.9698543548584  \n",
            "train iteration 29:loss 159.1381778717041  \n",
            "train iteration 30:loss 162.8439483642578  \n",
            "train iteration 31:loss 170.3886432647705  \n",
            "train iteration 32:loss 176.8095579147339  \n",
            "train iteration 33:loss 182.74939727783203  \n",
            "train iteration 34:loss 190.7328462600708  \n",
            "train iteration 35:loss 195.25115299224854  \n",
            "train iteration 36:loss 201.96125316619873  \n",
            "train iteration 37:loss 208.9027442932129  \n",
            "train iteration 38:loss 211.8935022354126  \n",
            "train iteration 39:loss 218.0168228149414  \n",
            "train iteration 40:loss 222.56451892852783  \n",
            "train iteration 41:loss 227.9285020828247  \n",
            "train iteration 42:loss 239.14988613128662  \n",
            "train iteration 43:loss 243.76352882385254  \n",
            "train iteration 44:loss 247.83842945098877  \n",
            "train iteration 45:loss 251.5297555923462  \n",
            "train iteration 46:loss 257.0076274871826  \n",
            "train iteration 47:loss 263.71501636505127  \n",
            "train iteration 48:loss 275.76690101623535  \n",
            "train iteration 49:loss 280.4049530029297  \n",
            "train iteration 50:loss 284.01661682128906  \n",
            "train iteration 51:loss 289.5182418823242  \n",
            "train iteration 52:loss 292.6018409729004  \n",
            "train iteration 53:loss 297.22978115081787  \n",
            "train iteration 54:loss 301.1239700317383  \n",
            "train iteration 55:loss 305.51933002471924  \n",
            "train iteration 56:loss 312.5775318145752  \n",
            "train iteration 57:loss 316.08040142059326  \n",
            "train iteration 58:loss 320.17659282684326  \n",
            "train iteration 59:loss 327.76140213012695  \n",
            "train iteration 60:loss 338.20421600341797  \n",
            "train iteration 61:loss 345.3763189315796  \n",
            "train iteration 62:loss 349.69386291503906  \n",
            "train iteration 63:loss 354.92601108551025  \n",
            "train iteration 64:loss 359.3428792953491  \n",
            "train iteration 65:loss 364.48453521728516  \n",
            "train iteration 66:loss 369.59356594085693  \n",
            "train iteration 67:loss 374.54248046875  \n",
            "train iteration 68:loss 378.53326892852783  \n",
            "train iteration 69:loss 384.41310119628906  \n",
            "train iteration 70:loss 388.0508918762207  \n",
            "train iteration 71:loss 392.86306381225586  \n",
            "train iteration 72:loss 397.42499828338623  \n",
            "train iteration 73:loss 404.0235986709595  \n",
            "train iteration 74:loss 409.47388076782227  \n",
            "train iteration 75:loss 415.7327356338501  \n",
            "train iteration 76:loss 424.2513246536255  \n",
            "train iteration 77:loss 428.28954887390137  \n",
            "train iteration 78:loss 432.17169094085693  \n",
            "train iteration 79:loss 438.9648246765137  \n",
            "train iteration 80:loss 441.1048460006714  \n",
            "train iteration 81:loss 450.10197830200195  \n",
            "train iteration 82:loss 453.5291357040405  \n",
            "train iteration 83:loss 458.10159969329834  \n",
            "train iteration 84:loss 462.6544351577759  \n",
            "train iteration 85:loss 466.2607145309448  \n",
            "train iteration 86:loss 470.5344400405884  \n",
            "train iteration 87:loss 474.9637985229492  \n",
            "train iteration 88:loss 483.1715564727783  \n",
            "train iteration 89:loss 488.29329204559326  \n",
            "train iteration 90:loss 494.5103588104248  \n",
            "train iteration 91:loss 499.34105682373047  \n",
            "train iteration 92:loss 502.48213291168213  \n",
            "train iteration 93:loss 505.6021957397461  \n",
            "train iteration 94:loss 511.77976989746094  \n",
            "train iteration 95:loss 517.0472679138184  \n",
            "train iteration 96:loss 520.363733291626  \n",
            "train iteration 97:loss 524.4156513214111  \n",
            "train iteration 98:loss 535.6065797805786  \n",
            "train iteration 99:loss 540.5591144561768  \n",
            "train iteration 100:loss 543.9813977777958  \n",
            "train Loss: 0.0853 Acc: 0.9933\n",
            "val iteration 1:loss 33.12643051147461  \n",
            "val iteration 2:loss 58.2701530456543  \n",
            "val iteration 3:loss 89.70162582397461  \n",
            "val iteration 4:loss 129.04822540283203  \n",
            "val iteration 5:loss 161.1938018798828  \n",
            "val iteration 6:loss 205.7693748474121  \n",
            "val iteration 7:loss 253.259521484375  \n",
            "val iteration 8:loss 283.9759159088135  \n",
            "val iteration 9:loss 353.8347110748291  \n",
            "val iteration 10:loss 380.1578884124756  \n",
            "val iteration 11:loss 419.7830333709717  \n",
            "val iteration 12:loss 440.6248302459717  \n",
            "val iteration 13:loss 475.1321964263916  \n",
            "val iteration 14:loss 524.1933155059814  \n",
            "val iteration 15:loss 581.529146194458  \n",
            "val iteration 16:loss 612.2406673431396  \n",
            "val iteration 17:loss 651.2978038787842  \n",
            "val iteration 18:loss 681.6890277862549  \n",
            "val iteration 19:loss 713.080644607544  \n",
            "val iteration 20:loss 754.8952007293701  \n",
            "val iteration 21:loss 789.3700695037842  \n",
            "val iteration 22:loss 829.74294090271  \n",
            "val iteration 23:loss 864.4283580780029  \n",
            "val iteration 24:loss 890.1104946136475  \n",
            "val iteration 25:loss 935.3298931121826  \n",
            "val iteration 26:loss 972.1606349945068  \n",
            "val iteration 27:loss 1003.0442981719971  \n",
            "val iteration 28:loss 1035.8074741363525  \n",
            "val iteration 29:loss 1074.5622272491455  \n",
            "val iteration 30:loss 1098.9939975738525  \n",
            "val iteration 31:loss 1126.3800010681152  \n",
            "val iteration 32:loss 1177.1905212402344  \n",
            "val iteration 33:loss 1220.2429809570312  \n",
            "val iteration 34:loss 1264.132827758789  \n",
            "val iteration 35:loss 1285.1135635375977  \n",
            "val iteration 36:loss 1332.4919738769531  \n",
            "val iteration 37:loss 1369.3704414367676  \n",
            "val iteration 38:loss 1397.8945960998535  \n",
            "val iteration 39:loss 1433.5341606140137  \n",
            "val iteration 40:loss 1474.0392723083496  \n",
            "val iteration 41:loss 1514.297924041748  \n",
            "val iteration 42:loss 1563.0327339172363  \n",
            "val iteration 43:loss 1593.8352165222168  \n",
            "val iteration 44:loss 1627.8526611328125  \n",
            "val iteration 45:loss 1669.8194961547852  \n",
            "val iteration 46:loss 1710.0069122314453  \n",
            "val iteration 47:loss 1758.299057006836  \n",
            "val iteration 48:loss 1783.546543121338  \n",
            "val iteration 49:loss 1822.9584617614746  \n",
            "val iteration 50:loss 1853.1268005371094  \n",
            "val iteration 51:loss 1893.837718963623  \n",
            "val iteration 52:loss 1916.6134719848633  \n",
            "val iteration 53:loss 1956.435287475586  \n",
            "val iteration 54:loss 1989.3304061889648  \n",
            "val iteration 55:loss 2021.6086044311523  \n",
            "val iteration 56:loss 2027.0737524032593  \n",
            "val Loss: 1.1491 Acc: 0.7098\n",
            "train iteration 1:loss 2.942995071411133  \n",
            "train iteration 2:loss 9.15673542022705  \n",
            "train iteration 3:loss 12.286785125732422  \n",
            "train iteration 4:loss 17.312171936035156  \n",
            "train iteration 5:loss 21.014676094055176  \n",
            "train iteration 6:loss 25.00254726409912  \n",
            "train iteration 7:loss 32.68252468109131  \n",
            "train iteration 8:loss 35.39375305175781  \n",
            "train iteration 9:loss 40.39062786102295  \n",
            "train iteration 10:loss 43.36191368103027  \n",
            "train iteration 11:loss 47.103511810302734  \n",
            "train iteration 12:loss 49.833736419677734  \n",
            "train iteration 13:loss 53.62384796142578  \n",
            "train iteration 14:loss 57.010562896728516  \n",
            "train iteration 15:loss 60.447266578674316  \n",
            "train iteration 16:loss 65.09779644012451  \n",
            "train iteration 17:loss 73.10794162750244  \n",
            "train iteration 18:loss 76.99343013763428  \n",
            "train iteration 19:loss 84.40527248382568  \n",
            "train iteration 20:loss 86.3046236038208  \n",
            "train iteration 21:loss 91.71795558929443  \n",
            "train iteration 22:loss 95.45898723602295  \n",
            "train iteration 23:loss 100.73757696151733  \n",
            "train iteration 24:loss 104.58710813522339  \n",
            "train iteration 25:loss 111.98949003219604  \n",
            "train iteration 26:loss 116.65607786178589  \n",
            "train iteration 27:loss 119.69147062301636  \n",
            "train iteration 28:loss 122.63977670669556  \n",
            "train iteration 29:loss 126.15833711624146  \n",
            "train iteration 30:loss 130.3900113105774  \n",
            "train iteration 31:loss 135.6977391242981  \n",
            "train iteration 32:loss 139.6110348701477  \n",
            "train iteration 33:loss 142.434072971344  \n",
            "train iteration 34:loss 145.16310358047485  \n",
            "train iteration 35:loss 147.55097341537476  \n",
            "train iteration 36:loss 150.9736475944519  \n",
            "train iteration 37:loss 153.90704202651978  \n",
            "train iteration 38:loss 158.11599397659302  \n",
            "train iteration 39:loss 165.41984796524048  \n",
            "train iteration 40:loss 168.72313261032104  \n",
            "train iteration 41:loss 171.85476160049438  \n",
            "train iteration 42:loss 174.48971319198608  \n",
            "train iteration 43:loss 177.53764009475708  \n",
            "train iteration 44:loss 183.14634943008423  \n",
            "train iteration 45:loss 186.09759759902954  \n",
            "train iteration 46:loss 190.5540099143982  \n",
            "train iteration 47:loss 194.46256589889526  \n",
            "train iteration 48:loss 197.70978307724  \n",
            "train iteration 49:loss 203.2217984199524  \n",
            "train iteration 50:loss 207.23675680160522  \n",
            "train iteration 51:loss 211.35993432998657  \n",
            "train iteration 52:loss 213.58390188217163  \n",
            "train iteration 53:loss 217.04040575027466  \n",
            "train iteration 54:loss 220.85123682022095  \n",
            "train iteration 55:loss 223.1188292503357  \n",
            "train iteration 56:loss 226.9837613105774  \n",
            "train iteration 57:loss 229.63866567611694  \n",
            "train iteration 58:loss 232.59758806228638  \n",
            "train iteration 59:loss 236.42785024642944  \n",
            "train iteration 60:loss 239.2900595664978  \n",
            "train iteration 61:loss 242.02009630203247  \n",
            "train iteration 62:loss 245.46825075149536  \n",
            "train iteration 63:loss 248.09352922439575  \n",
            "train iteration 64:loss 259.2836079597473  \n",
            "train iteration 65:loss 263.74253606796265  \n",
            "train iteration 66:loss 268.90803480148315  \n",
            "train iteration 67:loss 272.02719926834106  \n",
            "train iteration 68:loss 274.84049367904663  \n",
            "train iteration 69:loss 277.10037183761597  \n",
            "train iteration 70:loss 280.5676341056824  \n",
            "train iteration 71:loss 285.1321930885315  \n",
            "train iteration 72:loss 288.2938914299011  \n",
            "train iteration 73:loss 292.8059058189392  \n",
            "train iteration 74:loss 301.80200147628784  \n",
            "train iteration 75:loss 306.33115339279175  \n",
            "train iteration 76:loss 310.0199809074402  \n",
            "train iteration 77:loss 314.0938277244568  \n",
            "train iteration 78:loss 320.7438998222351  \n",
            "train iteration 79:loss 327.82650232315063  \n",
            "train iteration 80:loss 331.55889654159546  \n",
            "train iteration 81:loss 335.85413694381714  \n",
            "train iteration 82:loss 340.6405711174011  \n",
            "train iteration 83:loss 343.6438126564026  \n",
            "train iteration 84:loss 346.3083596229553  \n",
            "train iteration 85:loss 352.54955434799194  \n",
            "train iteration 86:loss 355.57112073898315  \n",
            "train iteration 87:loss 366.269407749176  \n",
            "train iteration 88:loss 369.51145601272583  \n",
            "train iteration 89:loss 372.13523626327515  \n",
            "train iteration 90:loss 374.37290716171265  \n",
            "train iteration 91:loss 377.9806933403015  \n",
            "train iteration 92:loss 379.99683713912964  \n",
            "train iteration 93:loss 389.39198637008667  \n",
            "train iteration 94:loss 392.43340826034546  \n",
            "train iteration 95:loss 395.5183758735657  \n",
            "train iteration 96:loss 401.27159547805786  \n",
            "train iteration 97:loss 406.2066082954407  \n",
            "train iteration 98:loss 408.7422308921814  \n",
            "train iteration 99:loss 411.2255816459656  \n",
            "train iteration 100:loss 415.52683967351913  \n",
            "train Loss: 0.0651 Acc: 0.9940\n",
            "val iteration 1:loss 35.45122146606445  \n",
            "val iteration 2:loss 70.76080703735352  \n",
            "val iteration 3:loss 95.6279296875  \n",
            "val iteration 4:loss 136.0447883605957  \n",
            "val iteration 5:loss 169.10995864868164  \n",
            "val iteration 6:loss 212.0191993713379  \n",
            "val iteration 7:loss 260.22633361816406  \n",
            "val iteration 8:loss 318.5299491882324  \n",
            "val iteration 9:loss 369.2963447570801  \n",
            "val iteration 10:loss 408.74124908447266  \n",
            "val iteration 11:loss 428.5197238922119  \n",
            "val iteration 12:loss 466.40235710144043  \n",
            "val iteration 13:loss 497.74036407470703  \n",
            "val iteration 14:loss 536.2174911499023  \n",
            "val iteration 15:loss 556.7145347595215  \n",
            "val iteration 16:loss 594.6161308288574  \n",
            "val iteration 17:loss 635.8709297180176  \n",
            "val iteration 18:loss 672.8214797973633  \n",
            "val iteration 19:loss 687.0048408508301  \n",
            "val iteration 20:loss 733.1893882751465  \n",
            "val iteration 21:loss 767.4460639953613  \n",
            "val iteration 22:loss 808.0807075500488  \n",
            "val iteration 23:loss 838.0070838928223  \n",
            "val iteration 24:loss 881.9229125976562  \n",
            "val iteration 25:loss 924.3398666381836  \n",
            "val iteration 26:loss 959.0298042297363  \n",
            "val iteration 27:loss 999.2080078125  \n",
            "val iteration 28:loss 1025.3328475952148  \n",
            "val iteration 29:loss 1058.5749130249023  \n",
            "val iteration 30:loss 1086.5521965026855  \n",
            "val iteration 31:loss 1124.2187805175781  \n",
            "val iteration 32:loss 1158.7334060668945  \n",
            "val iteration 33:loss 1187.7593536376953  \n",
            "val iteration 34:loss 1229.7470245361328  \n",
            "val iteration 35:loss 1256.8335456848145  \n",
            "val iteration 36:loss 1287.2293109893799  \n",
            "val iteration 37:loss 1315.1903057098389  \n",
            "val iteration 38:loss 1358.0053462982178  \n",
            "val iteration 39:loss 1391.4045429229736  \n",
            "val iteration 40:loss 1442.29465675354  \n",
            "val iteration 41:loss 1475.024522781372  \n",
            "val iteration 42:loss 1502.7808876037598  \n",
            "val iteration 43:loss 1541.9006462097168  \n",
            "val iteration 44:loss 1583.2598762512207  \n",
            "val iteration 45:loss 1611.869592666626  \n",
            "val iteration 46:loss 1651.0982608795166  \n",
            "val iteration 47:loss 1677.9986400604248  \n",
            "val iteration 48:loss 1722.8782215118408  \n",
            "val iteration 49:loss 1768.3617572784424  \n",
            "val iteration 50:loss 1812.5224704742432  \n",
            "val iteration 51:loss 1845.8957347869873  \n",
            "val iteration 52:loss 1886.7381610870361  \n",
            "val iteration 53:loss 1932.8855724334717  \n",
            "val iteration 54:loss 1963.7169952392578  \n",
            "val iteration 55:loss 1993.385986328125  \n",
            "val iteration 56:loss 1999.6143565177917  \n",
            "val Loss: 1.1336 Acc: 0.7183\n",
            "train iteration 1:loss 2.136322021484375  \n",
            "train iteration 2:loss 6.141008377075195  \n",
            "train iteration 3:loss 10.823979377746582  \n",
            "train iteration 4:loss 15.144196510314941  \n",
            "train iteration 5:loss 17.183794021606445  \n",
            "train iteration 6:loss 20.283506393432617  \n",
            "train iteration 7:loss 22.513928413391113  \n",
            "train iteration 8:loss 24.790142059326172  \n",
            "train iteration 9:loss 26.550793647766113  \n",
            "train iteration 10:loss 28.998620986938477  \n",
            "train iteration 11:loss 31.959053993225098  \n",
            "train iteration 12:loss 33.70237445831299  \n",
            "train iteration 13:loss 35.9192533493042  \n",
            "train iteration 14:loss 39.13110065460205  \n",
            "train iteration 15:loss 42.12000274658203  \n",
            "train iteration 16:loss 46.06509208679199  \n",
            "train iteration 17:loss 49.81652641296387  \n",
            "train iteration 18:loss 55.65760898590088  \n",
            "train iteration 19:loss 58.15487194061279  \n",
            "train iteration 20:loss 62.224839210510254  \n",
            "train iteration 21:loss 64.40446758270264  \n",
            "train iteration 22:loss 66.80046272277832  \n",
            "train iteration 23:loss 71.58383560180664  \n",
            "train iteration 24:loss 77.15875434875488  \n",
            "train iteration 25:loss 79.21854877471924  \n",
            "train iteration 26:loss 83.09440422058105  \n",
            "train iteration 27:loss 86.69784832000732  \n",
            "train iteration 28:loss 88.99493312835693  \n",
            "train iteration 29:loss 91.29493999481201  \n",
            "train iteration 30:loss 93.41460609436035  \n",
            "train iteration 31:loss 96.04984092712402  \n",
            "train iteration 32:loss 99.13173961639404  \n",
            "train iteration 33:loss 102.11752510070801  \n",
            "train iteration 34:loss 106.88236427307129  \n",
            "train iteration 35:loss 110.1286153793335  \n",
            "train iteration 36:loss 112.0758171081543  \n",
            "train iteration 37:loss 113.9190559387207  \n",
            "train iteration 38:loss 116.8320369720459  \n",
            "train iteration 39:loss 120.6915807723999  \n",
            "train iteration 40:loss 122.1401596069336  \n",
            "train iteration 41:loss 124.09826183319092  \n",
            "train iteration 42:loss 126.18848896026611  \n",
            "train iteration 43:loss 129.62758255004883  \n",
            "train iteration 44:loss 131.26481342315674  \n",
            "train iteration 45:loss 134.46503067016602  \n",
            "train iteration 46:loss 138.01792240142822  \n",
            "train iteration 47:loss 140.54971981048584  \n",
            "train iteration 48:loss 145.1631383895874  \n",
            "train iteration 49:loss 147.08288955688477  \n",
            "train iteration 50:loss 149.95345211029053  \n",
            "train iteration 51:loss 153.3836441040039  \n",
            "train iteration 52:loss 158.57508087158203  \n",
            "train iteration 53:loss 160.5642442703247  \n",
            "train iteration 54:loss 163.411150932312  \n",
            "train iteration 55:loss 165.84032821655273  \n",
            "train iteration 56:loss 167.31995964050293  \n",
            "train iteration 57:loss 169.43949127197266  \n",
            "train iteration 58:loss 172.01929759979248  \n",
            "train iteration 59:loss 173.64775848388672  \n",
            "train iteration 60:loss 176.20427703857422  \n",
            "train iteration 61:loss 179.05719470977783  \n",
            "train iteration 62:loss 181.55237770080566  \n",
            "train iteration 63:loss 183.83230018615723  \n",
            "train iteration 64:loss 185.5990695953369  \n",
            "train iteration 65:loss 187.99795150756836  \n",
            "train iteration 66:loss 193.37268161773682  \n",
            "train iteration 67:loss 195.18690013885498  \n",
            "train iteration 68:loss 196.6713581085205  \n",
            "train iteration 69:loss 199.4257230758667  \n",
            "train iteration 70:loss 201.6984052658081  \n",
            "train iteration 71:loss 203.61984157562256  \n",
            "train iteration 72:loss 206.13979816436768  \n",
            "train iteration 73:loss 208.89692211151123  \n",
            "train iteration 74:loss 210.76908493041992  \n",
            "train iteration 75:loss 212.95916366577148  \n",
            "train iteration 76:loss 215.49804496765137  \n",
            "train iteration 77:loss 217.70583057403564  \n",
            "train iteration 78:loss 219.62102890014648  \n",
            "train iteration 79:loss 221.73412704467773  \n",
            "train iteration 80:loss 224.88952541351318  \n",
            "train iteration 81:loss 227.4037218093872  \n",
            "train iteration 82:loss 230.28069591522217  \n",
            "train iteration 83:loss 232.9778060913086  \n",
            "train iteration 84:loss 234.49754524230957  \n",
            "train iteration 85:loss 236.4813108444214  \n",
            "train iteration 86:loss 243.4130220413208  \n",
            "train iteration 87:loss 246.10269165039062  \n",
            "train iteration 88:loss 248.34369659423828  \n",
            "train iteration 89:loss 250.74880504608154  \n",
            "train iteration 90:loss 252.9538335800171  \n",
            "train iteration 91:loss 254.8097095489502  \n",
            "train iteration 92:loss 262.6633777618408  \n",
            "train iteration 93:loss 264.374547958374  \n",
            "train iteration 94:loss 267.42958545684814  \n",
            "train iteration 95:loss 274.1027135848999  \n",
            "train iteration 96:loss 283.1200838088989  \n",
            "train iteration 97:loss 284.4801836013794  \n",
            "train iteration 98:loss 286.6197967529297  \n",
            "train iteration 99:loss 288.15299224853516  \n",
            "train iteration 100:loss 290.0630007535219  \n",
            "train Loss: 0.0455 Acc: 0.9967\n",
            "val iteration 1:loss 39.478370666503906  \n",
            "val iteration 2:loss 70.78419494628906  \n",
            "val iteration 3:loss 117.02303695678711  \n",
            "val iteration 4:loss 155.848388671875  \n",
            "val iteration 5:loss 178.83417510986328  \n",
            "val iteration 6:loss 206.46473503112793  \n",
            "val iteration 7:loss 240.2985439300537  \n",
            "val iteration 8:loss 272.4687328338623  \n",
            "val iteration 9:loss 308.3151607513428  \n",
            "val iteration 10:loss 335.8074321746826  \n",
            "val iteration 11:loss 385.1583614349365  \n",
            "val iteration 12:loss 436.8860569000244  \n",
            "val iteration 13:loss 466.3415584564209  \n",
            "val iteration 14:loss 499.8885326385498  \n",
            "val iteration 15:loss 529.773738861084  \n",
            "val iteration 16:loss 553.1948318481445  \n",
            "val iteration 17:loss 590.9056854248047  \n",
            "val iteration 18:loss 636.4688758850098  \n",
            "val iteration 19:loss 664.640718460083  \n",
            "val iteration 20:loss 713.6412410736084  \n",
            "val iteration 21:loss 743.1718139648438  \n",
            "val iteration 22:loss 777.4195899963379  \n",
            "val iteration 23:loss 813.8360443115234  \n",
            "val iteration 24:loss 841.7405242919922  \n",
            "val iteration 25:loss 877.8209991455078  \n",
            "val iteration 26:loss 911.6738548278809  \n",
            "val iteration 27:loss 928.113468170166  \n",
            "val iteration 28:loss 952.1781959533691  \n",
            "val iteration 29:loss 990.7964210510254  \n",
            "val iteration 30:loss 1033.4847373962402  \n",
            "val iteration 31:loss 1077.1213264465332  \n",
            "val iteration 32:loss 1106.266674041748  \n",
            "val iteration 33:loss 1135.4611701965332  \n",
            "val iteration 34:loss 1173.051628112793  \n",
            "val iteration 35:loss 1214.750129699707  \n",
            "val iteration 36:loss 1262.5708198547363  \n",
            "val iteration 37:loss 1304.549575805664  \n",
            "val iteration 38:loss 1338.2434539794922  \n",
            "val iteration 39:loss 1372.5383987426758  \n",
            "val iteration 40:loss 1408.6279640197754  \n",
            "val iteration 41:loss 1421.5922174453735  \n",
            "val iteration 42:loss 1454.708794593811  \n",
            "val iteration 43:loss 1483.694748878479  \n",
            "val iteration 44:loss 1514.6482248306274  \n",
            "val iteration 45:loss 1551.8112497329712  \n",
            "val iteration 46:loss 1598.335015296936  \n",
            "val iteration 47:loss 1643.990502357483  \n",
            "val iteration 48:loss 1670.0797414779663  \n",
            "val iteration 49:loss 1699.7814168930054  \n",
            "val iteration 50:loss 1736.8174962997437  \n",
            "val iteration 51:loss 1787.1513967514038  \n",
            "val iteration 52:loss 1819.9383459091187  \n",
            "val iteration 53:loss 1872.3481550216675  \n",
            "val iteration 54:loss 1912.2104749679565  \n",
            "val iteration 55:loss 1969.2800817489624  \n",
            "val iteration 56:loss 1969.4750871658325  \n",
            "val Loss: 1.1165 Acc: 0.7307\n",
            "train iteration 1:loss 1.7774267196655273  \n",
            "train iteration 2:loss 6.911538124084473  \n",
            "train iteration 3:loss 8.652246475219727  \n",
            "train iteration 4:loss 10.29612922668457  \n",
            "train iteration 5:loss 11.906742095947266  \n",
            "train iteration 6:loss 13.312561988830566  \n",
            "train iteration 7:loss 15.827444076538086  \n",
            "train iteration 8:loss 17.321880340576172  \n",
            "train iteration 9:loss 18.604564666748047  \n",
            "train iteration 10:loss 20.47075080871582  \n",
            "train iteration 11:loss 22.055354118347168  \n",
            "train iteration 12:loss 24.451685905456543  \n",
            "train iteration 13:loss 26.537660598754883  \n",
            "train iteration 14:loss 28.313715934753418  \n",
            "train iteration 15:loss 29.910521507263184  \n",
            "train iteration 16:loss 31.66996955871582  \n",
            "train iteration 17:loss 33.771531105041504  \n",
            "train iteration 18:loss 35.65076446533203  \n",
            "train iteration 19:loss 37.27663230895996  \n",
            "train iteration 20:loss 39.07624435424805  \n",
            "train iteration 21:loss 41.17839241027832  \n",
            "train iteration 22:loss 43.038620948791504  \n",
            "train iteration 23:loss 44.46388339996338  \n",
            "train iteration 24:loss 46.665802001953125  \n",
            "train iteration 25:loss 50.53925609588623  \n",
            "train iteration 26:loss 58.438894271850586  \n",
            "train iteration 27:loss 62.181121826171875  \n",
            "train iteration 28:loss 65.40450382232666  \n",
            "train iteration 29:loss 66.83760833740234  \n",
            "train iteration 30:loss 70.79526805877686  \n",
            "train iteration 31:loss 72.0998592376709  \n",
            "train iteration 32:loss 74.76438045501709  \n",
            "train iteration 33:loss 76.51399040222168  \n",
            "train iteration 34:loss 78.45947551727295  \n",
            "train iteration 35:loss 80.16123485565186  \n",
            "train iteration 36:loss 82.03634357452393  \n",
            "train iteration 37:loss 83.59340572357178  \n",
            "train iteration 38:loss 85.52485752105713  \n",
            "train iteration 39:loss 87.58892250061035  \n",
            "train iteration 40:loss 90.05586624145508  \n",
            "train iteration 41:loss 91.48017024993896  \n",
            "train iteration 42:loss 93.8107557296753  \n",
            "train iteration 43:loss 96.2640151977539  \n",
            "train iteration 44:loss 97.29815196990967  \n",
            "train iteration 45:loss 99.14320850372314  \n",
            "train iteration 46:loss 101.75200939178467  \n",
            "train iteration 47:loss 102.68420886993408  \n",
            "train iteration 48:loss 104.47985649108887  \n",
            "train iteration 49:loss 106.4484052658081  \n",
            "train iteration 50:loss 108.22277069091797  \n",
            "train iteration 51:loss 109.61515235900879  \n",
            "train iteration 52:loss 114.0760383605957  \n",
            "train iteration 53:loss 116.2254695892334  \n",
            "train iteration 54:loss 119.97259521484375  \n",
            "train iteration 55:loss 121.10086822509766  \n",
            "train iteration 56:loss 122.68459415435791  \n",
            "train iteration 57:loss 124.69314479827881  \n",
            "train iteration 58:loss 126.37647151947021  \n",
            "train iteration 59:loss 127.8463249206543  \n",
            "train iteration 60:loss 130.45702075958252  \n",
            "train iteration 61:loss 132.1069803237915  \n",
            "train iteration 62:loss 133.4415340423584  \n",
            "train iteration 63:loss 135.63945388793945  \n",
            "train iteration 64:loss 137.39889526367188  \n",
            "train iteration 65:loss 141.72492790222168  \n",
            "train iteration 66:loss 143.76139736175537  \n",
            "train iteration 67:loss 145.14516735076904  \n",
            "train iteration 68:loss 146.6416425704956  \n",
            "train iteration 69:loss 148.4041109085083  \n",
            "train iteration 70:loss 150.1488742828369  \n",
            "train iteration 71:loss 152.7277135848999  \n",
            "train iteration 72:loss 154.77193069458008  \n",
            "train iteration 73:loss 156.71913528442383  \n",
            "train iteration 74:loss 158.17228317260742  \n",
            "train iteration 75:loss 160.0308380126953  \n",
            "train iteration 76:loss 164.56943035125732  \n",
            "train iteration 77:loss 166.10971450805664  \n",
            "train iteration 78:loss 168.62133979797363  \n",
            "train iteration 79:loss 171.16071605682373  \n",
            "train iteration 80:loss 172.44572257995605  \n",
            "train iteration 81:loss 174.3940725326538  \n",
            "train iteration 82:loss 177.54328155517578  \n",
            "train iteration 83:loss 179.0912628173828  \n",
            "train iteration 84:loss 180.3186550140381  \n",
            "train iteration 85:loss 181.958420753479  \n",
            "train iteration 86:loss 183.67802238464355  \n",
            "train iteration 87:loss 185.11545658111572  \n",
            "train iteration 88:loss 193.11540842056274  \n",
            "train iteration 89:loss 195.03206777572632  \n",
            "train iteration 90:loss 196.96002531051636  \n",
            "train iteration 91:loss 198.1647448539734  \n",
            "train iteration 92:loss 204.45087480545044  \n",
            "train iteration 93:loss 206.18856573104858  \n",
            "train iteration 94:loss 211.00434064865112  \n",
            "train iteration 95:loss 212.36668252944946  \n",
            "train iteration 96:loss 213.7364592552185  \n",
            "train iteration 97:loss 215.86204290390015  \n",
            "train iteration 98:loss 217.08631467819214  \n",
            "train iteration 99:loss 218.82238721847534  \n",
            "train iteration 100:loss 221.20154325664043  \n",
            "train Loss: 0.0347 Acc: 0.9980\n",
            "val iteration 1:loss 37.78772735595703  \n",
            "val iteration 2:loss 76.4131851196289  \n",
            "val iteration 3:loss 118.72393035888672  \n",
            "val iteration 4:loss 151.40078735351562  \n",
            "val iteration 5:loss 185.74758529663086  \n",
            "val iteration 6:loss 206.31158828735352  \n",
            "val iteration 7:loss 247.69185256958008  \n",
            "val iteration 8:loss 282.0184555053711  \n",
            "val iteration 9:loss 320.55035400390625  \n",
            "val iteration 10:loss 351.24480628967285  \n",
            "val iteration 11:loss 388.25812339782715  \n",
            "val iteration 12:loss 411.73410987854004  \n",
            "val iteration 13:loss 457.1748294830322  \n",
            "val iteration 14:loss 516.0203495025635  \n",
            "val iteration 15:loss 554.2142124176025  \n",
            "val iteration 16:loss 593.9066371917725  \n",
            "val iteration 17:loss 638.6764812469482  \n",
            "val iteration 18:loss 660.7667465209961  \n",
            "val iteration 19:loss 692.0852546691895  \n",
            "val iteration 20:loss 718.2173881530762  \n",
            "val iteration 21:loss 745.0731010437012  \n",
            "val iteration 22:loss 773.1959991455078  \n",
            "val iteration 23:loss 803.5825939178467  \n",
            "val iteration 24:loss 854.3404521942139  \n",
            "val iteration 25:loss 879.6054172515869  \n",
            "val iteration 26:loss 915.6528263092041  \n",
            "val iteration 27:loss 948.1209392547607  \n",
            "val iteration 28:loss 984.001989364624  \n",
            "val iteration 29:loss 1015.4539775848389  \n",
            "val iteration 30:loss 1044.8818798065186  \n",
            "val iteration 31:loss 1060.985279083252  \n",
            "val iteration 32:loss 1097.6955032348633  \n",
            "val iteration 33:loss 1122.8572463989258  \n",
            "val iteration 34:loss 1156.582275390625  \n",
            "val iteration 35:loss 1194.648338317871  \n",
            "val iteration 36:loss 1225.624267578125  \n",
            "val iteration 37:loss 1259.6165199279785  \n",
            "val iteration 38:loss 1293.6714057922363  \n",
            "val iteration 39:loss 1322.6266536712646  \n",
            "val iteration 40:loss 1368.4481716156006  \n",
            "val iteration 41:loss 1404.7892742156982  \n",
            "val iteration 42:loss 1418.442120552063  \n",
            "val iteration 43:loss 1455.273449897766  \n",
            "val iteration 44:loss 1487.4076776504517  \n",
            "val iteration 45:loss 1507.6948976516724  \n",
            "val iteration 46:loss 1540.3206720352173  \n",
            "val iteration 47:loss 1572.095519065857  \n",
            "val iteration 48:loss 1597.5520868301392  \n",
            "val iteration 49:loss 1651.5009355545044  \n",
            "val iteration 50:loss 1684.7723474502563  \n",
            "val iteration 51:loss 1718.0444650650024  \n",
            "val iteration 52:loss 1770.5677728652954  \n",
            "val iteration 53:loss 1800.9186792373657  \n",
            "val iteration 54:loss 1852.200364112854  \n",
            "val iteration 55:loss 1891.9011240005493  \n",
            "val iteration 56:loss 1895.7601537704468  \n",
            "val Loss: 1.0747 Acc: 0.7307\n",
            "train iteration 1:loss 2.167024612426758  \n",
            "train iteration 2:loss 3.3760690689086914  \n",
            "train iteration 3:loss 5.369349479675293  \n",
            "train iteration 4:loss 6.332062721252441  \n",
            "train iteration 5:loss 8.316682815551758  \n",
            "train iteration 6:loss 10.195440292358398  \n",
            "train iteration 7:loss 13.57790756225586  \n",
            "train iteration 8:loss 14.779176712036133  \n",
            "train iteration 9:loss 16.36154842376709  \n",
            "train iteration 10:loss 17.47463321685791  \n",
            "train iteration 11:loss 18.915287017822266  \n",
            "train iteration 12:loss 20.47808265686035  \n",
            "train iteration 13:loss 22.44415855407715  \n",
            "train iteration 14:loss 23.723149299621582  \n",
            "train iteration 15:loss 25.466556549072266  \n",
            "train iteration 16:loss 27.051618576049805  \n",
            "train iteration 17:loss 33.07110786437988  \n",
            "train iteration 18:loss 34.60284614562988  \n",
            "train iteration 19:loss 36.162110328674316  \n",
            "train iteration 20:loss 37.86323642730713  \n",
            "train iteration 21:loss 39.54492950439453  \n",
            "train iteration 22:loss 40.973548889160156  \n",
            "train iteration 23:loss 42.75409698486328  \n",
            "train iteration 24:loss 44.10536003112793  \n",
            "train iteration 25:loss 45.459689140319824  \n",
            "train iteration 26:loss 46.76974582672119  \n",
            "train iteration 27:loss 48.007657051086426  \n",
            "train iteration 28:loss 49.38033485412598  \n",
            "train iteration 29:loss 50.49846363067627  \n",
            "train iteration 30:loss 52.501712799072266  \n",
            "train iteration 31:loss 54.262746810913086  \n",
            "train iteration 32:loss 55.99658012390137  \n",
            "train iteration 33:loss 57.18156814575195  \n",
            "train iteration 34:loss 58.48994541168213  \n",
            "train iteration 35:loss 59.74403953552246  \n",
            "train iteration 36:loss 61.10734558105469  \n",
            "train iteration 37:loss 65.9132490158081  \n",
            "train iteration 38:loss 67.06348133087158  \n",
            "train iteration 39:loss 68.46974086761475  \n",
            "train iteration 40:loss 70.2352991104126  \n",
            "train iteration 41:loss 71.41747188568115  \n",
            "train iteration 42:loss 73.31552124023438  \n",
            "train iteration 43:loss 74.090163230896  \n",
            "train iteration 44:loss 75.66173553466797  \n",
            "train iteration 45:loss 77.4448709487915  \n",
            "train iteration 46:loss 79.25877475738525  \n",
            "train iteration 47:loss 85.29232692718506  \n",
            "train iteration 48:loss 86.7733678817749  \n",
            "train iteration 49:loss 88.04317474365234  \n",
            "train iteration 50:loss 89.72151470184326  \n",
            "train iteration 51:loss 90.99939918518066  \n",
            "train iteration 52:loss 92.23031902313232  \n",
            "train iteration 53:loss 93.49708271026611  \n",
            "train iteration 54:loss 94.69410705566406  \n",
            "train iteration 55:loss 96.37596225738525  \n",
            "train iteration 56:loss 97.63372611999512  \n",
            "train iteration 57:loss 98.81742000579834  \n",
            "train iteration 58:loss 100.02989196777344  \n",
            "train iteration 59:loss 101.3274736404419  \n",
            "train iteration 60:loss 102.43940448760986  \n",
            "train iteration 61:loss 103.5526647567749  \n",
            "train iteration 62:loss 104.86379623413086  \n",
            "train iteration 63:loss 106.60537338256836  \n",
            "train iteration 64:loss 107.99819564819336  \n",
            "train iteration 65:loss 109.56143283843994  \n",
            "train iteration 66:loss 110.76193046569824  \n",
            "train iteration 67:loss 112.04124546051025  \n",
            "train iteration 68:loss 113.28302097320557  \n",
            "train iteration 69:loss 114.37200355529785  \n",
            "train iteration 70:loss 115.52452850341797  \n",
            "train iteration 71:loss 116.62363338470459  \n",
            "train iteration 72:loss 117.72563552856445  \n",
            "train iteration 73:loss 118.79279136657715  \n",
            "train iteration 74:loss 120.33143138885498  \n",
            "train iteration 75:loss 121.35836410522461  \n",
            "train iteration 76:loss 122.93151473999023  \n",
            "train iteration 77:loss 123.9860897064209  \n",
            "train iteration 78:loss 125.34995746612549  \n",
            "train iteration 79:loss 126.90549755096436  \n",
            "train iteration 80:loss 128.86563110351562  \n",
            "train iteration 81:loss 130.30964469909668  \n",
            "train iteration 82:loss 131.75271606445312  \n",
            "train iteration 83:loss 134.66903018951416  \n",
            "train iteration 84:loss 135.93705081939697  \n",
            "train iteration 85:loss 137.40413665771484  \n",
            "train iteration 86:loss 138.79094982147217  \n",
            "train iteration 87:loss 140.1524314880371  \n",
            "train iteration 88:loss 141.22619342803955  \n",
            "train iteration 89:loss 142.834246635437  \n",
            "train iteration 90:loss 145.0203685760498  \n",
            "train iteration 91:loss 150.17560195922852  \n",
            "train iteration 92:loss 151.1126527786255  \n",
            "train iteration 93:loss 152.3938570022583  \n",
            "train iteration 94:loss 153.58157062530518  \n",
            "train iteration 95:loss 155.32404708862305  \n",
            "train iteration 96:loss 157.05302143096924  \n",
            "train iteration 97:loss 158.29468727111816  \n",
            "train iteration 98:loss 159.50565910339355  \n",
            "train iteration 99:loss 160.59331607818604  \n",
            "train iteration 100:loss 161.33627793192863  \n",
            "train Loss: 0.0253 Acc: 0.9991\n",
            "val iteration 1:loss 53.51154327392578  \n",
            "val iteration 2:loss 84.68009185791016  \n",
            "val iteration 3:loss 125.56185150146484  \n",
            "val iteration 4:loss 169.50822830200195  \n",
            "val iteration 5:loss 207.54558181762695  \n",
            "val iteration 6:loss 248.59192276000977  \n",
            "val iteration 7:loss 296.46403884887695  \n",
            "val iteration 8:loss 333.00791931152344  \n",
            "val iteration 9:loss 360.5913772583008  \n",
            "val iteration 10:loss 384.7143440246582  \n",
            "val iteration 11:loss 417.8313407897949  \n",
            "val iteration 12:loss 440.3313217163086  \n",
            "val iteration 13:loss 488.7373466491699  \n",
            "val iteration 14:loss 507.9885902404785  \n",
            "val iteration 15:loss 535.06591796875  \n",
            "val iteration 16:loss 557.7293338775635  \n",
            "val iteration 17:loss 572.3863353729248  \n",
            "val iteration 18:loss 610.165506362915  \n",
            "val iteration 19:loss 631.3600387573242  \n",
            "val iteration 20:loss 671.5685958862305  \n",
            "val iteration 21:loss 693.911657333374  \n",
            "val iteration 22:loss 718.772294998169  \n",
            "val iteration 23:loss 752.954195022583  \n",
            "val iteration 24:loss 772.9904880523682  \n",
            "val iteration 25:loss 816.1704654693604  \n",
            "val iteration 26:loss 843.4922885894775  \n",
            "val iteration 27:loss 864.1562423706055  \n",
            "val iteration 28:loss 899.383602142334  \n",
            "val iteration 29:loss 930.5274829864502  \n",
            "val iteration 30:loss 966.9905605316162  \n",
            "val iteration 31:loss 1019.1596240997314  \n",
            "val iteration 32:loss 1048.3756275177002  \n",
            "val iteration 33:loss 1089.5375080108643  \n",
            "val iteration 34:loss 1124.470796585083  \n",
            "val iteration 35:loss 1160.1776142120361  \n",
            "val iteration 36:loss 1208.0439472198486  \n",
            "val iteration 37:loss 1250.2210788726807  \n",
            "val iteration 38:loss 1284.4013576507568  \n",
            "val iteration 39:loss 1323.1743297576904  \n",
            "val iteration 40:loss 1357.051435470581  \n",
            "val iteration 41:loss 1385.436122894287  \n",
            "val iteration 42:loss 1418.6916122436523  \n",
            "val iteration 43:loss 1457.4793128967285  \n",
            "val iteration 44:loss 1492.1409454345703  \n",
            "val iteration 45:loss 1516.0469512939453  \n",
            "val iteration 46:loss 1544.3932723999023  \n",
            "val iteration 47:loss 1574.7283172607422  \n",
            "val iteration 48:loss 1610.668701171875  \n",
            "val iteration 49:loss 1651.198886871338  \n",
            "val iteration 50:loss 1691.8942489624023  \n",
            "val iteration 51:loss 1713.4321155548096  \n",
            "val iteration 52:loss 1752.3009433746338  \n",
            "val iteration 53:loss 1792.8600406646729  \n",
            "val iteration 54:loss 1825.8480319976807  \n",
            "val iteration 55:loss 1846.8193683624268  \n",
            "val iteration 56:loss 1853.558889389038  \n",
            "val Loss: 1.0508 Acc: 0.7392\n",
            "train iteration 1:loss 1.2462739944458008  \n",
            "train iteration 2:loss 2.9778575897216797  \n",
            "train iteration 3:loss 4.454089164733887  \n",
            "train iteration 4:loss 5.751580238342285  \n",
            "train iteration 5:loss 7.341778755187988  \n",
            "train iteration 6:loss 8.356643676757812  \n",
            "train iteration 7:loss 9.249540328979492  \n",
            "train iteration 8:loss 10.190608978271484  \n",
            "train iteration 9:loss 11.22194766998291  \n",
            "train iteration 10:loss 12.312870025634766  \n",
            "train iteration 11:loss 13.306666374206543  \n",
            "train iteration 12:loss 17.016043663024902  \n",
            "train iteration 13:loss 18.719515800476074  \n",
            "train iteration 14:loss 20.94000244140625  \n",
            "train iteration 15:loss 22.683972358703613  \n",
            "train iteration 16:loss 24.520230293273926  \n",
            "train iteration 17:loss 25.515686988830566  \n",
            "train iteration 18:loss 26.733840942382812  \n",
            "train iteration 19:loss 27.752717971801758  \n",
            "train iteration 20:loss 28.660043716430664  \n",
            "train iteration 21:loss 29.670998573303223  \n",
            "train iteration 22:loss 30.8734188079834  \n",
            "train iteration 23:loss 32.11229610443115  \n",
            "train iteration 24:loss 32.91937255859375  \n",
            "train iteration 25:loss 35.937682151794434  \n",
            "train iteration 26:loss 37.412108421325684  \n",
            "train iteration 27:loss 38.31324005126953  \n",
            "train iteration 28:loss 39.355695724487305  \n",
            "train iteration 29:loss 40.57668876647949  \n",
            "train iteration 30:loss 41.75477695465088  \n",
            "train iteration 31:loss 43.573286056518555  \n",
            "train iteration 32:loss 44.545220375061035  \n",
            "train iteration 33:loss 45.648409843444824  \n",
            "train iteration 34:loss 47.34456443786621  \n",
            "train iteration 35:loss 48.41494846343994  \n",
            "train iteration 36:loss 50.91653347015381  \n",
            "train iteration 37:loss 51.91504955291748  \n",
            "train iteration 38:loss 53.08449745178223  \n",
            "train iteration 39:loss 54.32322120666504  \n",
            "train iteration 40:loss 55.58235836029053  \n",
            "train iteration 41:loss 56.89202117919922  \n",
            "train iteration 42:loss 58.283249855041504  \n",
            "train iteration 43:loss 59.44035339355469  \n",
            "train iteration 44:loss 60.404296875  \n",
            "train iteration 45:loss 61.61790466308594  \n",
            "train iteration 46:loss 63.28853416442871  \n",
            "train iteration 47:loss 70.40077686309814  \n",
            "train iteration 48:loss 77.1769962310791  \n",
            "train iteration 49:loss 78.16410541534424  \n",
            "train iteration 50:loss 79.46342754364014  \n",
            "train iteration 51:loss 80.43391036987305  \n",
            "train iteration 52:loss 81.77560997009277  \n",
            "train iteration 53:loss 83.43991279602051  \n",
            "train iteration 54:loss 84.40885543823242  \n",
            "train iteration 55:loss 85.76241493225098  \n",
            "train iteration 56:loss 89.05754089355469  \n",
            "train iteration 57:loss 90.74903202056885  \n",
            "train iteration 58:loss 92.04394721984863  \n",
            "train iteration 59:loss 94.92067623138428  \n",
            "train iteration 60:loss 96.0431489944458  \n",
            "train iteration 61:loss 97.55866432189941  \n",
            "train iteration 62:loss 99.03077507019043  \n",
            "train iteration 63:loss 100.5490140914917  \n",
            "train iteration 64:loss 101.6452465057373  \n",
            "train iteration 65:loss 102.8488712310791  \n",
            "train iteration 66:loss 104.06621646881104  \n",
            "train iteration 67:loss 105.23777294158936  \n",
            "train iteration 68:loss 106.23720836639404  \n",
            "train iteration 69:loss 107.08281230926514  \n",
            "train iteration 70:loss 108.37192916870117  \n",
            "train iteration 71:loss 109.80584621429443  \n",
            "train iteration 72:loss 114.53034114837646  \n",
            "train iteration 73:loss 115.73977184295654  \n",
            "train iteration 74:loss 116.99141502380371  \n",
            "train iteration 75:loss 117.82604026794434  \n",
            "train iteration 76:loss 118.90918350219727  \n",
            "train iteration 77:loss 119.98606014251709  \n",
            "train iteration 78:loss 121.12996864318848  \n",
            "train iteration 79:loss 127.1540470123291  \n",
            "train iteration 80:loss 128.29922199249268  \n",
            "train iteration 81:loss 129.21386528015137  \n",
            "train iteration 82:loss 130.34491729736328  \n",
            "train iteration 83:loss 131.89171028137207  \n",
            "train iteration 84:loss 133.14942359924316  \n",
            "train iteration 85:loss 133.9041976928711  \n",
            "train iteration 86:loss 134.8405647277832  \n",
            "train iteration 87:loss 137.81175422668457  \n",
            "train iteration 88:loss 140.35128116607666  \n",
            "train iteration 89:loss 141.3672752380371  \n",
            "train iteration 90:loss 142.45042514801025  \n",
            "train iteration 91:loss 143.63263607025146  \n",
            "train iteration 92:loss 145.60611248016357  \n",
            "train iteration 93:loss 146.94553089141846  \n",
            "train iteration 94:loss 148.22726154327393  \n",
            "train iteration 95:loss 149.37210273742676  \n",
            "train iteration 96:loss 150.27683353424072  \n",
            "train iteration 97:loss 151.22790050506592  \n",
            "train iteration 98:loss 152.40996170043945  \n",
            "train iteration 99:loss 153.5001401901245  \n",
            "train iteration 100:loss 154.65908905863762  \n",
            "train Loss: 0.0242 Acc: 0.9980\n",
            "val iteration 1:loss 20.493343353271484  \n",
            "val iteration 2:loss 51.2048225402832  \n",
            "val iteration 3:loss 93.70510482788086  \n",
            "val iteration 4:loss 124.56778526306152  \n",
            "val iteration 5:loss 157.1782054901123  \n",
            "val iteration 6:loss 180.41320991516113  \n",
            "val iteration 7:loss 228.8469181060791  \n",
            "val iteration 8:loss 257.216854095459  \n",
            "val iteration 9:loss 283.15064430236816  \n",
            "val iteration 10:loss 307.9459762573242  \n",
            "val iteration 11:loss 342.9009704589844  \n",
            "val iteration 12:loss 369.1249256134033  \n",
            "val iteration 13:loss 394.3350067138672  \n",
            "val iteration 14:loss 426.73431396484375  \n",
            "val iteration 15:loss 472.97037506103516  \n",
            "val iteration 16:loss 520.053466796875  \n",
            "val iteration 17:loss 551.575590133667  \n",
            "val iteration 18:loss 595.4183902740479  \n",
            "val iteration 19:loss 645.6338787078857  \n",
            "val iteration 20:loss 676.1962070465088  \n",
            "val iteration 21:loss 703.9307651519775  \n",
            "val iteration 22:loss 749.8606433868408  \n",
            "val iteration 23:loss 786.7277698516846  \n",
            "val iteration 24:loss 819.69114112854  \n",
            "val iteration 25:loss 841.4031200408936  \n",
            "val iteration 26:loss 888.0990657806396  \n",
            "val iteration 27:loss 939.1906070709229  \n",
            "val iteration 28:loss 956.3757152557373  \n",
            "val iteration 29:loss 977.7480945587158  \n",
            "val iteration 30:loss 1011.8418102264404  \n",
            "val iteration 31:loss 1049.2841396331787  \n",
            "val iteration 32:loss 1089.975248336792  \n",
            "val iteration 33:loss 1123.4994716644287  \n",
            "val iteration 34:loss 1162.875337600708  \n",
            "val iteration 35:loss 1175.2650470733643  \n",
            "val iteration 36:loss 1210.6151142120361  \n",
            "val iteration 37:loss 1238.2644481658936  \n",
            "val iteration 38:loss 1272.5517559051514  \n",
            "val iteration 39:loss 1300.897066116333  \n",
            "val iteration 40:loss 1350.2677631378174  \n",
            "val iteration 41:loss 1385.3356533050537  \n",
            "val iteration 42:loss 1421.6227550506592  \n",
            "val iteration 43:loss 1448.9166145324707  \n",
            "val iteration 44:loss 1466.0988121032715  \n",
            "val iteration 45:loss 1506.4803276062012  \n",
            "val iteration 46:loss 1536.6975784301758  \n",
            "val iteration 47:loss 1570.1521072387695  \n",
            "val iteration 48:loss 1603.1638221740723  \n",
            "val iteration 49:loss 1629.4051628112793  \n",
            "val iteration 50:loss 1648.160488128662  \n",
            "val iteration 51:loss 1687.7071723937988  \n",
            "val iteration 52:loss 1737.6734580993652  \n",
            "val iteration 53:loss 1778.712070465088  \n",
            "val iteration 54:loss 1817.9817276000977  \n",
            "val iteration 55:loss 1843.1295146942139  \n",
            "val iteration 56:loss 1848.4699640274048  \n",
            "val Loss: 1.0479 Acc: 0.7449\n",
            "train iteration 1:loss 0.7751569747924805  \n",
            "train iteration 2:loss 1.6811609268188477  \n",
            "train iteration 3:loss 2.6790428161621094  \n",
            "train iteration 4:loss 3.929689407348633  \n",
            "train iteration 5:loss 5.179558753967285  \n",
            "train iteration 6:loss 6.284990310668945  \n",
            "train iteration 7:loss 7.079256057739258  \n",
            "train iteration 8:loss 9.362831115722656  \n",
            "train iteration 9:loss 10.928006172180176  \n",
            "train iteration 10:loss 11.981758117675781  \n",
            "train iteration 11:loss 13.046582221984863  \n",
            "train iteration 12:loss 14.01107120513916  \n",
            "train iteration 13:loss 14.864042282104492  \n",
            "train iteration 14:loss 15.789800643920898  \n",
            "train iteration 15:loss 16.80542278289795  \n",
            "train iteration 16:loss 17.95588779449463  \n",
            "train iteration 17:loss 19.148587226867676  \n",
            "train iteration 18:loss 20.165361404418945  \n",
            "train iteration 19:loss 23.447543144226074  \n",
            "train iteration 20:loss 24.731433868408203  \n",
            "train iteration 21:loss 25.73613452911377  \n",
            "train iteration 22:loss 26.76678466796875  \n",
            "train iteration 23:loss 32.776153564453125  \n",
            "train iteration 24:loss 33.685415267944336  \n",
            "train iteration 25:loss 34.27840709686279  \n",
            "train iteration 26:loss 35.272871017456055  \n",
            "train iteration 27:loss 36.89300346374512  \n",
            "train iteration 28:loss 38.179444313049316  \n",
            "train iteration 29:loss 38.914817810058594  \n",
            "train iteration 30:loss 42.2492151260376  \n",
            "train iteration 31:loss 43.14313316345215  \n",
            "train iteration 32:loss 43.95921325683594  \n",
            "train iteration 33:loss 45.14674377441406  \n",
            "train iteration 34:loss 47.31709003448486  \n",
            "train iteration 35:loss 48.62610626220703  \n",
            "train iteration 36:loss 49.49270439147949  \n",
            "train iteration 37:loss 50.91481685638428  \n",
            "train iteration 38:loss 51.98640441894531  \n",
            "train iteration 39:loss 53.22101974487305  \n",
            "train iteration 40:loss 54.40424919128418  \n",
            "train iteration 41:loss 55.4670991897583  \n",
            "train iteration 42:loss 56.48152446746826  \n",
            "train iteration 43:loss 57.347710609436035  \n",
            "train iteration 44:loss 63.65122985839844  \n",
            "train iteration 45:loss 64.79445171356201  \n",
            "train iteration 46:loss 65.79421710968018  \n",
            "train iteration 47:loss 67.24354934692383  \n",
            "train iteration 48:loss 74.41542100906372  \n",
            "train iteration 49:loss 75.431800365448  \n",
            "train iteration 50:loss 76.1702036857605  \n",
            "train iteration 51:loss 77.03682947158813  \n",
            "train iteration 52:loss 77.76604986190796  \n",
            "train iteration 53:loss 78.73940515518188  \n",
            "train iteration 54:loss 80.33743810653687  \n",
            "train iteration 55:loss 81.63788366317749  \n",
            "train iteration 56:loss 82.69777917861938  \n",
            "train iteration 57:loss 83.61973428726196  \n",
            "train iteration 58:loss 84.77811002731323  \n",
            "train iteration 59:loss 85.98730897903442  \n",
            "train iteration 60:loss 86.74936151504517  \n",
            "train iteration 61:loss 92.54803133010864  \n",
            "train iteration 62:loss 93.85644483566284  \n",
            "train iteration 63:loss 94.75550889968872  \n",
            "train iteration 64:loss 96.29463243484497  \n",
            "train iteration 65:loss 97.38005685806274  \n",
            "train iteration 66:loss 98.15002965927124  \n",
            "train iteration 67:loss 99.64967679977417  \n",
            "train iteration 68:loss 100.93845796585083  \n",
            "train iteration 69:loss 102.05971097946167  \n",
            "train iteration 70:loss 104.20612001419067  \n",
            "train iteration 71:loss 104.98202180862427  \n",
            "train iteration 72:loss 105.83746004104614  \n",
            "train iteration 73:loss 106.83993482589722  \n",
            "train iteration 74:loss 107.56903314590454  \n",
            "train iteration 75:loss 108.62487363815308  \n",
            "train iteration 76:loss 109.4917893409729  \n",
            "train iteration 77:loss 110.28515481948853  \n",
            "train iteration 78:loss 111.53892374038696  \n",
            "train iteration 79:loss 116.20698499679565  \n",
            "train iteration 80:loss 118.88299131393433  \n",
            "train iteration 81:loss 119.8971037864685  \n",
            "train iteration 82:loss 120.87833452224731  \n",
            "train iteration 83:loss 122.03301286697388  \n",
            "train iteration 84:loss 122.91309595108032  \n",
            "train iteration 85:loss 123.58325242996216  \n",
            "train iteration 86:loss 124.32880353927612  \n",
            "train iteration 87:loss 125.59296941757202  \n",
            "train iteration 88:loss 126.29803991317749  \n",
            "train iteration 89:loss 126.89072179794312  \n",
            "train iteration 90:loss 128.22642278671265  \n",
            "train iteration 91:loss 129.28215265274048  \n",
            "train iteration 92:loss 130.3597674369812  \n",
            "train iteration 93:loss 131.46264219284058  \n",
            "train iteration 94:loss 132.32660722732544  \n",
            "train iteration 95:loss 133.18453073501587  \n",
            "train iteration 96:loss 134.07369375228882  \n",
            "train iteration 97:loss 134.9509196281433  \n",
            "train iteration 98:loss 140.360981464386  \n",
            "train iteration 99:loss 141.16009187698364  \n",
            "train iteration 100:loss 142.03813318163157  \n",
            "train Loss: 0.0223 Acc: 0.9981\n",
            "val iteration 1:loss 30.937868118286133  \n",
            "val iteration 2:loss 64.56243705749512  \n",
            "val iteration 3:loss 106.32192039489746  \n",
            "val iteration 4:loss 147.21744346618652  \n",
            "val iteration 5:loss 178.1375904083252  \n",
            "val iteration 6:loss 210.62145805358887  \n",
            "val iteration 7:loss 246.29301643371582  \n",
            "val iteration 8:loss 279.8742618560791  \n",
            "val iteration 9:loss 316.3041248321533  \n",
            "val iteration 10:loss 351.8822650909424  \n",
            "val iteration 11:loss 381.790246963501  \n",
            "val iteration 12:loss 418.0656108856201  \n",
            "val iteration 13:loss 431.48469066619873  \n",
            "val iteration 14:loss 469.9406728744507  \n",
            "val iteration 15:loss 496.05194568634033  \n",
            "val iteration 16:loss 545.6889543533325  \n",
            "val iteration 17:loss 582.1495943069458  \n",
            "val iteration 18:loss 609.1934804916382  \n",
            "val iteration 19:loss 630.6029787063599  \n",
            "val iteration 20:loss 651.5178384780884  \n",
            "val iteration 21:loss 688.3427667617798  \n",
            "val iteration 22:loss 707.5993146896362  \n",
            "val iteration 23:loss 757.2454481124878  \n",
            "val iteration 24:loss 786.2996397018433  \n",
            "val iteration 25:loss 826.0002202987671  \n",
            "val iteration 26:loss 851.3759508132935  \n",
            "val iteration 27:loss 907.4664278030396  \n",
            "val iteration 28:loss 940.7970895767212  \n",
            "val iteration 29:loss 969.3042345046997  \n",
            "val iteration 30:loss 999.8718385696411  \n",
            "val iteration 31:loss 1022.3971529006958  \n",
            "val iteration 32:loss 1057.0435457229614  \n",
            "val iteration 33:loss 1106.6302499771118  \n",
            "val iteration 34:loss 1139.2387323379517  \n",
            "val iteration 35:loss 1180.3436784744263  \n",
            "val iteration 36:loss 1209.2349271774292  \n",
            "val iteration 37:loss 1233.2256116867065  \n",
            "val iteration 38:loss 1262.8745527267456  \n",
            "val iteration 39:loss 1291.6551542282104  \n",
            "val iteration 40:loss 1323.5954656600952  \n",
            "val iteration 41:loss 1354.1684675216675  \n",
            "val iteration 42:loss 1377.347098350525  \n",
            "val iteration 43:loss 1403.578164100647  \n",
            "val iteration 44:loss 1448.4792833328247  \n",
            "val iteration 45:loss 1484.4040727615356  \n",
            "val iteration 46:loss 1509.082299232483  \n",
            "val iteration 47:loss 1543.3814706802368  \n",
            "val iteration 48:loss 1579.810643196106  \n",
            "val iteration 49:loss 1605.604920387268  \n",
            "val iteration 50:loss 1649.6864519119263  \n",
            "val iteration 51:loss 1674.926537513733  \n",
            "val iteration 52:loss 1717.5958528518677  \n",
            "val iteration 53:loss 1771.4603090286255  \n",
            "val iteration 54:loss 1793.9578218460083  \n",
            "val iteration 55:loss 1829.7963495254517  \n",
            "val iteration 56:loss 1834.6467714309692  \n",
            "val Loss: 1.0400 Acc: 0.7460\n",
            "train iteration 1:loss 1.441171646118164  \n",
            "train iteration 2:loss 2.122281074523926  \n",
            "train iteration 3:loss 3.0564441680908203  \n",
            "train iteration 4:loss 4.316468238830566  \n",
            "train iteration 5:loss 5.159623146057129  \n",
            "train iteration 6:loss 6.165191650390625  \n",
            "train iteration 7:loss 7.314201354980469  \n",
            "train iteration 8:loss 8.024019241333008  \n",
            "train iteration 9:loss 9.011590957641602  \n",
            "train iteration 10:loss 9.904767036437988  \n",
            "train iteration 11:loss 10.794119834899902  \n",
            "train iteration 12:loss 12.542455673217773  \n",
            "train iteration 13:loss 13.474990844726562  \n",
            "train iteration 14:loss 14.602542877197266  \n",
            "train iteration 15:loss 15.498734474182129  \n",
            "train iteration 16:loss 16.42296028137207  \n",
            "train iteration 17:loss 17.090959548950195  \n",
            "train iteration 18:loss 17.940932273864746  \n",
            "train iteration 19:loss 18.521538734436035  \n",
            "train iteration 20:loss 19.32176685333252  \n",
            "train iteration 21:loss 20.219244956970215  \n",
            "train iteration 22:loss 20.908318519592285  \n",
            "train iteration 23:loss 21.64202117919922  \n",
            "train iteration 24:loss 22.992226600646973  \n",
            "train iteration 25:loss 24.70940113067627  \n",
            "train iteration 26:loss 28.05647563934326  \n",
            "train iteration 27:loss 29.02114200592041  \n",
            "train iteration 28:loss 29.617250442504883  \n",
            "train iteration 29:loss 30.486191749572754  \n",
            "train iteration 30:loss 31.27816677093506  \n",
            "train iteration 31:loss 32.07050704956055  \n",
            "train iteration 32:loss 33.16238975524902  \n",
            "train iteration 33:loss 37.746310234069824  \n",
            "train iteration 34:loss 38.86101818084717  \n",
            "train iteration 35:loss 40.268856048583984  \n",
            "train iteration 36:loss 41.725998878479004  \n",
            "train iteration 37:loss 42.49624443054199  \n",
            "train iteration 38:loss 43.48016548156738  \n",
            "train iteration 39:loss 44.48128414154053  \n",
            "train iteration 40:loss 45.28306198120117  \n",
            "train iteration 41:loss 46.54848289489746  \n",
            "train iteration 42:loss 47.14089298248291  \n",
            "train iteration 43:loss 47.88374900817871  \n",
            "train iteration 44:loss 48.64234256744385  \n",
            "train iteration 45:loss 49.9923038482666  \n",
            "train iteration 46:loss 53.57048177719116  \n",
            "train iteration 47:loss 54.24072027206421  \n",
            "train iteration 48:loss 55.19653940200806  \n",
            "train iteration 49:loss 56.05340623855591  \n",
            "train iteration 50:loss 56.94918394088745  \n",
            "train iteration 51:loss 57.83756875991821  \n",
            "train iteration 52:loss 58.777199268341064  \n",
            "train iteration 53:loss 60.7194037437439  \n",
            "train iteration 54:loss 61.49908685684204  \n",
            "train iteration 55:loss 62.35447931289673  \n",
            "train iteration 56:loss 63.20700025558472  \n",
            "train iteration 57:loss 63.95982027053833  \n",
            "train iteration 58:loss 64.6349835395813  \n",
            "train iteration 59:loss 65.66034746170044  \n",
            "train iteration 60:loss 66.49875783920288  \n",
            "train iteration 61:loss 67.42773389816284  \n",
            "train iteration 62:loss 68.12292814254761  \n",
            "train iteration 63:loss 69.37009000778198  \n",
            "train iteration 64:loss 70.20193243026733  \n",
            "train iteration 65:loss 70.97231245040894  \n",
            "train iteration 66:loss 71.78938341140747  \n",
            "train iteration 67:loss 72.5924973487854  \n",
            "train iteration 68:loss 73.51708555221558  \n",
            "train iteration 69:loss 74.30489015579224  \n",
            "train iteration 70:loss 75.13272905349731  \n",
            "train iteration 71:loss 75.65679788589478  \n",
            "train iteration 72:loss 76.59167051315308  \n",
            "train iteration 73:loss 77.42251253128052  \n",
            "train iteration 74:loss 78.26109743118286  \n",
            "train iteration 75:loss 79.12758207321167  \n",
            "train iteration 76:loss 84.41506719589233  \n",
            "train iteration 77:loss 85.24139451980591  \n",
            "train iteration 78:loss 85.8179259300232  \n",
            "train iteration 79:loss 87.17670202255249  \n",
            "train iteration 80:loss 88.2236704826355  \n",
            "train iteration 81:loss 89.00541353225708  \n",
            "train iteration 82:loss 89.79733228683472  \n",
            "train iteration 83:loss 90.6902813911438  \n",
            "train iteration 84:loss 91.53726625442505  \n",
            "train iteration 85:loss 94.73383092880249  \n",
            "train iteration 86:loss 95.97361135482788  \n",
            "train iteration 87:loss 99.65736436843872  \n",
            "train iteration 88:loss 100.67794370651245  \n",
            "train iteration 89:loss 101.5421347618103  \n",
            "train iteration 90:loss 102.25239896774292  \n",
            "train iteration 91:loss 103.02430009841919  \n",
            "train iteration 92:loss 103.96042108535767  \n",
            "train iteration 93:loss 104.58852243423462  \n",
            "train iteration 94:loss 105.406014919281  \n",
            "train iteration 95:loss 109.82167673110962  \n",
            "train iteration 96:loss 110.59635782241821  \n",
            "train iteration 97:loss 111.43485498428345  \n",
            "train iteration 98:loss 112.1045355796814  \n",
            "train iteration 99:loss 113.12351369857788  \n",
            "train iteration 100:loss 113.73652792349458  \n",
            "train Loss: 0.0178 Acc: 0.9984\n",
            "val iteration 1:loss 38.53196334838867  \n",
            "val iteration 2:loss 83.8841323852539  \n",
            "val iteration 3:loss 122.8887825012207  \n",
            "val iteration 4:loss 146.42076301574707  \n",
            "val iteration 5:loss 175.318265914917  \n",
            "val iteration 6:loss 209.52869987487793  \n",
            "val iteration 7:loss 233.37616157531738  \n",
            "val iteration 8:loss 265.56970024108887  \n",
            "val iteration 9:loss 294.10509300231934  \n",
            "val iteration 10:loss 321.47498893737793  \n",
            "val iteration 11:loss 361.8892192840576  \n",
            "val iteration 12:loss 387.1479244232178  \n",
            "val iteration 13:loss 415.2868709564209  \n",
            "val iteration 14:loss 456.6901226043701  \n",
            "val iteration 15:loss 497.188383102417  \n",
            "val iteration 16:loss 539.6253681182861  \n",
            "val iteration 17:loss 558.492015838623  \n",
            "val iteration 18:loss 599.5445213317871  \n",
            "val iteration 19:loss 637.7430458068848  \n",
            "val iteration 20:loss 677.9853324890137  \n",
            "val iteration 21:loss 697.722900390625  \n",
            "val iteration 22:loss 727.996660232544  \n",
            "val iteration 23:loss 761.907190322876  \n",
            "val iteration 24:loss 785.7847805023193  \n",
            "val iteration 25:loss 811.0160045623779  \n",
            "val iteration 26:loss 860.153112411499  \n",
            "val iteration 27:loss 870.8103113174438  \n",
            "val iteration 28:loss 903.2894945144653  \n",
            "val iteration 29:loss 926.1013555526733  \n",
            "val iteration 30:loss 969.1222333908081  \n",
            "val iteration 31:loss 1009.4814748764038  \n",
            "val iteration 32:loss 1043.8628797531128  \n",
            "val iteration 33:loss 1071.6531591415405  \n",
            "val iteration 34:loss 1105.0052938461304  \n",
            "val iteration 35:loss 1147.3647680282593  \n",
            "val iteration 36:loss 1176.2518396377563  \n",
            "val iteration 37:loss 1205.9018754959106  \n",
            "val iteration 38:loss 1237.343342781067  \n",
            "val iteration 39:loss 1277.1209497451782  \n",
            "val iteration 40:loss 1311.2311143875122  \n",
            "val iteration 41:loss 1341.7023305892944  \n",
            "val iteration 42:loss 1368.0299978256226  \n",
            "val iteration 43:loss 1409.7855024337769  \n",
            "val iteration 44:loss 1435.8165712356567  \n",
            "val iteration 45:loss 1455.9522943496704  \n",
            "val iteration 46:loss 1470.1394519805908  \n",
            "val iteration 47:loss 1503.7893466949463  \n",
            "val iteration 48:loss 1542.8289546966553  \n",
            "val iteration 49:loss 1603.701410293579  \n",
            "val iteration 50:loss 1633.8622779846191  \n",
            "val iteration 51:loss 1664.4845027923584  \n",
            "val iteration 52:loss 1692.2546215057373  \n",
            "val iteration 53:loss 1749.6464290618896  \n",
            "val iteration 54:loss 1775.449291229248  \n",
            "val iteration 55:loss 1796.76535987854  \n",
            "val iteration 56:loss 1810.2084426879883  \n",
            "val Loss: 1.0262 Acc: 0.7404\n",
            "train iteration 1:loss 0.7032022476196289  \n",
            "train iteration 2:loss 1.2225875854492188  \n",
            "train iteration 3:loss 1.9230918884277344  \n",
            "train iteration 4:loss 2.7958431243896484  \n",
            "train iteration 5:loss 3.564059257507324  \n",
            "train iteration 6:loss 4.179483413696289  \n",
            "train iteration 7:loss 4.864245414733887  \n",
            "train iteration 8:loss 5.882153511047363  \n",
            "train iteration 9:loss 6.66463565826416  \n",
            "train iteration 10:loss 7.3840837478637695  \n",
            "train iteration 11:loss 7.996097564697266  \n",
            "train iteration 12:loss 8.749634742736816  \n",
            "train iteration 13:loss 9.482405662536621  \n",
            "train iteration 14:loss 10.105660438537598  \n",
            "train iteration 15:loss 10.75537109375  \n",
            "train iteration 16:loss 11.420949935913086  \n",
            "train iteration 17:loss 12.497480392456055  \n",
            "train iteration 18:loss 13.295900344848633  \n",
            "train iteration 19:loss 14.04146957397461  \n",
            "train iteration 20:loss 14.739461898803711  \n",
            "train iteration 21:loss 15.523616790771484  \n",
            "train iteration 22:loss 16.193058013916016  \n",
            "train iteration 23:loss 16.937946319580078  \n",
            "train iteration 24:loss 17.616029739379883  \n",
            "train iteration 25:loss 18.314574241638184  \n",
            "train iteration 26:loss 21.21540069580078  \n",
            "train iteration 27:loss 21.85463809967041  \n",
            "train iteration 28:loss 23.492709159851074  \n",
            "train iteration 29:loss 24.23874282836914  \n",
            "train iteration 30:loss 25.075355529785156  \n",
            "train iteration 31:loss 25.54618549346924  \n",
            "train iteration 32:loss 26.28865146636963  \n",
            "train iteration 33:loss 27.0405855178833  \n",
            "train iteration 34:loss 27.82713222503662  \n",
            "train iteration 35:loss 28.76103401184082  \n",
            "train iteration 36:loss 29.304325103759766  \n",
            "train iteration 37:loss 29.985743522644043  \n",
            "train iteration 38:loss 30.724365234375  \n",
            "train iteration 39:loss 32.77404022216797  \n",
            "train iteration 40:loss 37.12023210525513  \n",
            "train iteration 41:loss 37.52438974380493  \n",
            "train iteration 42:loss 38.062355518341064  \n",
            "train iteration 43:loss 38.72834539413452  \n",
            "train iteration 44:loss 39.527063846588135  \n",
            "train iteration 45:loss 40.259984493255615  \n",
            "train iteration 46:loss 43.9998517036438  \n",
            "train iteration 47:loss 44.87053728103638  \n",
            "train iteration 48:loss 45.585073947906494  \n",
            "train iteration 49:loss 49.234365940093994  \n",
            "train iteration 50:loss 50.37785291671753  \n",
            "train iteration 51:loss 51.240869998931885  \n",
            "train iteration 52:loss 51.87044286727905  \n",
            "train iteration 53:loss 52.62089490890503  \n",
            "train iteration 54:loss 53.35658121109009  \n",
            "train iteration 55:loss 53.95729970932007  \n",
            "train iteration 56:loss 54.90796995162964  \n",
            "train iteration 57:loss 55.65724802017212  \n",
            "train iteration 58:loss 56.4598708152771  \n",
            "train iteration 59:loss 57.54254198074341  \n",
            "train iteration 60:loss 59.53113794326782  \n",
            "train iteration 61:loss 61.90499258041382  \n",
            "train iteration 62:loss 62.84660291671753  \n",
            "train iteration 63:loss 63.6493239402771  \n",
            "train iteration 64:loss 64.23323774337769  \n",
            "train iteration 65:loss 65.45994329452515  \n",
            "train iteration 66:loss 66.12994813919067  \n",
            "train iteration 67:loss 66.7969298362732  \n",
            "train iteration 68:loss 67.56103563308716  \n",
            "train iteration 69:loss 70.64157247543335  \n",
            "train iteration 70:loss 71.32603693008423  \n",
            "train iteration 71:loss 72.43748998641968  \n",
            "train iteration 72:loss 73.0710711479187  \n",
            "train iteration 73:loss 73.85724973678589  \n",
            "train iteration 74:loss 74.46785974502563  \n",
            "train iteration 75:loss 75.14957761764526  \n",
            "train iteration 76:loss 76.17346239089966  \n",
            "train iteration 77:loss 76.94402360916138  \n",
            "train iteration 78:loss 77.80511522293091  \n",
            "train iteration 79:loss 78.47834253311157  \n",
            "train iteration 80:loss 81.76162767410278  \n",
            "train iteration 81:loss 83.11592245101929  \n",
            "train iteration 82:loss 83.86486673355103  \n",
            "train iteration 83:loss 84.67541360855103  \n",
            "train iteration 84:loss 86.34335660934448  \n",
            "train iteration 85:loss 86.87524175643921  \n",
            "train iteration 86:loss 87.62915086746216  \n",
            "train iteration 87:loss 88.35010194778442  \n",
            "train iteration 88:loss 89.34111070632935  \n",
            "train iteration 89:loss 90.04828023910522  \n",
            "train iteration 90:loss 93.52066373825073  \n",
            "train iteration 91:loss 94.23328924179077  \n",
            "train iteration 92:loss 94.85823678970337  \n",
            "train iteration 93:loss 95.57235193252563  \n",
            "train iteration 94:loss 96.20501565933228  \n",
            "train iteration 95:loss 96.79777574539185  \n",
            "train iteration 96:loss 97.98371648788452  \n",
            "train iteration 97:loss 98.65142869949341  \n",
            "train iteration 98:loss 99.45679521560669  \n",
            "train iteration 99:loss 100.19440221786499  \n",
            "train iteration 100:loss 100.79109241068363  \n",
            "train Loss: 0.0158 Acc: 0.9984\n",
            "val iteration 1:loss 21.155841827392578  \n",
            "val iteration 2:loss 42.34086799621582  \n",
            "val iteration 3:loss 66.5661506652832  \n",
            "val iteration 4:loss 128.47872924804688  \n",
            "val iteration 5:loss 179.94468688964844  \n",
            "val iteration 6:loss 234.20329666137695  \n",
            "val iteration 7:loss 264.63494873046875  \n",
            "val iteration 8:loss 287.7911682128906  \n",
            "val iteration 9:loss 323.27624130249023  \n",
            "val iteration 10:loss 355.372802734375  \n",
            "val iteration 11:loss 385.75048065185547  \n",
            "val iteration 12:loss 414.0024604797363  \n",
            "val iteration 13:loss 442.98516845703125  \n",
            "val iteration 14:loss 453.37113857269287  \n",
            "val iteration 15:loss 488.0799169540405  \n",
            "val iteration 16:loss 518.5118474960327  \n",
            "val iteration 17:loss 546.9526853561401  \n",
            "val iteration 18:loss 574.7304849624634  \n",
            "val iteration 19:loss 592.3750925064087  \n",
            "val iteration 20:loss 638.1013193130493  \n",
            "val iteration 21:loss 666.8400926589966  \n",
            "val iteration 22:loss 707.538857460022  \n",
            "val iteration 23:loss 754.110463142395  \n",
            "val iteration 24:loss 786.2066888809204  \n",
            "val iteration 25:loss 831.1543741226196  \n",
            "val iteration 26:loss 871.9661455154419  \n",
            "val iteration 27:loss 909.1146974563599  \n",
            "val iteration 28:loss 941.9714555740356  \n",
            "val iteration 29:loss 960.0030088424683  \n",
            "val iteration 30:loss 989.665961265564  \n",
            "val iteration 31:loss 1013.3504400253296  \n",
            "val iteration 32:loss 1052.5500707626343  \n",
            "val iteration 33:loss 1071.5863332748413  \n",
            "val iteration 34:loss 1092.591685295105  \n",
            "val iteration 35:loss 1117.180432319641  \n",
            "val iteration 36:loss 1163.3682870864868  \n",
            "val iteration 37:loss 1182.8654108047485  \n",
            "val iteration 38:loss 1230.9457483291626  \n",
            "val iteration 39:loss 1265.1830606460571  \n",
            "val iteration 40:loss 1306.3939714431763  \n",
            "val iteration 41:loss 1352.583104133606  \n",
            "val iteration 42:loss 1397.3562097549438  \n",
            "val iteration 43:loss 1425.6157331466675  \n",
            "val iteration 44:loss 1456.2768926620483  \n",
            "val iteration 45:loss 1488.4593076705933  \n",
            "val iteration 46:loss 1508.9428606033325  \n",
            "val iteration 47:loss 1520.960916519165  \n",
            "val iteration 48:loss 1558.7856578826904  \n",
            "val iteration 49:loss 1583.7442035675049  \n",
            "val iteration 50:loss 1630.9869785308838  \n",
            "val iteration 51:loss 1659.9067001342773  \n",
            "val iteration 52:loss 1707.4312362670898  \n",
            "val iteration 53:loss 1748.0851593017578  \n",
            "val iteration 54:loss 1780.1376342773438  \n",
            "val iteration 55:loss 1797.204984664917  \n",
            "val iteration 56:loss 1799.7543153762817  \n",
            "val Loss: 1.0203 Acc: 0.7449\n",
            "train iteration 1:loss 1.149094581604004  \n",
            "train iteration 2:loss 1.7715673446655273  \n",
            "train iteration 3:loss 2.5329456329345703  \n",
            "train iteration 4:loss 3.2535276412963867  \n",
            "train iteration 5:loss 5.349892616271973  \n",
            "train iteration 6:loss 6.085964202880859  \n",
            "train iteration 7:loss 6.83668327331543  \n",
            "train iteration 8:loss 7.558135986328125  \n",
            "train iteration 9:loss 8.196006774902344  \n",
            "train iteration 10:loss 8.893974304199219  \n",
            "train iteration 11:loss 9.59259033203125  \n",
            "train iteration 12:loss 10.830400466918945  \n",
            "train iteration 13:loss 11.704641342163086  \n",
            "train iteration 14:loss 12.503121376037598  \n",
            "train iteration 15:loss 13.080638885498047  \n",
            "train iteration 16:loss 13.695298194885254  \n",
            "train iteration 17:loss 14.235258102416992  \n",
            "train iteration 18:loss 14.70666790008545  \n",
            "train iteration 19:loss 15.559029579162598  \n",
            "train iteration 20:loss 16.188003540039062  \n",
            "train iteration 21:loss 16.825387001037598  \n",
            "train iteration 22:loss 17.84032917022705  \n",
            "train iteration 23:loss 18.930744171142578  \n",
            "train iteration 24:loss 19.486884117126465  \n",
            "train iteration 25:loss 20.135616302490234  \n",
            "train iteration 26:loss 20.91004467010498  \n",
            "train iteration 27:loss 21.402742385864258  \n",
            "train iteration 28:loss 22.310163497924805  \n",
            "train iteration 29:loss 22.954323768615723  \n",
            "train iteration 30:loss 23.589701652526855  \n",
            "train iteration 31:loss 24.1867036819458  \n",
            "train iteration 32:loss 25.08806800842285  \n",
            "train iteration 33:loss 30.39095115661621  \n",
            "train iteration 34:loss 32.22276210784912  \n",
            "train iteration 35:loss 32.68022060394287  \n",
            "train iteration 36:loss 33.37809371948242  \n",
            "train iteration 37:loss 33.94464683532715  \n",
            "train iteration 38:loss 34.49889850616455  \n",
            "train iteration 39:loss 35.166131019592285  \n",
            "train iteration 40:loss 36.066086769104004  \n",
            "train iteration 41:loss 36.62501907348633  \n",
            "train iteration 42:loss 37.36258506774902  \n",
            "train iteration 43:loss 38.170796394348145  \n",
            "train iteration 44:loss 38.72338104248047  \n",
            "train iteration 45:loss 39.49580669403076  \n",
            "train iteration 46:loss 40.01479911804199  \n",
            "train iteration 47:loss 40.60515022277832  \n",
            "train iteration 48:loss 44.156596183776855  \n",
            "train iteration 49:loss 45.1695613861084  \n",
            "train iteration 50:loss 45.65640449523926  \n",
            "train iteration 51:loss 46.19626426696777  \n",
            "train iteration 52:loss 46.66183090209961  \n",
            "train iteration 53:loss 49.7974328994751  \n",
            "train iteration 54:loss 50.5699520111084  \n",
            "train iteration 55:loss 51.47322940826416  \n",
            "train iteration 56:loss 52.15838146209717  \n",
            "train iteration 57:loss 52.66264820098877  \n",
            "train iteration 58:loss 53.33812618255615  \n",
            "train iteration 59:loss 54.2192497253418  \n",
            "train iteration 60:loss 54.94809913635254  \n",
            "train iteration 61:loss 55.838674545288086  \n",
            "train iteration 62:loss 57.21022891998291  \n",
            "train iteration 63:loss 57.947011947631836  \n",
            "train iteration 64:loss 58.701050758361816  \n",
            "train iteration 65:loss 59.59821796417236  \n",
            "train iteration 66:loss 60.10860252380371  \n",
            "train iteration 67:loss 60.732967376708984  \n",
            "train iteration 68:loss 61.384888648986816  \n",
            "train iteration 69:loss 62.44441890716553  \n",
            "train iteration 70:loss 63.021681785583496  \n",
            "train iteration 71:loss 63.48303413391113  \n",
            "train iteration 72:loss 64.02793884277344  \n",
            "train iteration 73:loss 64.54605865478516  \n",
            "train iteration 74:loss 65.12535095214844  \n",
            "train iteration 75:loss 65.64623355865479  \n",
            "train iteration 76:loss 66.27660083770752  \n",
            "train iteration 77:loss 66.90522575378418  \n",
            "train iteration 78:loss 67.75258922576904  \n",
            "train iteration 79:loss 68.44266986846924  \n",
            "train iteration 80:loss 69.10933876037598  \n",
            "train iteration 81:loss 69.77252674102783  \n",
            "train iteration 82:loss 70.36188411712646  \n",
            "train iteration 83:loss 71.03047847747803  \n",
            "train iteration 84:loss 71.57218837738037  \n",
            "train iteration 85:loss 72.39965343475342  \n",
            "train iteration 86:loss 73.36352443695068  \n",
            "train iteration 87:loss 77.32726573944092  \n",
            "train iteration 88:loss 82.24477481842041  \n",
            "train iteration 89:loss 83.05911445617676  \n",
            "train iteration 90:loss 85.93734931945801  \n",
            "train iteration 91:loss 90.16737651824951  \n",
            "train iteration 92:loss 90.98778915405273  \n",
            "train iteration 93:loss 91.53414821624756  \n",
            "train iteration 94:loss 91.99714183807373  \n",
            "train iteration 95:loss 92.69817161560059  \n",
            "train iteration 96:loss 93.35253524780273  \n",
            "train iteration 97:loss 93.94770050048828  \n",
            "train iteration 98:loss 96.0971269607544  \n",
            "train iteration 99:loss 96.86770534515381  \n",
            "train iteration 100:loss 97.41971684992313  \n",
            "train Loss: 0.0153 Acc: 0.9981\n",
            "val iteration 1:loss 32.04393005371094  \n",
            "val iteration 2:loss 69.72473907470703  \n",
            "val iteration 3:loss 95.53991317749023  \n",
            "val iteration 4:loss 118.79450798034668  \n",
            "val iteration 5:loss 142.78395462036133  \n",
            "val iteration 6:loss 173.26902389526367  \n",
            "val iteration 7:loss 205.6110610961914  \n",
            "val iteration 8:loss 249.90179824829102  \n",
            "val iteration 9:loss 280.4309196472168  \n",
            "val iteration 10:loss 311.90232849121094  \n",
            "val iteration 11:loss 352.47493743896484  \n",
            "val iteration 12:loss 390.9476623535156  \n",
            "val iteration 13:loss 438.02368927001953  \n",
            "val iteration 14:loss 480.5744323730469  \n",
            "val iteration 15:loss 516.8984222412109  \n",
            "val iteration 16:loss 544.897819519043  \n",
            "val iteration 17:loss 579.0196647644043  \n",
            "val iteration 18:loss 613.4994468688965  \n",
            "val iteration 19:loss 647.5336227416992  \n",
            "val iteration 20:loss 681.5470123291016  \n",
            "val iteration 21:loss 708.3824577331543  \n",
            "val iteration 22:loss 741.4180870056152  \n",
            "val iteration 23:loss 787.9093971252441  \n",
            "val iteration 24:loss 828.9068031311035  \n",
            "val iteration 25:loss 870.5314979553223  \n",
            "val iteration 26:loss 893.2329444885254  \n",
            "val iteration 27:loss 919.4668083190918  \n",
            "val iteration 28:loss 944.752347946167  \n",
            "val iteration 29:loss 975.1688632965088  \n",
            "val iteration 30:loss 999.0833702087402  \n",
            "val iteration 31:loss 1019.5319557189941  \n",
            "val iteration 32:loss 1040.6670265197754  \n",
            "val iteration 33:loss 1071.912893295288  \n",
            "val iteration 34:loss 1088.5171508789062  \n",
            "val iteration 35:loss 1124.3284339904785  \n",
            "val iteration 36:loss 1153.7149810791016  \n",
            "val iteration 37:loss 1196.6115989685059  \n",
            "val iteration 38:loss 1241.8781929016113  \n",
            "val iteration 39:loss 1267.044994354248  \n",
            "val iteration 40:loss 1296.7824420928955  \n",
            "val iteration 41:loss 1342.856767654419  \n",
            "val iteration 42:loss 1394.1349620819092  \n",
            "val iteration 43:loss 1425.445297241211  \n",
            "val iteration 44:loss 1457.0648822784424  \n",
            "val iteration 45:loss 1497.4944286346436  \n",
            "val iteration 46:loss 1516.5011043548584  \n",
            "val iteration 47:loss 1553.2467708587646  \n",
            "val iteration 48:loss 1577.0243015289307  \n",
            "val iteration 49:loss 1600.5638980865479  \n",
            "val iteration 50:loss 1635.5613193511963  \n",
            "val iteration 51:loss 1678.4092845916748  \n",
            "val iteration 52:loss 1725.0956020355225  \n",
            "val iteration 53:loss 1749.440465927124  \n",
            "val iteration 54:loss 1788.0573253631592  \n",
            "val iteration 55:loss 1820.8168087005615  \n",
            "val iteration 56:loss 1827.2767243385315  \n",
            "val Loss: 1.0359 Acc: 0.7466\n",
            "train iteration 1:loss 0.9407148361206055  \n",
            "train iteration 2:loss 1.3547563552856445  \n",
            "train iteration 3:loss 2.106062889099121  \n",
            "train iteration 4:loss 2.7983522415161133  \n",
            "train iteration 5:loss 3.3776350021362305  \n",
            "train iteration 6:loss 4.829320907592773  \n",
            "train iteration 7:loss 5.683661460876465  \n",
            "train iteration 8:loss 6.081393241882324  \n",
            "train iteration 9:loss 6.592462539672852  \n",
            "train iteration 10:loss 7.294938087463379  \n",
            "train iteration 11:loss 8.043903350830078  \n",
            "train iteration 12:loss 8.64421558380127  \n",
            "train iteration 13:loss 9.709675788879395  \n",
            "train iteration 14:loss 10.386302947998047  \n",
            "train iteration 15:loss 10.902482032775879  \n",
            "train iteration 16:loss 14.05284309387207  \n",
            "train iteration 17:loss 14.68364143371582  \n",
            "train iteration 18:loss 15.166144371032715  \n",
            "train iteration 19:loss 15.957064628601074  \n",
            "train iteration 20:loss 16.82114315032959  \n",
            "train iteration 21:loss 17.508752822875977  \n",
            "train iteration 22:loss 18.504950523376465  \n",
            "train iteration 23:loss 19.182059288024902  \n",
            "train iteration 24:loss 19.640355110168457  \n",
            "train iteration 25:loss 20.292418479919434  \n",
            "train iteration 26:loss 20.965639114379883  \n",
            "train iteration 27:loss 21.664889335632324  \n",
            "train iteration 28:loss 22.27680206298828  \n",
            "train iteration 29:loss 22.92716407775879  \n",
            "train iteration 30:loss 23.49245834350586  \n",
            "train iteration 31:loss 24.131561279296875  \n",
            "train iteration 32:loss 24.474356651306152  \n",
            "train iteration 33:loss 25.086305618286133  \n",
            "train iteration 34:loss 25.932364463806152  \n",
            "train iteration 35:loss 26.512516975402832  \n",
            "train iteration 36:loss 27.167051315307617  \n",
            "train iteration 37:loss 28.014180183410645  \n",
            "train iteration 38:loss 28.57893657684326  \n",
            "train iteration 39:loss 29.321706771850586  \n",
            "train iteration 40:loss 30.00532054901123  \n",
            "train iteration 41:loss 31.527507781982422  \n",
            "train iteration 42:loss 32.0609712600708  \n",
            "train iteration 43:loss 32.87031841278076  \n",
            "train iteration 44:loss 33.593716621398926  \n",
            "train iteration 45:loss 35.09295463562012  \n",
            "train iteration 46:loss 35.6642427444458  \n",
            "train iteration 47:loss 36.92569160461426  \n",
            "train iteration 48:loss 37.53944778442383  \n",
            "train iteration 49:loss 38.120731353759766  \n",
            "train iteration 50:loss 38.79512310028076  \n",
            "train iteration 51:loss 39.21815872192383  \n",
            "train iteration 52:loss 40.09179210662842  \n",
            "train iteration 53:loss 40.68943977355957  \n",
            "train iteration 54:loss 41.19194316864014  \n",
            "train iteration 55:loss 41.640628814697266  \n",
            "train iteration 56:loss 42.24344062805176  \n",
            "train iteration 57:loss 43.19647026062012  \n",
            "train iteration 58:loss 43.78373908996582  \n",
            "train iteration 59:loss 44.2992582321167  \n",
            "train iteration 60:loss 44.86009979248047  \n",
            "train iteration 61:loss 45.357224464416504  \n",
            "train iteration 62:loss 45.90364074707031  \n",
            "train iteration 63:loss 46.72063159942627  \n",
            "train iteration 64:loss 47.405089378356934  \n",
            "train iteration 65:loss 49.547651290893555  \n",
            "train iteration 66:loss 50.077009201049805  \n",
            "train iteration 67:loss 51.17020225524902  \n",
            "train iteration 68:loss 51.823588371276855  \n",
            "train iteration 69:loss 52.569801330566406  \n",
            "train iteration 70:loss 53.15385723114014  \n",
            "train iteration 71:loss 54.05539131164551  \n",
            "train iteration 72:loss 54.87287616729736  \n",
            "train iteration 73:loss 55.324984550476074  \n",
            "train iteration 74:loss 55.99775218963623  \n",
            "train iteration 75:loss 56.679582595825195  \n",
            "train iteration 76:loss 57.30760860443115  \n",
            "train iteration 77:loss 58.12662982940674  \n",
            "train iteration 78:loss 58.68042182922363  \n",
            "train iteration 79:loss 59.2935152053833  \n",
            "train iteration 80:loss 62.96712684631348  \n",
            "train iteration 81:loss 63.76684093475342  \n",
            "train iteration 82:loss 66.94906520843506  \n",
            "train iteration 83:loss 67.58482646942139  \n",
            "train iteration 84:loss 68.11964702606201  \n",
            "train iteration 85:loss 68.64429092407227  \n",
            "train iteration 86:loss 69.33191013336182  \n",
            "train iteration 87:loss 72.43704223632812  \n",
            "train iteration 88:loss 72.97210788726807  \n",
            "train iteration 89:loss 73.43856239318848  \n",
            "train iteration 90:loss 74.21283721923828  \n",
            "train iteration 91:loss 74.81195068359375  \n",
            "train iteration 92:loss 75.33876132965088  \n",
            "train iteration 93:loss 75.84383964538574  \n",
            "train iteration 94:loss 76.33638000488281  \n",
            "train iteration 95:loss 79.774169921875  \n",
            "train iteration 96:loss 80.22366333007812  \n",
            "train iteration 97:loss 81.08915042877197  \n",
            "train iteration 98:loss 83.77514362335205  \n",
            "train iteration 99:loss 84.35960865020752  \n",
            "train iteration 100:loss 85.21112534403801  \n",
            "train Loss: 0.0134 Acc: 0.9987\n",
            "val iteration 1:loss 22.68804931640625  \n",
            "val iteration 2:loss 58.529354095458984  \n",
            "val iteration 3:loss 102.40619659423828  \n",
            "val iteration 4:loss 133.48658561706543  \n",
            "val iteration 5:loss 161.61070823669434  \n",
            "val iteration 6:loss 192.60061073303223  \n",
            "val iteration 7:loss 226.4867458343506  \n",
            "val iteration 8:loss 256.4753818511963  \n",
            "val iteration 9:loss 310.76899909973145  \n",
            "val iteration 10:loss 338.91466903686523  \n",
            "val iteration 11:loss 371.2034721374512  \n",
            "val iteration 12:loss 405.1785545349121  \n",
            "val iteration 13:loss 456.7635154724121  \n",
            "val iteration 14:loss 476.52667236328125  \n",
            "val iteration 15:loss 500.44324493408203  \n",
            "val iteration 16:loss 536.3680763244629  \n",
            "val iteration 17:loss 574.7570152282715  \n",
            "val iteration 18:loss 605.7284297943115  \n",
            "val iteration 19:loss 639.558012008667  \n",
            "val iteration 20:loss 670.8974266052246  \n",
            "val iteration 21:loss 705.3130874633789  \n",
            "val iteration 22:loss 730.3953170776367  \n",
            "val iteration 23:loss 773.9665222167969  \n",
            "val iteration 24:loss 811.6839256286621  \n",
            "val iteration 25:loss 849.9050941467285  \n",
            "val iteration 26:loss 880.8621711730957  \n",
            "val iteration 27:loss 919.3362693786621  \n",
            "val iteration 28:loss 956.9261283874512  \n",
            "val iteration 29:loss 988.8591194152832  \n",
            "val iteration 30:loss 1015.2347717285156  \n",
            "val iteration 31:loss 1054.9935264587402  \n",
            "val iteration 32:loss 1078.1682434082031  \n",
            "val iteration 33:loss 1118.8840522766113  \n",
            "val iteration 34:loss 1160.8035736083984  \n",
            "val iteration 35:loss 1214.026611328125  \n",
            "val iteration 36:loss 1246.5603065490723  \n",
            "val iteration 37:loss 1276.7910957336426  \n",
            "val iteration 38:loss 1307.2085609436035  \n",
            "val iteration 39:loss 1327.4846992492676  \n",
            "val iteration 40:loss 1367.6781730651855  \n",
            "val iteration 41:loss 1398.9590072631836  \n",
            "val iteration 42:loss 1434.6821746826172  \n",
            "val iteration 43:loss 1466.448558807373  \n",
            "val iteration 44:loss 1505.3002395629883  \n",
            "val iteration 45:loss 1556.7049407958984  \n",
            "val iteration 46:loss 1574.301254272461  \n",
            "val iteration 47:loss 1603.74977684021  \n",
            "val iteration 48:loss 1628.1131496429443  \n",
            "val iteration 49:loss 1652.6091270446777  \n",
            "val iteration 50:loss 1690.3558387756348  \n",
            "val iteration 51:loss 1727.1447525024414  \n",
            "val iteration 52:loss 1758.655668258667  \n",
            "val iteration 53:loss 1787.843225479126  \n",
            "val iteration 54:loss 1822.551664352417  \n",
            "val iteration 55:loss 1848.6332187652588  \n",
            "val iteration 56:loss 1849.2544050216675  \n",
            "val Loss: 1.0483 Acc: 0.7375\n",
            "train iteration 1:loss 0.8475685119628906  \n",
            "train iteration 2:loss 1.412618637084961  \n",
            "train iteration 3:loss 1.8851490020751953  \n",
            "train iteration 4:loss 2.4475698471069336  \n",
            "train iteration 5:loss 3.0226526260375977  \n",
            "train iteration 6:loss 3.6827354431152344  \n",
            "train iteration 7:loss 4.274774551391602  \n",
            "train iteration 8:loss 4.975369453430176  \n",
            "train iteration 9:loss 8.975125312805176  \n",
            "train iteration 10:loss 9.647811889648438  \n",
            "train iteration 11:loss 11.482978820800781  \n",
            "train iteration 12:loss 12.054530143737793  \n",
            "train iteration 13:loss 12.85359001159668  \n",
            "train iteration 14:loss 13.377701759338379  \n",
            "train iteration 15:loss 13.96470832824707  \n",
            "train iteration 16:loss 14.584471702575684  \n",
            "train iteration 17:loss 15.306300163269043  \n",
            "train iteration 18:loss 16.172287940979004  \n",
            "train iteration 19:loss 16.81013011932373  \n",
            "train iteration 20:loss 17.48285961151123  \n",
            "train iteration 21:loss 20.685153007507324  \n",
            "train iteration 22:loss 21.267637252807617  \n",
            "train iteration 23:loss 22.04525375366211  \n",
            "train iteration 24:loss 22.5075044631958  \n",
            "train iteration 25:loss 23.020824432373047  \n",
            "train iteration 26:loss 23.708630561828613  \n",
            "train iteration 27:loss 24.181522369384766  \n",
            "train iteration 28:loss 24.8143892288208  \n",
            "train iteration 29:loss 25.33038902282715  \n",
            "train iteration 30:loss 28.758668422698975  \n",
            "train iteration 31:loss 29.365976810455322  \n",
            "train iteration 32:loss 30.002451419830322  \n",
            "train iteration 33:loss 30.517383098602295  \n",
            "train iteration 34:loss 31.453935146331787  \n",
            "train iteration 35:loss 32.0306830406189  \n",
            "train iteration 36:loss 32.821768283843994  \n",
            "train iteration 37:loss 33.473515033721924  \n",
            "train iteration 38:loss 33.992305278778076  \n",
            "train iteration 39:loss 34.75289964675903  \n",
            "train iteration 40:loss 35.411738872528076  \n",
            "train iteration 41:loss 35.97699975967407  \n",
            "train iteration 42:loss 36.4706597328186  \n",
            "train iteration 43:loss 37.01349878311157  \n",
            "train iteration 44:loss 37.7926459312439  \n",
            "train iteration 45:loss 38.397892475128174  \n",
            "train iteration 46:loss 39.08839178085327  \n",
            "train iteration 47:loss 39.609976291656494  \n",
            "train iteration 48:loss 40.62181806564331  \n",
            "train iteration 49:loss 41.1482892036438  \n",
            "train iteration 50:loss 41.61145830154419  \n",
            "train iteration 51:loss 42.164273738861084  \n",
            "train iteration 52:loss 42.721301555633545  \n",
            "train iteration 53:loss 44.12250089645386  \n",
            "train iteration 54:loss 44.63150358200073  \n",
            "train iteration 55:loss 45.25355672836304  \n",
            "train iteration 56:loss 45.723352909088135  \n",
            "train iteration 57:loss 46.148521900177  \n",
            "train iteration 58:loss 46.69179964065552  \n",
            "train iteration 59:loss 47.29240655899048  \n",
            "train iteration 60:loss 48.06462907791138  \n",
            "train iteration 61:loss 48.676528453826904  \n",
            "train iteration 62:loss 55.40812587738037  \n",
            "train iteration 63:loss 55.8823766708374  \n",
            "train iteration 64:loss 56.44839572906494  \n",
            "train iteration 65:loss 57.054871559143066  \n",
            "train iteration 66:loss 57.470587730407715  \n",
            "train iteration 67:loss 58.07600975036621  \n",
            "train iteration 68:loss 58.74075412750244  \n",
            "train iteration 69:loss 59.374576568603516  \n",
            "train iteration 70:loss 59.932108879089355  \n",
            "train iteration 71:loss 60.93222904205322  \n",
            "train iteration 72:loss 62.00084209442139  \n",
            "train iteration 73:loss 62.689735412597656  \n",
            "train iteration 74:loss 63.297667503356934  \n",
            "train iteration 75:loss 64.61041736602783  \n",
            "train iteration 76:loss 65.48686695098877  \n",
            "train iteration 77:loss 66.03209972381592  \n",
            "train iteration 78:loss 66.46180534362793  \n",
            "train iteration 79:loss 67.14026737213135  \n",
            "train iteration 80:loss 68.13918209075928  \n",
            "train iteration 81:loss 68.88482284545898  \n",
            "train iteration 82:loss 69.47294998168945  \n",
            "train iteration 83:loss 71.30196285247803  \n",
            "train iteration 84:loss 71.93651390075684  \n",
            "train iteration 85:loss 72.56752872467041  \n",
            "train iteration 86:loss 73.02937507629395  \n",
            "train iteration 87:loss 73.77717208862305  \n",
            "train iteration 88:loss 76.58742046356201  \n",
            "train iteration 89:loss 77.08281517028809  \n",
            "train iteration 90:loss 77.6724042892456  \n",
            "train iteration 91:loss 78.09234237670898  \n",
            "train iteration 92:loss 78.80475521087646  \n",
            "train iteration 93:loss 79.35894584655762  \n",
            "train iteration 94:loss 80.1310510635376  \n",
            "train iteration 95:loss 80.932448387146  \n",
            "train iteration 96:loss 81.40252304077148  \n",
            "train iteration 97:loss 82.03633689880371  \n",
            "train iteration 98:loss 82.62783908843994  \n",
            "train iteration 99:loss 83.30469417572021  \n",
            "train iteration 100:loss 86.52269366383553  \n",
            "train Loss: 0.0136 Acc: 0.9984\n",
            "val iteration 1:loss 31.386898040771484  \n",
            "val iteration 2:loss 53.671875  \n",
            "val iteration 3:loss 103.52288818359375  \n",
            "val iteration 4:loss 127.56460189819336  \n",
            "val iteration 5:loss 151.1734504699707  \n",
            "val iteration 6:loss 210.99841690063477  \n",
            "val iteration 7:loss 245.75676727294922  \n",
            "val iteration 8:loss 272.60930252075195  \n",
            "val iteration 9:loss 305.81116485595703  \n",
            "val iteration 10:loss 339.16797637939453  \n",
            "val iteration 11:loss 359.9500389099121  \n",
            "val iteration 12:loss 404.6270942687988  \n",
            "val iteration 13:loss 436.3266372680664  \n",
            "val iteration 14:loss 475.43310928344727  \n",
            "val iteration 15:loss 525.2596778869629  \n",
            "val iteration 16:loss 567.4020957946777  \n",
            "val iteration 17:loss 624.8925323486328  \n",
            "val iteration 18:loss 679.958683013916  \n",
            "val iteration 19:loss 704.0355567932129  \n",
            "val iteration 20:loss 740.2282485961914  \n",
            "val iteration 21:loss 781.4040603637695  \n",
            "val iteration 22:loss 803.170202255249  \n",
            "val iteration 23:loss 831.2198276519775  \n",
            "val iteration 24:loss 873.2357959747314  \n",
            "val iteration 25:loss 883.1442890167236  \n",
            "val iteration 26:loss 936.3746013641357  \n",
            "val iteration 27:loss 967.4479808807373  \n",
            "val iteration 28:loss 1002.6848163604736  \n",
            "val iteration 29:loss 1042.7453517913818  \n",
            "val iteration 30:loss 1068.4018478393555  \n",
            "val iteration 31:loss 1104.7622718811035  \n",
            "val iteration 32:loss 1133.7831115722656  \n",
            "val iteration 33:loss 1159.9370803833008  \n",
            "val iteration 34:loss 1179.5140857696533  \n",
            "val iteration 35:loss 1211.141752243042  \n",
            "val iteration 36:loss 1250.2005939483643  \n",
            "val iteration 37:loss 1273.7691383361816  \n",
            "val iteration 38:loss 1291.8296184539795  \n",
            "val iteration 39:loss 1314.0185527801514  \n",
            "val iteration 40:loss 1348.1748218536377  \n",
            "val iteration 41:loss 1376.6457347869873  \n",
            "val iteration 42:loss 1424.3749446868896  \n",
            "val iteration 43:loss 1447.9603519439697  \n",
            "val iteration 44:loss 1496.8087558746338  \n",
            "val iteration 45:loss 1511.9210863113403  \n",
            "val iteration 46:loss 1541.2213106155396  \n",
            "val iteration 47:loss 1565.3761701583862  \n",
            "val iteration 48:loss 1608.1197385787964  \n",
            "val iteration 49:loss 1631.8051595687866  \n",
            "val iteration 50:loss 1675.8936910629272  \n",
            "val iteration 51:loss 1718.1582860946655  \n",
            "val iteration 52:loss 1735.6309938430786  \n",
            "val iteration 53:loss 1754.015643119812  \n",
            "val iteration 54:loss 1773.622628211975  \n",
            "val iteration 55:loss 1793.0542459487915  \n",
            "val iteration 56:loss 1795.4949884414673  \n",
            "val Loss: 1.0179 Acc: 0.7506\n",
            "train iteration 1:loss 0.5739870071411133  \n",
            "train iteration 2:loss 1.2597846984863281  \n",
            "train iteration 3:loss 1.8945436477661133  \n",
            "train iteration 4:loss 2.358278274536133  \n",
            "train iteration 5:loss 3.1010046005249023  \n",
            "train iteration 6:loss 3.6866416931152344  \n",
            "train iteration 7:loss 4.305774688720703  \n",
            "train iteration 8:loss 4.76640510559082  \n",
            "train iteration 9:loss 5.396572113037109  \n",
            "train iteration 10:loss 5.876863479614258  \n",
            "train iteration 11:loss 6.301338195800781  \n",
            "train iteration 12:loss 6.704500198364258  \n",
            "train iteration 13:loss 7.349167823791504  \n",
            "train iteration 14:loss 8.01944637298584  \n",
            "train iteration 15:loss 8.60522747039795  \n",
            "train iteration 16:loss 9.020861625671387  \n",
            "train iteration 17:loss 9.514155387878418  \n",
            "train iteration 18:loss 10.132344245910645  \n",
            "train iteration 19:loss 10.528240203857422  \n",
            "train iteration 20:loss 10.90302848815918  \n",
            "train iteration 21:loss 11.303912162780762  \n",
            "train iteration 22:loss 11.615726470947266  \n",
            "train iteration 23:loss 12.147037506103516  \n",
            "train iteration 24:loss 12.560962677001953  \n",
            "train iteration 25:loss 13.207926750183105  \n",
            "train iteration 26:loss 13.874674797058105  \n",
            "train iteration 27:loss 18.94363784790039  \n",
            "train iteration 28:loss 19.369091987609863  \n",
            "train iteration 29:loss 20.47035503387451  \n",
            "train iteration 30:loss 21.1099214553833  \n",
            "train iteration 31:loss 21.607501983642578  \n",
            "train iteration 32:loss 22.1546049118042  \n",
            "train iteration 33:loss 24.91234016418457  \n",
            "train iteration 34:loss 25.290156364440918  \n",
            "train iteration 35:loss 25.839397430419922  \n",
            "train iteration 36:loss 26.444072723388672  \n",
            "train iteration 37:loss 26.89809513092041  \n",
            "train iteration 38:loss 27.492874145507812  \n",
            "train iteration 39:loss 28.073298454284668  \n",
            "train iteration 40:loss 28.75157070159912  \n",
            "train iteration 41:loss 32.203521728515625  \n",
            "train iteration 42:loss 32.6724967956543  \n",
            "train iteration 43:loss 36.99257755279541  \n",
            "train iteration 44:loss 37.385714530944824  \n",
            "train iteration 45:loss 37.98388957977295  \n",
            "train iteration 46:loss 38.88941287994385  \n",
            "train iteration 47:loss 39.245285987854004  \n",
            "train iteration 48:loss 39.87943458557129  \n",
            "train iteration 49:loss 40.70547103881836  \n",
            "train iteration 50:loss 41.16139602661133  \n",
            "train iteration 51:loss 41.876830101013184  \n",
            "train iteration 52:loss 42.594749450683594  \n",
            "train iteration 53:loss 44.59116268157959  \n",
            "train iteration 54:loss 45.10120391845703  \n",
            "train iteration 55:loss 45.58103370666504  \n",
            "train iteration 56:loss 46.033663749694824  \n",
            "train iteration 57:loss 46.471402168273926  \n",
            "train iteration 58:loss 46.98355770111084  \n",
            "train iteration 59:loss 47.347928047180176  \n",
            "train iteration 60:loss 48.79700946807861  \n",
            "train iteration 61:loss 49.23145580291748  \n",
            "train iteration 62:loss 49.7025089263916  \n",
            "train iteration 63:loss 50.11845874786377  \n",
            "train iteration 64:loss 50.51799488067627  \n",
            "train iteration 65:loss 51.43989181518555  \n",
            "train iteration 66:loss 52.02659893035889  \n",
            "train iteration 67:loss 53.91019058227539  \n",
            "train iteration 68:loss 54.653839111328125  \n",
            "train iteration 69:loss 56.37158393859863  \n",
            "train iteration 70:loss 56.98188781738281  \n",
            "train iteration 71:loss 57.43998336791992  \n",
            "train iteration 72:loss 58.23592662811279  \n",
            "train iteration 73:loss 63.52927112579346  \n",
            "train iteration 74:loss 64.02510070800781  \n",
            "train iteration 75:loss 66.53818035125732  \n",
            "train iteration 76:loss 66.99302196502686  \n",
            "train iteration 77:loss 67.41669273376465  \n",
            "train iteration 78:loss 67.94671535491943  \n",
            "train iteration 79:loss 68.37536239624023  \n",
            "train iteration 80:loss 68.85997295379639  \n",
            "train iteration 81:loss 69.63230991363525  \n",
            "train iteration 82:loss 70.54588603973389  \n",
            "train iteration 83:loss 72.7080078125  \n",
            "train iteration 84:loss 73.12952518463135  \n",
            "train iteration 85:loss 73.61826705932617  \n",
            "train iteration 86:loss 74.05781745910645  \n",
            "train iteration 87:loss 74.67201328277588  \n",
            "train iteration 88:loss 75.2193250656128  \n",
            "train iteration 89:loss 76.11666202545166  \n",
            "train iteration 90:loss 76.5918197631836  \n",
            "train iteration 91:loss 77.54630088806152  \n",
            "train iteration 92:loss 78.24488735198975  \n",
            "train iteration 93:loss 78.84318256378174  \n",
            "train iteration 94:loss 79.4577407836914  \n",
            "train iteration 95:loss 80.02340602874756  \n",
            "train iteration 96:loss 80.59852886199951  \n",
            "train iteration 97:loss 81.27439880371094  \n",
            "train iteration 98:loss 81.88129234313965  \n",
            "train iteration 99:loss 85.32161712646484  \n",
            "train iteration 100:loss 85.693488124758  \n",
            "train Loss: 0.0134 Acc: 0.9980\n",
            "val iteration 1:loss 38.531192779541016  \n",
            "val iteration 2:loss 65.21613693237305  \n",
            "val iteration 3:loss 109.19429397583008  \n",
            "val iteration 4:loss 152.7562255859375  \n",
            "val iteration 5:loss 188.59170532226562  \n",
            "val iteration 6:loss 224.24730682373047  \n",
            "val iteration 7:loss 254.17621612548828  \n",
            "val iteration 8:loss 319.7964859008789  \n",
            "val iteration 9:loss 349.2991695404053  \n",
            "val iteration 10:loss 376.77628898620605  \n",
            "val iteration 11:loss 404.70112800598145  \n",
            "val iteration 12:loss 425.13164138793945  \n",
            "val iteration 13:loss 460.32787704467773  \n",
            "val iteration 14:loss 485.846715927124  \n",
            "val iteration 15:loss 536.5867938995361  \n",
            "val iteration 16:loss 555.9052143096924  \n",
            "val iteration 17:loss 591.109697341919  \n",
            "val iteration 18:loss 624.7440433502197  \n",
            "val iteration 19:loss 644.6339321136475  \n",
            "val iteration 20:loss 684.834882736206  \n",
            "val iteration 21:loss 711.9724864959717  \n",
            "val iteration 22:loss 739.0572986602783  \n",
            "val iteration 23:loss 762.5692920684814  \n",
            "val iteration 24:loss 800.0848789215088  \n",
            "val iteration 25:loss 837.9605350494385  \n",
            "val iteration 26:loss 872.1445331573486  \n",
            "val iteration 27:loss 905.81662940979  \n",
            "val iteration 28:loss 953.4522190093994  \n",
            "val iteration 29:loss 973.9966449737549  \n",
            "val iteration 30:loss 1011.8068943023682  \n",
            "val iteration 31:loss 1044.4462909698486  \n",
            "val iteration 32:loss 1086.112382888794  \n",
            "val iteration 33:loss 1108.9087543487549  \n",
            "val iteration 34:loss 1157.7101306915283  \n",
            "val iteration 35:loss 1195.8221225738525  \n",
            "val iteration 36:loss 1231.1838054656982  \n",
            "val iteration 37:loss 1272.4177837371826  \n",
            "val iteration 38:loss 1293.629358291626  \n",
            "val iteration 39:loss 1315.2995834350586  \n",
            "val iteration 40:loss 1335.1430587768555  \n",
            "val iteration 41:loss 1370.5125465393066  \n",
            "val iteration 42:loss 1416.4659118652344  \n",
            "val iteration 43:loss 1448.2451477050781  \n",
            "val iteration 44:loss 1473.743631362915  \n",
            "val iteration 45:loss 1503.7356777191162  \n",
            "val iteration 46:loss 1519.326488494873  \n",
            "val iteration 47:loss 1556.9639854431152  \n",
            "val iteration 48:loss 1588.8105220794678  \n",
            "val iteration 49:loss 1616.6132793426514  \n",
            "val iteration 50:loss 1655.3377628326416  \n",
            "val iteration 51:loss 1695.4079303741455  \n",
            "val iteration 52:loss 1738.1278133392334  \n",
            "val iteration 53:loss 1760.9057941436768  \n",
            "val iteration 54:loss 1785.70334815979  \n",
            "val iteration 55:loss 1810.8330669403076  \n",
            "val iteration 56:loss 1814.7984466552734  \n",
            "val Loss: 1.0288 Acc: 0.7381\n",
            "train iteration 1:loss 0.45182037353515625  \n",
            "train iteration 2:loss 1.1002750396728516  \n",
            "train iteration 3:loss 1.7389659881591797  \n",
            "train iteration 4:loss 2.2420005798339844  \n",
            "train iteration 5:loss 2.738199234008789  \n",
            "train iteration 6:loss 3.853522300720215  \n",
            "train iteration 7:loss 4.31320858001709  \n",
            "train iteration 8:loss 4.957785606384277  \n",
            "train iteration 9:loss 5.448855400085449  \n",
            "train iteration 10:loss 5.804389953613281  \n",
            "train iteration 11:loss 9.907865524291992  \n",
            "train iteration 12:loss 10.38564682006836  \n",
            "train iteration 13:loss 10.939586639404297  \n",
            "train iteration 14:loss 11.440568923950195  \n",
            "train iteration 15:loss 12.729455947875977  \n",
            "train iteration 16:loss 13.221417427062988  \n",
            "train iteration 17:loss 13.65449333190918  \n",
            "train iteration 18:loss 14.248132705688477  \n",
            "train iteration 19:loss 14.7339506149292  \n",
            "train iteration 20:loss 15.268895149230957  \n",
            "train iteration 21:loss 15.660259246826172  \n",
            "train iteration 22:loss 16.138818740844727  \n",
            "train iteration 23:loss 16.690185546875  \n",
            "train iteration 24:loss 18.236892700195312  \n",
            "train iteration 25:loss 18.698461532592773  \n",
            "train iteration 26:loss 19.666622161865234  \n",
            "train iteration 27:loss 20.130990982055664  \n",
            "train iteration 28:loss 20.51137924194336  \n",
            "train iteration 29:loss 20.944809913635254  \n",
            "train iteration 30:loss 21.990052223205566  \n",
            "train iteration 31:loss 22.829578399658203  \n",
            "train iteration 32:loss 23.385077476501465  \n",
            "train iteration 33:loss 23.970890045166016  \n",
            "train iteration 34:loss 24.456482887268066  \n",
            "train iteration 35:loss 27.190704345703125  \n",
            "train iteration 36:loss 27.650928497314453  \n",
            "train iteration 37:loss 28.319384574890137  \n",
            "train iteration 38:loss 28.759004592895508  \n",
            "train iteration 39:loss 30.844724655151367  \n",
            "train iteration 40:loss 31.7341365814209  \n",
            "train iteration 41:loss 32.2088508605957  \n",
            "train iteration 42:loss 32.52615547180176  \n",
            "train iteration 43:loss 32.943984031677246  \n",
            "train iteration 44:loss 33.72827911376953  \n",
            "train iteration 45:loss 34.13597393035889  \n",
            "train iteration 46:loss 34.542951583862305  \n",
            "train iteration 47:loss 35.05540752410889  \n",
            "train iteration 48:loss 35.35621738433838  \n",
            "train iteration 49:loss 35.82059097290039  \n",
            "train iteration 50:loss 36.393781661987305  \n",
            "train iteration 51:loss 36.89243412017822  \n",
            "train iteration 52:loss 37.34062194824219  \n",
            "train iteration 53:loss 37.77413463592529  \n",
            "train iteration 54:loss 38.777926445007324  \n",
            "train iteration 55:loss 39.19368648529053  \n",
            "train iteration 56:loss 39.56654357910156  \n",
            "train iteration 57:loss 39.99741268157959  \n",
            "train iteration 58:loss 40.33054733276367  \n",
            "train iteration 59:loss 41.83406925201416  \n",
            "train iteration 60:loss 42.917677879333496  \n",
            "train iteration 61:loss 46.24311542510986  \n",
            "train iteration 62:loss 46.70015907287598  \n",
            "train iteration 63:loss 47.140371322631836  \n",
            "train iteration 64:loss 47.94778823852539  \n",
            "train iteration 65:loss 48.34129333496094  \n",
            "train iteration 66:loss 49.75525760650635  \n",
            "train iteration 67:loss 50.17155361175537  \n",
            "train iteration 68:loss 50.80404853820801  \n",
            "train iteration 69:loss 51.3056583404541  \n",
            "train iteration 70:loss 52.601956367492676  \n",
            "train iteration 71:loss 53.06241035461426  \n",
            "train iteration 72:loss 53.45565605163574  \n",
            "train iteration 73:loss 53.94363498687744  \n",
            "train iteration 74:loss 54.467979431152344  \n",
            "train iteration 75:loss 55.01933670043945  \n",
            "train iteration 76:loss 55.56931400299072  \n",
            "train iteration 77:loss 55.98567199707031  \n",
            "train iteration 78:loss 56.54497718811035  \n",
            "train iteration 79:loss 57.11612606048584  \n",
            "train iteration 80:loss 57.608314514160156  \n",
            "train iteration 81:loss 58.047101974487305  \n",
            "train iteration 82:loss 58.44773197174072  \n",
            "train iteration 83:loss 58.9839391708374  \n",
            "train iteration 84:loss 59.48932075500488  \n",
            "train iteration 85:loss 59.919596672058105  \n",
            "train iteration 86:loss 60.468565940856934  \n",
            "train iteration 87:loss 60.89189624786377  \n",
            "train iteration 88:loss 61.37048625946045  \n",
            "train iteration 89:loss 61.82437324523926  \n",
            "train iteration 90:loss 62.49137783050537  \n",
            "train iteration 91:loss 62.995238304138184  \n",
            "train iteration 92:loss 63.62541675567627  \n",
            "train iteration 93:loss 64.16149234771729  \n",
            "train iteration 94:loss 64.70255851745605  \n",
            "train iteration 95:loss 65.27064037322998  \n",
            "train iteration 96:loss 65.73305225372314  \n",
            "train iteration 97:loss 66.25101089477539  \n",
            "train iteration 98:loss 66.64815044403076  \n",
            "train iteration 99:loss 67.03289890289307  \n",
            "train iteration 100:loss 67.3786497041583  \n",
            "train Loss: 0.0106 Acc: 0.9989\n",
            "val iteration 1:loss 43.067230224609375  \n",
            "val iteration 2:loss 88.16453552246094  \n",
            "val iteration 3:loss 118.37352561950684  \n",
            "val iteration 4:loss 157.42804145812988  \n",
            "val iteration 5:loss 183.58395385742188  \n",
            "val iteration 6:loss 227.41885375976562  \n",
            "val iteration 7:loss 254.2675380706787  \n",
            "val iteration 8:loss 284.55967903137207  \n",
            "val iteration 9:loss 311.56374168395996  \n",
            "val iteration 10:loss 346.04801750183105  \n",
            "val iteration 11:loss 379.92098808288574  \n",
            "val iteration 12:loss 409.3958492279053  \n",
            "val iteration 13:loss 435.32074546813965  \n",
            "val iteration 14:loss 461.62247467041016  \n",
            "val iteration 15:loss 495.2128791809082  \n",
            "val iteration 16:loss 512.0371017456055  \n",
            "val iteration 17:loss 549.3355751037598  \n",
            "val iteration 18:loss 575.5703105926514  \n",
            "val iteration 19:loss 590.4768981933594  \n",
            "val iteration 20:loss 610.7563705444336  \n",
            "val iteration 21:loss 645.0681076049805  \n",
            "val iteration 22:loss 670.1378269195557  \n",
            "val iteration 23:loss 697.0757579803467  \n",
            "val iteration 24:loss 727.9266948699951  \n",
            "val iteration 25:loss 752.394567489624  \n",
            "val iteration 26:loss 803.9636554718018  \n",
            "val iteration 27:loss 822.2288341522217  \n",
            "val iteration 28:loss 870.4100170135498  \n",
            "val iteration 29:loss 905.5758419036865  \n",
            "val iteration 30:loss 937.4127082824707  \n",
            "val iteration 31:loss 965.2324409484863  \n",
            "val iteration 32:loss 988.8617706298828  \n",
            "val iteration 33:loss 1025.3262481689453  \n",
            "val iteration 34:loss 1062.830265045166  \n",
            "val iteration 35:loss 1089.6684322357178  \n",
            "val iteration 36:loss 1132.600549697876  \n",
            "val iteration 37:loss 1174.260877609253  \n",
            "val iteration 38:loss 1219.0039310455322  \n",
            "val iteration 39:loss 1260.2935848236084  \n",
            "val iteration 40:loss 1293.1786861419678  \n",
            "val iteration 41:loss 1338.734670639038  \n",
            "val iteration 42:loss 1367.5144958496094  \n",
            "val iteration 43:loss 1407.6268844604492  \n",
            "val iteration 44:loss 1434.1842784881592  \n",
            "val iteration 45:loss 1463.7717666625977  \n",
            "val iteration 46:loss 1509.1393432617188  \n",
            "val iteration 47:loss 1538.8298377990723  \n",
            "val iteration 48:loss 1559.1654930114746  \n",
            "val iteration 49:loss 1613.3846473693848  \n",
            "val iteration 50:loss 1645.9895401000977  \n",
            "val iteration 51:loss 1702.0047073364258  \n",
            "val iteration 52:loss 1737.9230155944824  \n",
            "val iteration 53:loss 1764.9194107055664  \n",
            "val iteration 54:loss 1809.5514602661133  \n",
            "val iteration 55:loss 1830.4284648895264  \n",
            "val iteration 56:loss 1830.6677026748657  \n",
            "val Loss: 1.0378 Acc: 0.7460\n",
            "train iteration 1:loss 0.3955717086791992  \n",
            "train iteration 2:loss 0.874018669128418  \n",
            "train iteration 3:loss 1.3660879135131836  \n",
            "train iteration 4:loss 2.315932273864746  \n",
            "train iteration 5:loss 2.6605844497680664  \n",
            "train iteration 6:loss 3.171126365661621  \n",
            "train iteration 7:loss 3.707408905029297  \n",
            "train iteration 8:loss 4.075905799865723  \n",
            "train iteration 9:loss 4.589417457580566  \n",
            "train iteration 10:loss 5.114660263061523  \n",
            "train iteration 11:loss 5.468135833740234  \n",
            "train iteration 12:loss 7.097835540771484  \n",
            "train iteration 13:loss 7.547842025756836  \n",
            "train iteration 14:loss 8.100481986999512  \n",
            "train iteration 15:loss 8.464255332946777  \n",
            "train iteration 16:loss 8.888948440551758  \n",
            "train iteration 17:loss 9.229165077209473  \n",
            "train iteration 18:loss 9.661345481872559  \n",
            "train iteration 19:loss 11.84561538696289  \n",
            "train iteration 20:loss 12.316266059875488  \n",
            "train iteration 21:loss 12.654805183410645  \n",
            "train iteration 22:loss 13.217766761779785  \n",
            "train iteration 23:loss 13.645387649536133  \n",
            "train iteration 24:loss 14.08871078491211  \n",
            "train iteration 25:loss 14.557188034057617  \n",
            "train iteration 26:loss 14.976062774658203  \n",
            "train iteration 27:loss 15.497681617736816  \n",
            "train iteration 28:loss 15.976373672485352  \n",
            "train iteration 29:loss 16.3685245513916  \n",
            "train iteration 30:loss 16.84772300720215  \n",
            "train iteration 31:loss 17.15137481689453  \n",
            "train iteration 32:loss 17.745285987854004  \n",
            "train iteration 33:loss 18.211146354675293  \n",
            "train iteration 34:loss 18.78730583190918  \n",
            "train iteration 35:loss 19.755908966064453  \n",
            "train iteration 36:loss 20.434844970703125  \n",
            "train iteration 37:loss 20.858856201171875  \n",
            "train iteration 38:loss 21.32616901397705  \n",
            "train iteration 39:loss 21.89037036895752  \n",
            "train iteration 40:loss 22.34598445892334  \n",
            "train iteration 41:loss 22.875232696533203  \n",
            "train iteration 42:loss 23.20839786529541  \n",
            "train iteration 43:loss 23.626181602478027  \n",
            "train iteration 44:loss 24.2144832611084  \n",
            "train iteration 45:loss 24.59699535369873  \n",
            "train iteration 46:loss 25.013054847717285  \n",
            "train iteration 47:loss 25.43912410736084  \n",
            "train iteration 48:loss 25.806994438171387  \n",
            "train iteration 49:loss 26.21915054321289  \n",
            "train iteration 50:loss 26.634907722473145  \n",
            "train iteration 51:loss 26.953458786010742  \n",
            "train iteration 52:loss 27.593741416931152  \n",
            "train iteration 53:loss 28.84960174560547  \n",
            "train iteration 54:loss 29.237911224365234  \n",
            "train iteration 55:loss 29.765538215637207  \n",
            "train iteration 56:loss 30.184921264648438  \n",
            "train iteration 57:loss 30.829050064086914  \n",
            "train iteration 58:loss 31.145926475524902  \n",
            "train iteration 59:loss 31.707772254943848  \n",
            "train iteration 60:loss 32.3052396774292  \n",
            "train iteration 61:loss 33.62546157836914  \n",
            "train iteration 62:loss 34.749409675598145  \n",
            "train iteration 63:loss 35.22882080078125  \n",
            "train iteration 64:loss 35.63042163848877  \n",
            "train iteration 65:loss 35.979597091674805  \n",
            "train iteration 66:loss 36.40584850311279  \n",
            "train iteration 67:loss 36.842957496643066  \n",
            "train iteration 68:loss 37.18928146362305  \n",
            "train iteration 69:loss 38.74408149719238  \n",
            "train iteration 70:loss 39.37248516082764  \n",
            "train iteration 71:loss 39.888075828552246  \n",
            "train iteration 72:loss 40.40847873687744  \n",
            "train iteration 73:loss 40.88070106506348  \n",
            "train iteration 74:loss 41.303364753723145  \n",
            "train iteration 75:loss 45.72044277191162  \n",
            "train iteration 76:loss 46.16540718078613  \n",
            "train iteration 77:loss 46.5319938659668  \n",
            "train iteration 78:loss 46.933528900146484  \n",
            "train iteration 79:loss 47.50098419189453  \n",
            "train iteration 80:loss 48.021162033081055  \n",
            "train iteration 81:loss 48.42817306518555  \n",
            "train iteration 82:loss 48.89467239379883  \n",
            "train iteration 83:loss 51.42049694061279  \n",
            "train iteration 84:loss 51.70819091796875  \n",
            "train iteration 85:loss 54.250250816345215  \n",
            "train iteration 86:loss 54.672574043273926  \n",
            "train iteration 87:loss 55.3035888671875  \n",
            "train iteration 88:loss 55.712989807128906  \n",
            "train iteration 89:loss 57.30831527709961  \n",
            "train iteration 90:loss 57.70262908935547  \n",
            "train iteration 91:loss 58.075297355651855  \n",
            "train iteration 92:loss 59.7758846282959  \n",
            "train iteration 93:loss 60.26902961730957  \n",
            "train iteration 94:loss 60.680556297302246  \n",
            "train iteration 95:loss 61.1344690322876  \n",
            "train iteration 96:loss 61.55316925048828  \n",
            "train iteration 97:loss 61.97685146331787  \n",
            "train iteration 98:loss 62.37295913696289  \n",
            "train iteration 99:loss 62.82352542877197  \n",
            "train iteration 100:loss 63.34629915654659  \n",
            "train Loss: 0.0099 Acc: 0.9983\n",
            "val iteration 1:loss 25.8281307220459  \n",
            "val iteration 2:loss 53.82084846496582  \n",
            "val iteration 3:loss 98.96546363830566  \n",
            "val iteration 4:loss 126.16701126098633  \n",
            "val iteration 5:loss 160.33961486816406  \n",
            "val iteration 6:loss 198.19889450073242  \n",
            "val iteration 7:loss 222.953462600708  \n",
            "val iteration 8:loss 251.1260108947754  \n",
            "val iteration 9:loss 275.15943908691406  \n",
            "val iteration 10:loss 318.12744903564453  \n",
            "val iteration 11:loss 355.1429138183594  \n",
            "val iteration 12:loss 385.82193756103516  \n",
            "val iteration 13:loss 405.8050308227539  \n",
            "val iteration 14:loss 418.04268646240234  \n",
            "val iteration 15:loss 452.04761505126953  \n",
            "val iteration 16:loss 487.02986907958984  \n",
            "val iteration 17:loss 528.0533752441406  \n",
            "val iteration 18:loss 582.2934875488281  \n",
            "val iteration 19:loss 608.4961433410645  \n",
            "val iteration 20:loss 638.1725883483887  \n",
            "val iteration 21:loss 672.8079490661621  \n",
            "val iteration 22:loss 696.8608303070068  \n",
            "val iteration 23:loss 734.0230503082275  \n",
            "val iteration 24:loss 781.5907230377197  \n",
            "val iteration 25:loss 825.4882144927979  \n",
            "val iteration 26:loss 846.8144683837891  \n",
            "val iteration 27:loss 880.9990158081055  \n",
            "val iteration 28:loss 922.2327766418457  \n",
            "val iteration 29:loss 954.8906173706055  \n",
            "val iteration 30:loss 988.899772644043  \n",
            "val iteration 31:loss 1027.2573127746582  \n",
            "val iteration 32:loss 1053.0783462524414  \n",
            "val iteration 33:loss 1082.5651931762695  \n",
            "val iteration 34:loss 1121.4106750488281  \n",
            "val iteration 35:loss 1165.0939750671387  \n",
            "val iteration 36:loss 1208.4843215942383  \n",
            "val iteration 37:loss 1240.228687286377  \n",
            "val iteration 38:loss 1253.404507637024  \n",
            "val iteration 39:loss 1285.4665269851685  \n",
            "val iteration 40:loss 1318.6084032058716  \n",
            "val iteration 41:loss 1357.712285041809  \n",
            "val iteration 42:loss 1386.5668230056763  \n",
            "val iteration 43:loss 1405.1377382278442  \n",
            "val iteration 44:loss 1425.3731317520142  \n",
            "val iteration 45:loss 1454.084267616272  \n",
            "val iteration 46:loss 1482.6151857376099  \n",
            "val iteration 47:loss 1503.2149629592896  \n",
            "val iteration 48:loss 1541.9427614212036  \n",
            "val iteration 49:loss 1570.9053411483765  \n",
            "val iteration 50:loss 1587.7556219100952  \n",
            "val iteration 51:loss 1611.0162591934204  \n",
            "val iteration 52:loss 1636.1714029312134  \n",
            "val iteration 53:loss 1677.395043373108  \n",
            "val iteration 54:loss 1731.0951623916626  \n",
            "val iteration 55:loss 1766.7178583145142  \n",
            "val iteration 56:loss 1771.6679344177246  \n",
            "val Loss: 1.0043 Acc: 0.7426\n",
            "train iteration 1:loss 2.121744155883789  \n",
            "train iteration 2:loss 2.5885744094848633  \n",
            "train iteration 3:loss 2.958864212036133  \n",
            "train iteration 4:loss 3.4700927734375  \n",
            "train iteration 5:loss 3.8760595321655273  \n",
            "train iteration 6:loss 5.111871719360352  \n",
            "train iteration 7:loss 6.031289100646973  \n",
            "train iteration 8:loss 6.341831207275391  \n",
            "train iteration 9:loss 6.655638694763184  \n",
            "train iteration 10:loss 7.168051719665527  \n",
            "train iteration 11:loss 7.644942283630371  \n",
            "train iteration 12:loss 8.030050277709961  \n",
            "train iteration 13:loss 8.596648216247559  \n",
            "train iteration 14:loss 9.057361602783203  \n",
            "train iteration 15:loss 9.447916984558105  \n",
            "train iteration 16:loss 9.966214179992676  \n",
            "train iteration 17:loss 10.380026817321777  \n",
            "train iteration 18:loss 10.784567832946777  \n",
            "train iteration 19:loss 11.37240982055664  \n",
            "train iteration 20:loss 11.808091163635254  \n",
            "train iteration 21:loss 12.154007911682129  \n",
            "train iteration 22:loss 12.495626449584961  \n",
            "train iteration 23:loss 12.93604564666748  \n",
            "train iteration 24:loss 13.307172775268555  \n",
            "train iteration 25:loss 13.821252822875977  \n",
            "train iteration 26:loss 14.216879844665527  \n",
            "train iteration 27:loss 14.644038200378418  \n",
            "train iteration 28:loss 15.114726066589355  \n",
            "train iteration 29:loss 15.765487670898438  \n",
            "train iteration 30:loss 16.11786460876465  \n",
            "train iteration 31:loss 16.52227783203125  \n",
            "train iteration 32:loss 16.922670364379883  \n",
            "train iteration 33:loss 17.36590003967285  \n",
            "train iteration 34:loss 17.790966987609863  \n",
            "train iteration 35:loss 19.114051818847656  \n",
            "train iteration 36:loss 19.50765895843506  \n",
            "train iteration 37:loss 19.865378379821777  \n",
            "train iteration 38:loss 20.361462593078613  \n",
            "train iteration 39:loss 20.92751979827881  \n",
            "train iteration 40:loss 21.34031391143799  \n",
            "train iteration 41:loss 23.17411231994629  \n",
            "train iteration 42:loss 23.530784606933594  \n",
            "train iteration 43:loss 23.95986270904541  \n",
            "train iteration 44:loss 24.361019134521484  \n",
            "train iteration 45:loss 24.86946964263916  \n",
            "train iteration 46:loss 25.51076316833496  \n",
            "train iteration 47:loss 25.88537311553955  \n",
            "train iteration 48:loss 26.56207847595215  \n",
            "train iteration 49:loss 26.942713737487793  \n",
            "train iteration 50:loss 27.458250999450684  \n",
            "train iteration 51:loss 27.823506355285645  \n",
            "train iteration 52:loss 28.2643461227417  \n",
            "train iteration 53:loss 28.707881927490234  \n",
            "train iteration 54:loss 29.431031227111816  \n",
            "train iteration 55:loss 29.74914836883545  \n",
            "train iteration 56:loss 30.10930347442627  \n",
            "train iteration 57:loss 30.60270595550537  \n",
            "train iteration 58:loss 32.20092582702637  \n",
            "train iteration 59:loss 32.48587989807129  \n",
            "train iteration 60:loss 34.216301918029785  \n",
            "train iteration 61:loss 34.5501708984375  \n",
            "train iteration 62:loss 34.98324394226074  \n",
            "train iteration 63:loss 35.374587059020996  \n",
            "train iteration 64:loss 35.68569850921631  \n",
            "train iteration 65:loss 36.076552391052246  \n",
            "train iteration 66:loss 36.67671298980713  \n",
            "train iteration 67:loss 37.14928436279297  \n",
            "train iteration 68:loss 37.55135250091553  \n",
            "train iteration 69:loss 38.00642776489258  \n",
            "train iteration 70:loss 38.59980392456055  \n",
            "train iteration 71:loss 39.03100395202637  \n",
            "train iteration 72:loss 39.591179847717285  \n",
            "train iteration 73:loss 40.18922805786133  \n",
            "train iteration 74:loss 40.558823585510254  \n",
            "train iteration 75:loss 44.593180656433105  \n",
            "train iteration 76:loss 44.941195487976074  \n",
            "train iteration 77:loss 47.431546211242676  \n",
            "train iteration 78:loss 47.843658447265625  \n",
            "train iteration 79:loss 48.27161407470703  \n",
            "train iteration 80:loss 48.68017578125  \n",
            "train iteration 81:loss 49.09823703765869  \n",
            "train iteration 82:loss 51.67451858520508  \n",
            "train iteration 83:loss 52.11542320251465  \n",
            "train iteration 84:loss 52.73692798614502  \n",
            "train iteration 85:loss 55.55013084411621  \n",
            "train iteration 86:loss 55.96044921875  \n",
            "train iteration 87:loss 56.83116340637207  \n",
            "train iteration 88:loss 58.83492660522461  \n",
            "train iteration 89:loss 59.16789627075195  \n",
            "train iteration 90:loss 59.58038902282715  \n",
            "train iteration 91:loss 59.976945877075195  \n",
            "train iteration 92:loss 60.39013481140137  \n",
            "train iteration 93:loss 60.766794204711914  \n",
            "train iteration 94:loss 61.263298988342285  \n",
            "train iteration 95:loss 61.627686500549316  \n",
            "train iteration 96:loss 62.21007061004639  \n",
            "train iteration 97:loss 62.75358009338379  \n",
            "train iteration 98:loss 65.70047760009766  \n",
            "train iteration 99:loss 66.23634052276611  \n",
            "train iteration 100:loss 66.58168887719512  \n",
            "train Loss: 0.0104 Acc: 0.9981\n",
            "val iteration 1:loss 20.858226776123047  \n",
            "val iteration 2:loss 62.008060455322266  \n",
            "val iteration 3:loss 96.65017318725586  \n",
            "val iteration 4:loss 128.91801834106445  \n",
            "val iteration 5:loss 162.5521011352539  \n",
            "val iteration 6:loss 195.5999870300293  \n",
            "val iteration 7:loss 229.11330032348633  \n",
            "val iteration 8:loss 266.26834869384766  \n",
            "val iteration 9:loss 290.54199409484863  \n",
            "val iteration 10:loss 323.4623775482178  \n",
            "val iteration 11:loss 357.7882137298584  \n",
            "val iteration 12:loss 387.353551864624  \n",
            "val iteration 13:loss 417.10687828063965  \n",
            "val iteration 14:loss 453.72497367858887  \n",
            "val iteration 15:loss 484.5001392364502  \n",
            "val iteration 16:loss 527.7754001617432  \n",
            "val iteration 17:loss 560.0428791046143  \n",
            "val iteration 18:loss 597.5736789703369  \n",
            "val iteration 19:loss 650.1712970733643  \n",
            "val iteration 20:loss 677.6616649627686  \n",
            "val iteration 21:loss 689.4466514587402  \n",
            "val iteration 22:loss 718.2829055786133  \n",
            "val iteration 23:loss 754.8415756225586  \n",
            "val iteration 24:loss 786.9601860046387  \n",
            "val iteration 25:loss 827.0882301330566  \n",
            "val iteration 26:loss 848.4114952087402  \n",
            "val iteration 27:loss 882.4599304199219  \n",
            "val iteration 28:loss 913.8701210021973  \n",
            "val iteration 29:loss 946.8775978088379  \n",
            "val iteration 30:loss 965.0599136352539  \n",
            "val iteration 31:loss 990.5527763366699  \n",
            "val iteration 32:loss 1018.2288970947266  \n",
            "val iteration 33:loss 1064.2711601257324  \n",
            "val iteration 34:loss 1105.841724395752  \n",
            "val iteration 35:loss 1145.7467498779297  \n",
            "val iteration 36:loss 1162.7074089050293  \n",
            "val iteration 37:loss 1205.4638633728027  \n",
            "val iteration 38:loss 1249.3236045837402  \n",
            "val iteration 39:loss 1273.5365905761719  \n",
            "val iteration 40:loss 1304.8328857421875  \n",
            "val iteration 41:loss 1338.7628021240234  \n",
            "val iteration 42:loss 1351.849588394165  \n",
            "val iteration 43:loss 1382.3137493133545  \n",
            "val iteration 44:loss 1406.9330348968506  \n",
            "val iteration 45:loss 1446.996576309204  \n",
            "val iteration 46:loss 1469.4194889068604  \n",
            "val iteration 47:loss 1523.612325668335  \n",
            "val iteration 48:loss 1585.7959651947021  \n",
            "val iteration 49:loss 1612.655138015747  \n",
            "val iteration 50:loss 1631.2671451568604  \n",
            "val iteration 51:loss 1657.4675064086914  \n",
            "val iteration 52:loss 1706.0248336791992  \n",
            "val iteration 53:loss 1734.7454814910889  \n",
            "val iteration 54:loss 1775.0985317230225  \n",
            "val iteration 55:loss 1802.9228324890137  \n",
            "val iteration 56:loss 1807.8552803993225  \n",
            "val Loss: 1.0249 Acc: 0.7455\n",
            "train iteration 1:loss 0.6208086013793945  \n",
            "train iteration 2:loss 0.9964141845703125  \n",
            "train iteration 3:loss 1.369821548461914  \n",
            "train iteration 4:loss 1.9286699295043945  \n",
            "train iteration 5:loss 2.3963499069213867  \n",
            "train iteration 6:loss 2.9149703979492188  \n",
            "train iteration 7:loss 3.349344253540039  \n",
            "train iteration 8:loss 3.705866813659668  \n",
            "train iteration 9:loss 4.123312950134277  \n",
            "train iteration 10:loss 4.496161460876465  \n",
            "train iteration 11:loss 4.941056251525879  \n",
            "train iteration 12:loss 5.357846260070801  \n",
            "train iteration 13:loss 5.732629776000977  \n",
            "train iteration 14:loss 6.046883583068848  \n",
            "train iteration 15:loss 6.487872123718262  \n",
            "train iteration 16:loss 6.917238235473633  \n",
            "train iteration 17:loss 7.1995849609375  \n",
            "train iteration 18:loss 7.543347358703613  \n",
            "train iteration 19:loss 7.811814308166504  \n",
            "train iteration 20:loss 8.225815773010254  \n",
            "train iteration 21:loss 8.678531646728516  \n",
            "train iteration 22:loss 9.419270515441895  \n",
            "train iteration 23:loss 9.851330757141113  \n",
            "train iteration 24:loss 10.210650444030762  \n",
            "train iteration 25:loss 10.515463829040527  \n",
            "train iteration 26:loss 11.05086898803711  \n",
            "train iteration 27:loss 11.5309476852417  \n",
            "train iteration 28:loss 11.87730598449707  \n",
            "train iteration 29:loss 14.427011489868164  \n",
            "train iteration 30:loss 14.742402076721191  \n",
            "train iteration 31:loss 15.314894676208496  \n",
            "train iteration 32:loss 15.711050033569336  \n",
            "train iteration 33:loss 16.85960578918457  \n",
            "train iteration 34:loss 17.226526260375977  \n",
            "train iteration 35:loss 20.260438919067383  \n",
            "train iteration 36:loss 21.046420097351074  \n",
            "train iteration 37:loss 21.494009971618652  \n",
            "train iteration 38:loss 22.061617851257324  \n",
            "train iteration 39:loss 22.59082794189453  \n",
            "train iteration 40:loss 22.97658348083496  \n",
            "train iteration 41:loss 23.586986541748047  \n",
            "train iteration 42:loss 23.90617561340332  \n",
            "train iteration 43:loss 24.3988037109375  \n",
            "train iteration 44:loss 24.8957576751709  \n",
            "train iteration 45:loss 25.50751781463623  \n",
            "train iteration 46:loss 26.160888671875  \n",
            "train iteration 47:loss 26.64341449737549  \n",
            "train iteration 48:loss 27.18968105316162  \n",
            "train iteration 49:loss 27.651397705078125  \n",
            "train iteration 50:loss 28.082375526428223  \n",
            "train iteration 51:loss 28.639917373657227  \n",
            "train iteration 52:loss 35.34273147583008  \n",
            "train iteration 53:loss 35.741639137268066  \n",
            "train iteration 54:loss 36.155999183654785  \n",
            "train iteration 55:loss 36.47895908355713  \n",
            "train iteration 56:loss 36.82097148895264  \n",
            "train iteration 57:loss 37.53713607788086  \n",
            "train iteration 58:loss 38.08505344390869  \n",
            "train iteration 59:loss 38.56852149963379  \n",
            "train iteration 60:loss 38.97301197052002  \n",
            "train iteration 61:loss 41.609375953674316  \n",
            "train iteration 62:loss 41.91378879547119  \n",
            "train iteration 63:loss 43.73608684539795  \n",
            "train iteration 64:loss 44.32605457305908  \n",
            "train iteration 65:loss 44.71676540374756  \n",
            "train iteration 66:loss 45.15169906616211  \n",
            "train iteration 67:loss 45.570481300354004  \n",
            "train iteration 68:loss 45.95462989807129  \n",
            "train iteration 69:loss 47.72015571594238  \n",
            "train iteration 70:loss 48.15303325653076  \n",
            "train iteration 71:loss 48.43666744232178  \n",
            "train iteration 72:loss 49.57843494415283  \n",
            "train iteration 73:loss 50.12718486785889  \n",
            "train iteration 74:loss 50.53633213043213  \n",
            "train iteration 75:loss 50.9620418548584  \n",
            "train iteration 76:loss 51.33556079864502  \n",
            "train iteration 77:loss 51.72243785858154  \n",
            "train iteration 78:loss 53.23664951324463  \n",
            "train iteration 79:loss 53.617295265197754  \n",
            "train iteration 80:loss 54.20343208312988  \n",
            "train iteration 81:loss 56.90122127532959  \n",
            "train iteration 82:loss 57.39669990539551  \n",
            "train iteration 83:loss 57.68776512145996  \n",
            "train iteration 84:loss 57.97364139556885  \n",
            "train iteration 85:loss 58.57886028289795  \n",
            "train iteration 86:loss 58.83320140838623  \n",
            "train iteration 87:loss 59.31340026855469  \n",
            "train iteration 88:loss 59.7208366394043  \n",
            "train iteration 89:loss 60.19775390625  \n",
            "train iteration 90:loss 60.52489471435547  \n",
            "train iteration 91:loss 60.821685791015625  \n",
            "train iteration 92:loss 61.23924446105957  \n",
            "train iteration 93:loss 62.412699699401855  \n",
            "train iteration 94:loss 62.67768096923828  \n",
            "train iteration 95:loss 63.0953311920166  \n",
            "train iteration 96:loss 63.57080554962158  \n",
            "train iteration 97:loss 64.01848411560059  \n",
            "train iteration 98:loss 64.36419200897217  \n",
            "train iteration 99:loss 64.81637763977051  \n",
            "train iteration 100:loss 65.11976719275117  \n",
            "train Loss: 0.0102 Acc: 0.9987\n",
            "val iteration 1:loss 31.706607818603516  \n",
            "val iteration 2:loss 57.92092323303223  \n",
            "val iteration 3:loss 90.18696784973145  \n",
            "val iteration 4:loss 121.59815406799316  \n",
            "val iteration 5:loss 157.50421333312988  \n",
            "val iteration 6:loss 193.27927207946777  \n",
            "val iteration 7:loss 222.43958473205566  \n",
            "val iteration 8:loss 265.8443202972412  \n",
            "val iteration 9:loss 318.19909858703613  \n",
            "val iteration 10:loss 344.721960067749  \n",
            "val iteration 11:loss 359.5362501144409  \n",
            "val iteration 12:loss 385.46733379364014  \n",
            "val iteration 13:loss 432.8105459213257  \n",
            "val iteration 14:loss 451.6040258407593  \n",
            "val iteration 15:loss 483.7874593734741  \n",
            "val iteration 16:loss 536.9958333969116  \n",
            "val iteration 17:loss 570.5665578842163  \n",
            "val iteration 18:loss 591.6754598617554  \n",
            "val iteration 19:loss 622.3564348220825  \n",
            "val iteration 20:loss 648.6877298355103  \n",
            "val iteration 21:loss 688.6625146865845  \n",
            "val iteration 22:loss 734.5437707901001  \n",
            "val iteration 23:loss 787.2494897842407  \n",
            "val iteration 24:loss 825.9135751724243  \n",
            "val iteration 25:loss 862.3603067398071  \n",
            "val iteration 26:loss 889.4236249923706  \n",
            "val iteration 27:loss 918.8984689712524  \n",
            "val iteration 28:loss 937.7645711898804  \n",
            "val iteration 29:loss 981.5649213790894  \n",
            "val iteration 30:loss 1008.885256767273  \n",
            "val iteration 31:loss 1046.9770765304565  \n",
            "val iteration 32:loss 1080.0632543563843  \n",
            "val iteration 33:loss 1119.2135648727417  \n",
            "val iteration 34:loss 1147.989634513855  \n",
            "val iteration 35:loss 1185.7583951950073  \n",
            "val iteration 36:loss 1204.703314781189  \n",
            "val iteration 37:loss 1223.4027166366577  \n",
            "val iteration 38:loss 1260.2966871261597  \n",
            "val iteration 39:loss 1292.9805707931519  \n",
            "val iteration 40:loss 1321.098422050476  \n",
            "val iteration 41:loss 1366.9839506149292  \n",
            "val iteration 42:loss 1396.482398033142  \n",
            "val iteration 43:loss 1433.0129537582397  \n",
            "val iteration 44:loss 1460.5528402328491  \n",
            "val iteration 45:loss 1496.4246091842651  \n",
            "val iteration 46:loss 1553.2733030319214  \n",
            "val iteration 47:loss 1583.8430967330933  \n",
            "val iteration 48:loss 1615.0194234848022  \n",
            "val iteration 49:loss 1621.9706325531006  \n",
            "val iteration 50:loss 1657.092004776001  \n",
            "val iteration 51:loss 1683.6382904052734  \n",
            "val iteration 52:loss 1706.5376052856445  \n",
            "val iteration 53:loss 1738.3641548156738  \n",
            "val iteration 54:loss 1771.788776397705  \n",
            "val iteration 55:loss 1796.363079071045  \n",
            "val iteration 56:loss 1796.4731340408325  \n",
            "val Loss: 1.0184 Acc: 0.7455\n",
            "train iteration 1:loss 0.38866329193115234  \n",
            "train iteration 2:loss 1.8206396102905273  \n",
            "train iteration 3:loss 2.5648746490478516  \n",
            "train iteration 4:loss 2.9627561569213867  \n",
            "train iteration 5:loss 3.3132734298706055  \n",
            "train iteration 6:loss 3.974048614501953  \n",
            "train iteration 7:loss 4.3924760818481445  \n",
            "train iteration 8:loss 4.8545074462890625  \n",
            "train iteration 9:loss 5.432469367980957  \n",
            "train iteration 10:loss 5.7516984939575195  \n",
            "train iteration 11:loss 6.200268745422363  \n",
            "train iteration 12:loss 6.70255184173584  \n",
            "train iteration 13:loss 7.088778495788574  \n",
            "train iteration 14:loss 7.551536560058594  \n",
            "train iteration 15:loss 8.01714038848877  \n",
            "train iteration 16:loss 8.358529090881348  \n",
            "train iteration 17:loss 8.805448532104492  \n",
            "train iteration 18:loss 9.325506210327148  \n",
            "train iteration 19:loss 9.80049991607666  \n",
            "train iteration 20:loss 10.289743423461914  \n",
            "train iteration 21:loss 10.70311450958252  \n",
            "train iteration 22:loss 11.085094451904297  \n",
            "train iteration 23:loss 11.538640022277832  \n",
            "train iteration 24:loss 11.897470474243164  \n",
            "train iteration 25:loss 12.237504005432129  \n",
            "train iteration 26:loss 12.691777229309082  \n",
            "train iteration 27:loss 12.978574752807617  \n",
            "train iteration 28:loss 13.242990493774414  \n",
            "train iteration 29:loss 13.56247329711914  \n",
            "train iteration 30:loss 13.963418006896973  \n",
            "train iteration 31:loss 14.349677085876465  \n",
            "train iteration 32:loss 14.750008583068848  \n",
            "train iteration 33:loss 16.0635404586792  \n",
            "train iteration 34:loss 16.550395965576172  \n",
            "train iteration 35:loss 17.046186447143555  \n",
            "train iteration 36:loss 17.45874309539795  \n",
            "train iteration 37:loss 17.73922824859619  \n",
            "train iteration 38:loss 18.186229705810547  \n",
            "train iteration 39:loss 18.569119453430176  \n",
            "train iteration 40:loss 18.962812423706055  \n",
            "train iteration 41:loss 19.413140296936035  \n",
            "train iteration 42:loss 19.75543785095215  \n",
            "train iteration 43:loss 20.179600715637207  \n",
            "train iteration 44:loss 21.04097557067871  \n",
            "train iteration 45:loss 21.505104064941406  \n",
            "train iteration 46:loss 21.874034881591797  \n",
            "train iteration 47:loss 22.355899810791016  \n",
            "train iteration 48:loss 22.824499130249023  \n",
            "train iteration 49:loss 23.25765609741211  \n",
            "train iteration 50:loss 23.51859760284424  \n",
            "train iteration 51:loss 23.960949897766113  \n",
            "train iteration 52:loss 24.38046932220459  \n",
            "train iteration 53:loss 24.82768726348877  \n",
            "train iteration 54:loss 25.18096351623535  \n",
            "train iteration 55:loss 26.574718475341797  \n",
            "train iteration 56:loss 28.301682472229004  \n",
            "train iteration 57:loss 28.761576652526855  \n",
            "train iteration 58:loss 30.035686492919922  \n",
            "train iteration 59:loss 30.466965675354004  \n",
            "train iteration 60:loss 30.76900577545166  \n",
            "train iteration 61:loss 31.156800270080566  \n",
            "train iteration 62:loss 34.62574481964111  \n",
            "train iteration 63:loss 35.0060396194458  \n",
            "train iteration 64:loss 38.26793956756592  \n",
            "train iteration 65:loss 38.57985210418701  \n",
            "train iteration 66:loss 38.91450500488281  \n",
            "train iteration 67:loss 39.299235343933105  \n",
            "train iteration 68:loss 39.53979682922363  \n",
            "train iteration 69:loss 39.884111404418945  \n",
            "train iteration 70:loss 40.3834867477417  \n",
            "train iteration 71:loss 40.839104652404785  \n",
            "train iteration 72:loss 41.21931266784668  \n",
            "train iteration 73:loss 41.606374740600586  \n",
            "train iteration 74:loss 41.975948333740234  \n",
            "train iteration 75:loss 42.3770170211792  \n",
            "train iteration 76:loss 47.59522533416748  \n",
            "train iteration 77:loss 47.96716785430908  \n",
            "train iteration 78:loss 48.3872766494751  \n",
            "train iteration 79:loss 48.70589542388916  \n",
            "train iteration 80:loss 49.07985877990723  \n",
            "train iteration 81:loss 49.41218280792236  \n",
            "train iteration 82:loss 49.78429985046387  \n",
            "train iteration 83:loss 50.15874671936035  \n",
            "train iteration 84:loss 50.550859451293945  \n",
            "train iteration 85:loss 51.05482864379883  \n",
            "train iteration 86:loss 51.56468391418457  \n",
            "train iteration 87:loss 51.96526908874512  \n",
            "train iteration 88:loss 52.490901947021484  \n",
            "train iteration 89:loss 52.8771858215332  \n",
            "train iteration 90:loss 54.14316272735596  \n",
            "train iteration 91:loss 55.19168949127197  \n",
            "train iteration 92:loss 55.705498695373535  \n",
            "train iteration 93:loss 56.000444412231445  \n",
            "train iteration 94:loss 56.40645694732666  \n",
            "train iteration 95:loss 56.76706600189209  \n",
            "train iteration 96:loss 57.48837089538574  \n",
            "train iteration 97:loss 58.935378074645996  \n",
            "train iteration 98:loss 59.40210247039795  \n",
            "train iteration 99:loss 59.76546287536621  \n",
            "train iteration 100:loss 60.10774993337691  \n",
            "train Loss: 0.0094 Acc: 0.9986\n",
            "val iteration 1:loss 25.153697967529297  \n",
            "val iteration 2:loss 47.845611572265625  \n",
            "val iteration 3:loss 74.89654159545898  \n",
            "val iteration 4:loss 109.86174392700195  \n",
            "val iteration 5:loss 141.29259490966797  \n",
            "val iteration 6:loss 172.3259792327881  \n",
            "val iteration 7:loss 217.24809074401855  \n",
            "val iteration 8:loss 268.5209445953369  \n",
            "val iteration 9:loss 296.05991554260254  \n",
            "val iteration 10:loss 322.2685375213623  \n",
            "val iteration 11:loss 344.5814666748047  \n",
            "val iteration 12:loss 395.01805114746094  \n",
            "val iteration 13:loss 430.4013977050781  \n",
            "val iteration 14:loss 451.6085777282715  \n",
            "val iteration 15:loss 478.6067581176758  \n",
            "val iteration 16:loss 512.934268951416  \n",
            "val iteration 17:loss 544.9514770507812  \n",
            "val iteration 18:loss 577.0293045043945  \n",
            "val iteration 19:loss 607.8782806396484  \n",
            "val iteration 20:loss 644.1615447998047  \n",
            "val iteration 21:loss 673.344295501709  \n",
            "val iteration 22:loss 692.9695720672607  \n",
            "val iteration 23:loss 731.8361186981201  \n",
            "val iteration 24:loss 763.2458553314209  \n",
            "val iteration 25:loss 794.9101810455322  \n",
            "val iteration 26:loss 821.5756988525391  \n",
            "val iteration 27:loss 840.9800968170166  \n",
            "val iteration 28:loss 878.4830417633057  \n",
            "val iteration 29:loss 912.2534122467041  \n",
            "val iteration 30:loss 941.0621700286865  \n",
            "val iteration 31:loss 987.9176006317139  \n",
            "val iteration 32:loss 1023.7976245880127  \n",
            "val iteration 33:loss 1067.3725147247314  \n",
            "val iteration 34:loss 1106.1013851165771  \n",
            "val iteration 35:loss 1147.1464824676514  \n",
            "val iteration 36:loss 1166.6709156036377  \n",
            "val iteration 37:loss 1209.3693866729736  \n",
            "val iteration 38:loss 1242.0887355804443  \n",
            "val iteration 39:loss 1273.366569519043  \n",
            "val iteration 40:loss 1313.671974182129  \n",
            "val iteration 41:loss 1333.573989868164  \n",
            "val iteration 42:loss 1370.404426574707  \n",
            "val iteration 43:loss 1399.8779697418213  \n",
            "val iteration 44:loss 1436.4021091461182  \n",
            "val iteration 45:loss 1457.182996749878  \n",
            "val iteration 46:loss 1477.8937091827393  \n",
            "val iteration 47:loss 1497.8698635101318  \n",
            "val iteration 48:loss 1548.8830547332764  \n",
            "val iteration 49:loss 1572.0623149871826  \n",
            "val iteration 50:loss 1622.4197406768799  \n",
            "val iteration 51:loss 1649.457181930542  \n",
            "val iteration 52:loss 1674.1168613433838  \n",
            "val iteration 53:loss 1718.5160083770752  \n",
            "val iteration 54:loss 1745.238790512085  \n",
            "val iteration 55:loss 1763.1779823303223  \n",
            "val iteration 56:loss 1763.4251232147217  \n",
            "val Loss: 0.9997 Acc: 0.7506\n",
            "train iteration 1:loss 0.3105182647705078  \n",
            "train iteration 2:loss 0.7208108901977539  \n",
            "train iteration 3:loss 1.9312667846679688  \n",
            "train iteration 4:loss 2.5034170150756836  \n",
            "train iteration 5:loss 2.965508460998535  \n",
            "train iteration 6:loss 3.350459098815918  \n",
            "train iteration 7:loss 3.6180801391601562  \n",
            "train iteration 8:loss 4.09922981262207  \n",
            "train iteration 9:loss 4.781674385070801  \n",
            "train iteration 10:loss 5.163712501525879  \n",
            "train iteration 11:loss 5.402876853942871  \n",
            "train iteration 12:loss 5.8502607345581055  \n",
            "train iteration 13:loss 6.360113143920898  \n",
            "train iteration 14:loss 6.776196479797363  \n",
            "train iteration 15:loss 7.258111953735352  \n",
            "train iteration 16:loss 7.626217842102051  \n",
            "train iteration 17:loss 8.538712501525879  \n",
            "train iteration 18:loss 9.004956245422363  \n",
            "train iteration 19:loss 9.2771635055542  \n",
            "train iteration 20:loss 9.706363677978516  \n",
            "train iteration 21:loss 10.014921188354492  \n",
            "train iteration 22:loss 10.343796730041504  \n",
            "train iteration 23:loss 10.76605224609375  \n",
            "train iteration 24:loss 11.06002426147461  \n",
            "train iteration 25:loss 11.369735717773438  \n",
            "train iteration 26:loss 11.684897422790527  \n",
            "train iteration 27:loss 12.107634544372559  \n",
            "train iteration 28:loss 12.411577224731445  \n",
            "train iteration 29:loss 14.594797134399414  \n",
            "train iteration 30:loss 15.081087112426758  \n",
            "train iteration 31:loss 15.414769172668457  \n",
            "train iteration 32:loss 15.832334518432617  \n",
            "train iteration 33:loss 16.141891479492188  \n",
            "train iteration 34:loss 16.44912624359131  \n",
            "train iteration 35:loss 16.759592056274414  \n",
            "train iteration 36:loss 17.09505558013916  \n",
            "train iteration 37:loss 17.415701866149902  \n",
            "train iteration 38:loss 17.729265213012695  \n",
            "train iteration 39:loss 18.11226463317871  \n",
            "train iteration 40:loss 18.563227653503418  \n",
            "train iteration 41:loss 18.901554107666016  \n",
            "train iteration 42:loss 19.3939847946167  \n",
            "train iteration 43:loss 19.840408325195312  \n",
            "train iteration 44:loss 20.13267993927002  \n",
            "train iteration 45:loss 20.542109489440918  \n",
            "train iteration 46:loss 21.474079132080078  \n",
            "train iteration 47:loss 21.783164024353027  \n",
            "train iteration 48:loss 22.10920238494873  \n",
            "train iteration 49:loss 22.5248966217041  \n",
            "train iteration 50:loss 22.986631393432617  \n",
            "train iteration 51:loss 23.336198806762695  \n",
            "train iteration 52:loss 23.686312675476074  \n",
            "train iteration 53:loss 24.0267276763916  \n",
            "train iteration 54:loss 24.394054412841797  \n",
            "train iteration 55:loss 24.787344932556152  \n",
            "train iteration 56:loss 25.189985275268555  \n",
            "train iteration 57:loss 27.931011199951172  \n",
            "train iteration 58:loss 30.19863796234131  \n",
            "train iteration 59:loss 30.59142780303955  \n",
            "train iteration 60:loss 30.86636447906494  \n",
            "train iteration 61:loss 31.50812530517578  \n",
            "train iteration 62:loss 31.86548900604248  \n",
            "train iteration 63:loss 32.240769386291504  \n",
            "train iteration 64:loss 32.50247573852539  \n",
            "train iteration 65:loss 34.230567932128906  \n",
            "train iteration 66:loss 34.64256477355957  \n",
            "train iteration 67:loss 35.06668758392334  \n",
            "train iteration 68:loss 35.42759609222412  \n",
            "train iteration 69:loss 35.85431098937988  \n",
            "train iteration 70:loss 36.167490005493164  \n",
            "train iteration 71:loss 36.5112943649292  \n",
            "train iteration 72:loss 36.89918327331543  \n",
            "train iteration 73:loss 39.33543395996094  \n",
            "train iteration 74:loss 39.662662506103516  \n",
            "train iteration 75:loss 39.94347953796387  \n",
            "train iteration 76:loss 40.334208488464355  \n",
            "train iteration 77:loss 46.71843242645264  \n",
            "train iteration 78:loss 47.05085372924805  \n",
            "train iteration 79:loss 47.39558506011963  \n",
            "train iteration 80:loss 47.73741817474365  \n",
            "train iteration 81:loss 48.176331520080566  \n",
            "train iteration 82:loss 51.088714599609375  \n",
            "train iteration 83:loss 51.494269371032715  \n",
            "train iteration 84:loss 51.938838958740234  \n",
            "train iteration 85:loss 52.33599853515625  \n",
            "train iteration 86:loss 52.687499046325684  \n",
            "train iteration 87:loss 53.07182693481445  \n",
            "train iteration 88:loss 53.44669437408447  \n",
            "train iteration 89:loss 53.932780265808105  \n",
            "train iteration 90:loss 54.37932300567627  \n",
            "train iteration 91:loss 54.71943950653076  \n",
            "train iteration 92:loss 55.0367317199707  \n",
            "train iteration 93:loss 55.335787773132324  \n",
            "train iteration 94:loss 55.87573528289795  \n",
            "train iteration 95:loss 56.2178430557251  \n",
            "train iteration 96:loss 56.49331092834473  \n",
            "train iteration 97:loss 56.85347366333008  \n",
            "train iteration 98:loss 58.26070594787598  \n",
            "train iteration 99:loss 59.27439498901367  \n",
            "train iteration 100:loss 59.588615426793694  \n",
            "train Loss: 0.0093 Acc: 0.9983\n",
            "val iteration 1:loss 23.921157836914062  \n",
            "val iteration 2:loss 49.69053840637207  \n",
            "val iteration 3:loss 83.99988746643066  \n",
            "val iteration 4:loss 122.37610816955566  \n",
            "val iteration 5:loss 156.08671760559082  \n",
            "val iteration 6:loss 196.4081974029541  \n",
            "val iteration 7:loss 228.98055839538574  \n",
            "val iteration 8:loss 260.74692726135254  \n",
            "val iteration 9:loss 303.2051944732666  \n",
            "val iteration 10:loss 356.23086738586426  \n",
            "val iteration 11:loss 397.60962104797363  \n",
            "val iteration 12:loss 447.7151584625244  \n",
            "val iteration 13:loss 480.0095691680908  \n",
            "val iteration 14:loss 498.00089836120605  \n",
            "val iteration 15:loss 536.3368511199951  \n",
            "val iteration 16:loss 570.2061100006104  \n",
            "val iteration 17:loss 583.63951587677  \n",
            "val iteration 18:loss 613.2205400466919  \n",
            "val iteration 19:loss 652.5996751785278  \n",
            "val iteration 20:loss 685.9225435256958  \n",
            "val iteration 21:loss 718.523434638977  \n",
            "val iteration 22:loss 744.682614326477  \n",
            "val iteration 23:loss 784.9743051528931  \n",
            "val iteration 24:loss 819.0683832168579  \n",
            "val iteration 25:loss 836.4991483688354  \n",
            "val iteration 26:loss 867.2655191421509  \n",
            "val iteration 27:loss 902.3149576187134  \n",
            "val iteration 28:loss 924.1928720474243  \n",
            "val iteration 29:loss 958.7990846633911  \n",
            "val iteration 30:loss 1003.7318277359009  \n",
            "val iteration 31:loss 1029.3179597854614  \n",
            "val iteration 32:loss 1066.282597541809  \n",
            "val iteration 33:loss 1102.2044649124146  \n",
            "val iteration 34:loss 1129.0664987564087  \n",
            "val iteration 35:loss 1163.9085321426392  \n",
            "val iteration 36:loss 1198.7519807815552  \n",
            "val iteration 37:loss 1216.9475812911987  \n",
            "val iteration 38:loss 1245.017424583435  \n",
            "val iteration 39:loss 1270.5159120559692  \n",
            "val iteration 40:loss 1295.1282835006714  \n",
            "val iteration 41:loss 1331.2820386886597  \n",
            "val iteration 42:loss 1361.3436059951782  \n",
            "val iteration 43:loss 1391.4625406265259  \n",
            "val iteration 44:loss 1441.432671546936  \n",
            "val iteration 45:loss 1466.9024896621704  \n",
            "val iteration 46:loss 1495.9327459335327  \n",
            "val iteration 47:loss 1531.124987602234  \n",
            "val iteration 48:loss 1553.6005220413208  \n",
            "val iteration 49:loss 1585.9422998428345  \n",
            "val iteration 50:loss 1620.6418237686157  \n",
            "val iteration 51:loss 1652.210877418518  \n",
            "val iteration 52:loss 1696.237416267395  \n",
            "val iteration 53:loss 1734.616955757141  \n",
            "val iteration 54:loss 1768.1438493728638  \n",
            "val iteration 55:loss 1801.5586214065552  \n",
            "val iteration 56:loss 1807.6970310211182  \n",
            "val Loss: 1.0248 Acc: 0.7415\n",
            "train iteration 1:loss 0.4375143051147461  \n",
            "train iteration 2:loss 0.8145198822021484  \n",
            "train iteration 3:loss 1.0753393173217773  \n",
            "train iteration 4:loss 1.3925743103027344  \n",
            "train iteration 5:loss 2.800800323486328  \n",
            "train iteration 6:loss 3.2741565704345703  \n",
            "train iteration 7:loss 3.6230907440185547  \n",
            "train iteration 8:loss 4.0215253829956055  \n",
            "train iteration 9:loss 4.359362602233887  \n",
            "train iteration 10:loss 4.660229682922363  \n",
            "train iteration 11:loss 4.97007942199707  \n",
            "train iteration 12:loss 5.502326011657715  \n",
            "train iteration 13:loss 5.858702659606934  \n",
            "train iteration 14:loss 6.268390655517578  \n",
            "train iteration 15:loss 8.619297981262207  \n",
            "train iteration 16:loss 9.033985137939453  \n",
            "train iteration 17:loss 9.359209060668945  \n",
            "train iteration 18:loss 9.693368911743164  \n",
            "train iteration 19:loss 10.0401029586792  \n",
            "train iteration 20:loss 10.329610824584961  \n",
            "train iteration 21:loss 10.716140747070312  \n",
            "train iteration 22:loss 11.037483215332031  \n",
            "train iteration 23:loss 11.327872276306152  \n",
            "train iteration 24:loss 15.48025131225586  \n",
            "train iteration 25:loss 15.872528076171875  \n",
            "train iteration 26:loss 16.221017837524414  \n",
            "train iteration 27:loss 19.304142951965332  \n",
            "train iteration 28:loss 19.72811508178711  \n",
            "train iteration 29:loss 20.29177474975586  \n",
            "train iteration 30:loss 22.345952033996582  \n",
            "train iteration 31:loss 22.76137924194336  \n",
            "train iteration 32:loss 23.39463996887207  \n",
            "train iteration 33:loss 23.78882884979248  \n",
            "train iteration 34:loss 24.271531105041504  \n",
            "train iteration 35:loss 24.54300308227539  \n",
            "train iteration 36:loss 24.901450157165527  \n",
            "train iteration 37:loss 25.217205047607422  \n",
            "train iteration 38:loss 25.508953094482422  \n",
            "train iteration 39:loss 25.91224765777588  \n",
            "train iteration 40:loss 26.190120697021484  \n",
            "train iteration 41:loss 26.630998611450195  \n",
            "train iteration 42:loss 27.026494026184082  \n",
            "train iteration 43:loss 27.310901641845703  \n",
            "train iteration 44:loss 29.23835563659668  \n",
            "train iteration 45:loss 29.815241813659668  \n",
            "train iteration 46:loss 30.276376724243164  \n",
            "train iteration 47:loss 30.595298767089844  \n",
            "train iteration 48:loss 30.976016998291016  \n",
            "train iteration 49:loss 31.29262351989746  \n",
            "train iteration 50:loss 33.016690254211426  \n",
            "train iteration 51:loss 33.30638885498047  \n",
            "train iteration 52:loss 35.866637229919434  \n",
            "train iteration 53:loss 36.24386119842529  \n",
            "train iteration 54:loss 36.69691753387451  \n",
            "train iteration 55:loss 36.930134773254395  \n",
            "train iteration 56:loss 37.314510345458984  \n",
            "train iteration 57:loss 37.60474967956543  \n",
            "train iteration 58:loss 38.08308410644531  \n",
            "train iteration 59:loss 38.41412544250488  \n",
            "train iteration 60:loss 38.736544609069824  \n",
            "train iteration 61:loss 39.1093053817749  \n",
            "train iteration 62:loss 39.69320201873779  \n",
            "train iteration 63:loss 40.06360721588135  \n",
            "train iteration 64:loss 40.34151840209961  \n",
            "train iteration 65:loss 40.709625244140625  \n",
            "train iteration 66:loss 41.076022148132324  \n",
            "train iteration 67:loss 41.337931632995605  \n",
            "train iteration 68:loss 41.619927406311035  \n",
            "train iteration 69:loss 41.90493392944336  \n",
            "train iteration 70:loss 42.372469902038574  \n",
            "train iteration 71:loss 42.665382385253906  \n",
            "train iteration 72:loss 42.950406074523926  \n",
            "train iteration 73:loss 43.29075813293457  \n",
            "train iteration 74:loss 43.576239585876465  \n",
            "train iteration 75:loss 43.9533634185791  \n",
            "train iteration 76:loss 44.449249267578125  \n",
            "train iteration 77:loss 44.787607192993164  \n",
            "train iteration 78:loss 45.0417537689209  \n",
            "train iteration 79:loss 45.831454277038574  \n",
            "train iteration 80:loss 48.762596130371094  \n",
            "train iteration 81:loss 49.070552825927734  \n",
            "train iteration 82:loss 49.36825370788574  \n",
            "train iteration 83:loss 49.76297569274902  \n",
            "train iteration 84:loss 50.03954887390137  \n",
            "train iteration 85:loss 50.85939121246338  \n",
            "train iteration 86:loss 53.3998327255249  \n",
            "train iteration 87:loss 53.75107669830322  \n",
            "train iteration 88:loss 54.11033916473389  \n",
            "train iteration 89:loss 54.39181709289551  \n",
            "train iteration 90:loss 57.14369773864746  \n",
            "train iteration 91:loss 57.41368293762207  \n",
            "train iteration 92:loss 57.72693634033203  \n",
            "train iteration 93:loss 58.30444526672363  \n",
            "train iteration 94:loss 58.675246238708496  \n",
            "train iteration 95:loss 59.05745220184326  \n",
            "train iteration 96:loss 59.336660385131836  \n",
            "train iteration 97:loss 59.73166084289551  \n",
            "train iteration 98:loss 60.01668453216553  \n",
            "train iteration 99:loss 60.38059425354004  \n",
            "train iteration 100:loss 60.60859585553408  \n",
            "train Loss: 0.0095 Acc: 0.9981\n",
            "val iteration 1:loss 14.627725601196289  \n",
            "val iteration 2:loss 48.198869705200195  \n",
            "val iteration 3:loss 75.01947593688965  \n",
            "val iteration 4:loss 123.98442649841309  \n",
            "val iteration 5:loss 143.87826919555664  \n",
            "val iteration 6:loss 168.8304500579834  \n",
            "val iteration 7:loss 220.10870170593262  \n",
            "val iteration 8:loss 249.11362838745117  \n",
            "val iteration 9:loss 274.6837463378906  \n",
            "val iteration 10:loss 303.2406768798828  \n",
            "val iteration 11:loss 327.2505989074707  \n",
            "val iteration 12:loss 355.62657928466797  \n",
            "val iteration 13:loss 383.57672691345215  \n",
            "val iteration 14:loss 408.84535026550293  \n",
            "val iteration 15:loss 430.16290855407715  \n",
            "val iteration 16:loss 473.5982265472412  \n",
            "val iteration 17:loss 495.40674209594727  \n",
            "val iteration 18:loss 518.4896068572998  \n",
            "val iteration 19:loss 552.2505702972412  \n",
            "val iteration 20:loss 613.0850658416748  \n",
            "val iteration 21:loss 665.6956233978271  \n",
            "val iteration 22:loss 696.5513877868652  \n",
            "val iteration 23:loss 736.3012619018555  \n",
            "val iteration 24:loss 767.19606590271  \n",
            "val iteration 25:loss 799.4819507598877  \n",
            "val iteration 26:loss 827.7529277801514  \n",
            "val iteration 27:loss 860.8157711029053  \n",
            "val iteration 28:loss 897.6580600738525  \n",
            "val iteration 29:loss 938.4902858734131  \n",
            "val iteration 30:loss 965.5517959594727  \n",
            "val iteration 31:loss 989.6045036315918  \n",
            "val iteration 32:loss 1018.7852725982666  \n",
            "val iteration 33:loss 1047.03560256958  \n",
            "val iteration 34:loss 1093.5943870544434  \n",
            "val iteration 35:loss 1128.644401550293  \n",
            "val iteration 36:loss 1166.885398864746  \n",
            "val iteration 37:loss 1196.9607238769531  \n",
            "val iteration 38:loss 1237.8405799865723  \n",
            "val iteration 39:loss 1261.4365196228027  \n",
            "val iteration 40:loss 1298.125057220459  \n",
            "val iteration 41:loss 1339.3070068359375  \n",
            "val iteration 42:loss 1369.8421611785889  \n",
            "val iteration 43:loss 1410.754861831665  \n",
            "val iteration 44:loss 1441.039987564087  \n",
            "val iteration 45:loss 1464.4394092559814  \n",
            "val iteration 46:loss 1497.1793460845947  \n",
            "val iteration 47:loss 1525.6618690490723  \n",
            "val iteration 48:loss 1554.070634841919  \n",
            "val iteration 49:loss 1583.5914516448975  \n",
            "val iteration 50:loss 1608.9271564483643  \n",
            "val iteration 51:loss 1637.6968021392822  \n",
            "val iteration 52:loss 1666.2956981658936  \n",
            "val iteration 53:loss 1714.4108028411865  \n",
            "val iteration 54:loss 1744.7654666900635  \n",
            "val iteration 55:loss 1798.589792251587  \n",
            "val iteration 56:loss 1798.6322469711304  \n",
            "val Loss: 1.0196 Acc: 0.7466\n",
            "train iteration 1:loss 1.7266359329223633  \n",
            "train iteration 2:loss 2.1029090881347656  \n",
            "train iteration 3:loss 2.384892463684082  \n",
            "train iteration 4:loss 2.6722612380981445  \n",
            "train iteration 5:loss 3.109593391418457  \n",
            "train iteration 6:loss 3.381545066833496  \n",
            "train iteration 7:loss 3.6854400634765625  \n",
            "train iteration 8:loss 4.107379913330078  \n",
            "train iteration 9:loss 4.396486282348633  \n",
            "train iteration 10:loss 4.655658721923828  \n",
            "train iteration 11:loss 5.041069984436035  \n",
            "train iteration 12:loss 5.374258995056152  \n",
            "train iteration 13:loss 5.847460746765137  \n",
            "train iteration 14:loss 6.262773513793945  \n",
            "train iteration 15:loss 6.457371711730957  \n",
            "train iteration 16:loss 6.789803504943848  \n",
            "train iteration 17:loss 7.066821098327637  \n",
            "train iteration 18:loss 7.810022354125977  \n",
            "train iteration 19:loss 8.182966232299805  \n",
            "train iteration 20:loss 8.48068904876709  \n",
            "train iteration 21:loss 8.76516056060791  \n",
            "train iteration 22:loss 9.084671020507812  \n",
            "train iteration 23:loss 9.721882820129395  \n",
            "train iteration 24:loss 9.995777130126953  \n",
            "train iteration 25:loss 10.280464172363281  \n",
            "train iteration 26:loss 10.745356559753418  \n",
            "train iteration 27:loss 11.125455856323242  \n",
            "train iteration 28:loss 11.739112854003906  \n",
            "train iteration 29:loss 12.009124755859375  \n",
            "train iteration 30:loss 12.343355178833008  \n",
            "train iteration 31:loss 12.740313529968262  \n",
            "train iteration 32:loss 13.14806842803955  \n",
            "train iteration 33:loss 13.44702434539795  \n",
            "train iteration 34:loss 14.450761795043945  \n",
            "train iteration 35:loss 14.747037887573242  \n",
            "train iteration 36:loss 15.006341934204102  \n",
            "train iteration 37:loss 15.25452709197998  \n",
            "train iteration 38:loss 18.46870231628418  \n",
            "train iteration 39:loss 18.811942100524902  \n",
            "train iteration 40:loss 19.14320182800293  \n",
            "train iteration 41:loss 19.87439250946045  \n",
            "train iteration 42:loss 20.294166564941406  \n",
            "train iteration 43:loss 20.576324462890625  \n",
            "train iteration 44:loss 20.888432502746582  \n",
            "train iteration 45:loss 21.17134189605713  \n",
            "train iteration 46:loss 22.862869262695312  \n",
            "train iteration 47:loss 23.237958908081055  \n",
            "train iteration 48:loss 23.47008228302002  \n",
            "train iteration 49:loss 23.99587345123291  \n",
            "train iteration 50:loss 24.23734188079834  \n",
            "train iteration 51:loss 24.509597778320312  \n",
            "train iteration 52:loss 24.769423484802246  \n",
            "train iteration 53:loss 25.718360900878906  \n",
            "train iteration 54:loss 28.385842323303223  \n",
            "train iteration 55:loss 28.66859245300293  \n",
            "train iteration 56:loss 28.970752716064453  \n",
            "train iteration 57:loss 32.369967460632324  \n",
            "train iteration 58:loss 32.69502925872803  \n",
            "train iteration 59:loss 33.137248039245605  \n",
            "train iteration 60:loss 33.46097373962402  \n",
            "train iteration 61:loss 33.72583484649658  \n",
            "train iteration 62:loss 34.020036697387695  \n",
            "train iteration 63:loss 34.366844177246094  \n",
            "train iteration 64:loss 34.77622699737549  \n",
            "train iteration 65:loss 35.09610939025879  \n",
            "train iteration 66:loss 35.5523042678833  \n",
            "train iteration 67:loss 36.07557678222656  \n",
            "train iteration 68:loss 36.35596561431885  \n",
            "train iteration 69:loss 36.786953926086426  \n",
            "train iteration 70:loss 37.102033615112305  \n",
            "train iteration 71:loss 37.36220455169678  \n",
            "train iteration 72:loss 37.620901107788086  \n",
            "train iteration 73:loss 37.93600845336914  \n",
            "train iteration 74:loss 38.46321678161621  \n",
            "train iteration 75:loss 38.908552169799805  \n",
            "train iteration 76:loss 39.16636657714844  \n",
            "train iteration 77:loss 41.229698181152344  \n",
            "train iteration 78:loss 41.61476516723633  \n",
            "train iteration 79:loss 41.90405559539795  \n",
            "train iteration 80:loss 42.231743812561035  \n",
            "train iteration 81:loss 42.62925052642822  \n",
            "train iteration 82:loss 42.98494243621826  \n",
            "train iteration 83:loss 43.41965961456299  \n",
            "train iteration 84:loss 43.69195556640625  \n",
            "train iteration 85:loss 44.0816535949707  \n",
            "train iteration 86:loss 44.39452934265137  \n",
            "train iteration 87:loss 46.49961185455322  \n",
            "train iteration 88:loss 46.78601932525635  \n",
            "train iteration 89:loss 47.117676734924316  \n",
            "train iteration 90:loss 47.50031661987305  \n",
            "train iteration 91:loss 47.79459285736084  \n",
            "train iteration 92:loss 51.96532440185547  \n",
            "train iteration 93:loss 52.34628105163574  \n",
            "train iteration 94:loss 52.64230251312256  \n",
            "train iteration 95:loss 52.908430099487305  \n",
            "train iteration 96:loss 53.636558532714844  \n",
            "train iteration 97:loss 53.987467765808105  \n",
            "train iteration 98:loss 54.25238800048828  \n",
            "train iteration 99:loss 54.59824180603027  \n",
            "train iteration 100:loss 54.86631393432617  \n",
            "train Loss: 0.0086 Acc: 0.9986\n",
            "val iteration 1:loss 24.34581756591797  \n",
            "val iteration 2:loss 35.28804016113281  \n",
            "val iteration 3:loss 61.973365783691406  \n",
            "val iteration 4:loss 87.74576950073242  \n",
            "val iteration 5:loss 117.06393432617188  \n",
            "val iteration 6:loss 147.88935089111328  \n",
            "val iteration 7:loss 183.67811965942383  \n",
            "val iteration 8:loss 208.9798469543457  \n",
            "val iteration 9:loss 269.1295356750488  \n",
            "val iteration 10:loss 302.4980583190918  \n",
            "val iteration 11:loss 349.3903465270996  \n",
            "val iteration 12:loss 393.8824043273926  \n",
            "val iteration 13:loss 424.94576263427734  \n",
            "val iteration 14:loss 484.1290969848633  \n",
            "val iteration 15:loss 521.9730949401855  \n",
            "val iteration 16:loss 549.7375068664551  \n",
            "val iteration 17:loss 584.8590354919434  \n",
            "val iteration 18:loss 611.5605201721191  \n",
            "val iteration 19:loss 630.9066333770752  \n",
            "val iteration 20:loss 650.3735485076904  \n",
            "val iteration 21:loss 660.9291591644287  \n",
            "val iteration 22:loss 692.0339584350586  \n",
            "val iteration 23:loss 729.2753143310547  \n",
            "val iteration 24:loss 758.4268455505371  \n",
            "val iteration 25:loss 789.6298999786377  \n",
            "val iteration 26:loss 830.2479190826416  \n",
            "val iteration 27:loss 861.9711627960205  \n",
            "val iteration 28:loss 879.5939083099365  \n",
            "val iteration 29:loss 903.5315246582031  \n",
            "val iteration 30:loss 943.8326950073242  \n",
            "val iteration 31:loss 977.3311157226562  \n",
            "val iteration 32:loss 1027.6697387695312  \n",
            "val iteration 33:loss 1056.316707611084  \n",
            "val iteration 34:loss 1092.603359222412  \n",
            "val iteration 35:loss 1114.3350677490234  \n",
            "val iteration 36:loss 1136.801160812378  \n",
            "val iteration 37:loss 1192.293909072876  \n",
            "val iteration 38:loss 1217.4006233215332  \n",
            "val iteration 39:loss 1234.4927406311035  \n",
            "val iteration 40:loss 1271.8033256530762  \n",
            "val iteration 41:loss 1304.3662910461426  \n",
            "val iteration 42:loss 1355.1686782836914  \n",
            "val iteration 43:loss 1370.1265811920166  \n",
            "val iteration 44:loss 1414.9706363677979  \n",
            "val iteration 45:loss 1474.2547054290771  \n",
            "val iteration 46:loss 1511.757131576538  \n",
            "val iteration 47:loss 1545.6310405731201  \n",
            "val iteration 48:loss 1586.9367046356201  \n",
            "val iteration 49:loss 1613.1522541046143  \n",
            "val iteration 50:loss 1655.9446506500244  \n",
            "val iteration 51:loss 1688.3081493377686  \n",
            "val iteration 52:loss 1717.9193058013916  \n",
            "val iteration 53:loss 1737.5553951263428  \n",
            "val iteration 54:loss 1766.0289211273193  \n",
            "val iteration 55:loss 1806.277093887329  \n",
            "val iteration 56:loss 1806.944224357605  \n",
            "val Loss: 1.0243 Acc: 0.7528\n",
            "train iteration 1:loss 0.31254100799560547  \n",
            "train iteration 2:loss 0.7089939117431641  \n",
            "train iteration 3:loss 1.094935417175293  \n",
            "train iteration 4:loss 1.4312610626220703  \n",
            "train iteration 5:loss 1.7108402252197266  \n",
            "train iteration 6:loss 2.1379966735839844  \n",
            "train iteration 7:loss 2.3559064865112305  \n",
            "train iteration 8:loss 2.776622772216797  \n",
            "train iteration 9:loss 3.8578853607177734  \n",
            "train iteration 10:loss 4.13008975982666  \n",
            "train iteration 11:loss 4.612118721008301  \n",
            "train iteration 12:loss 4.835788726806641  \n",
            "train iteration 13:loss 5.448215484619141  \n",
            "train iteration 14:loss 5.830513000488281  \n",
            "train iteration 15:loss 6.283576011657715  \n",
            "train iteration 16:loss 6.672890663146973  \n",
            "train iteration 17:loss 6.941769599914551  \n",
            "train iteration 18:loss 7.219654083251953  \n",
            "train iteration 19:loss 7.546134948730469  \n",
            "train iteration 20:loss 7.790558815002441  \n",
            "train iteration 21:loss 8.785149574279785  \n",
            "train iteration 22:loss 9.042131423950195  \n",
            "train iteration 23:loss 9.288774490356445  \n",
            "train iteration 24:loss 9.539355278015137  \n",
            "train iteration 25:loss 9.919930458068848  \n",
            "train iteration 26:loss 10.371415138244629  \n",
            "train iteration 27:loss 10.716941833496094  \n",
            "train iteration 28:loss 10.999454498291016  \n",
            "train iteration 29:loss 11.327240943908691  \n",
            "train iteration 30:loss 12.637428283691406  \n",
            "train iteration 31:loss 12.974651336669922  \n",
            "train iteration 32:loss 13.322186470031738  \n",
            "train iteration 33:loss 13.64690113067627  \n",
            "train iteration 34:loss 14.120893478393555  \n",
            "train iteration 35:loss 14.485031127929688  \n",
            "train iteration 36:loss 14.947150230407715  \n",
            "train iteration 37:loss 15.267040252685547  \n",
            "train iteration 38:loss 15.51082706451416  \n",
            "train iteration 39:loss 15.844584465026855  \n",
            "train iteration 40:loss 17.13982391357422  \n",
            "train iteration 41:loss 17.483580589294434  \n",
            "train iteration 42:loss 17.88464641571045  \n",
            "train iteration 43:loss 18.18772029876709  \n",
            "train iteration 44:loss 19.985605239868164  \n",
            "train iteration 45:loss 20.34920883178711  \n",
            "train iteration 46:loss 20.774259567260742  \n",
            "train iteration 47:loss 21.62916088104248  \n",
            "train iteration 48:loss 23.982151985168457  \n",
            "train iteration 49:loss 24.274494171142578  \n",
            "train iteration 50:loss 24.51630401611328  \n",
            "train iteration 51:loss 24.818730354309082  \n",
            "train iteration 52:loss 25.132155418395996  \n",
            "train iteration 53:loss 25.421247482299805  \n",
            "train iteration 54:loss 25.74117088317871  \n",
            "train iteration 55:loss 26.08665943145752  \n",
            "train iteration 56:loss 26.384458541870117  \n",
            "train iteration 57:loss 26.70954418182373  \n",
            "train iteration 58:loss 28.916284561157227  \n",
            "train iteration 59:loss 29.308693885803223  \n",
            "train iteration 60:loss 29.7791166305542  \n",
            "train iteration 61:loss 30.536813735961914  \n",
            "train iteration 62:loss 30.925504684448242  \n",
            "train iteration 63:loss 31.318157196044922  \n",
            "train iteration 64:loss 31.58363437652588  \n",
            "train iteration 65:loss 31.89854621887207  \n",
            "train iteration 66:loss 32.14394760131836  \n",
            "train iteration 67:loss 32.42692470550537  \n",
            "train iteration 68:loss 32.73943901062012  \n",
            "train iteration 69:loss 32.951537132263184  \n",
            "train iteration 70:loss 33.29873466491699  \n",
            "train iteration 71:loss 33.568288803100586  \n",
            "train iteration 72:loss 33.888548851013184  \n",
            "train iteration 73:loss 34.256577491760254  \n",
            "train iteration 74:loss 34.86950397491455  \n",
            "train iteration 75:loss 35.14897155761719  \n",
            "train iteration 76:loss 35.42216873168945  \n",
            "train iteration 77:loss 35.788458824157715  \n",
            "train iteration 78:loss 36.155181884765625  \n",
            "train iteration 79:loss 36.47385501861572  \n",
            "train iteration 80:loss 36.776336669921875  \n",
            "train iteration 81:loss 37.03041648864746  \n",
            "train iteration 82:loss 37.352599143981934  \n",
            "train iteration 83:loss 37.72012805938721  \n",
            "train iteration 84:loss 38.07245922088623  \n",
            "train iteration 85:loss 38.37508487701416  \n",
            "train iteration 86:loss 40.476372718811035  \n",
            "train iteration 87:loss 40.774953842163086  \n",
            "train iteration 88:loss 41.34740924835205  \n",
            "train iteration 89:loss 41.693726539611816  \n",
            "train iteration 90:loss 43.17216682434082  \n",
            "train iteration 91:loss 43.483421325683594  \n",
            "train iteration 92:loss 43.87834930419922  \n",
            "train iteration 93:loss 44.15052127838135  \n",
            "train iteration 94:loss 44.43632411956787  \n",
            "train iteration 95:loss 44.6850700378418  \n",
            "train iteration 96:loss 45.14223003387451  \n",
            "train iteration 97:loss 45.44594669342041  \n",
            "train iteration 98:loss 45.72768592834473  \n",
            "train iteration 99:loss 48.293710708618164  \n",
            "train iteration 100:loss 48.51952266134322  \n",
            "train Loss: 0.0076 Acc: 0.9987\n",
            "val iteration 1:loss 34.645835876464844  \n",
            "val iteration 2:loss 85.4283561706543  \n",
            "val iteration 3:loss 125.08316421508789  \n",
            "val iteration 4:loss 157.6120262145996  \n",
            "val iteration 5:loss 193.42842483520508  \n",
            "val iteration 6:loss 232.51429748535156  \n",
            "val iteration 7:loss 261.5653648376465  \n",
            "val iteration 8:loss 315.29672622680664  \n",
            "val iteration 9:loss 341.0095748901367  \n",
            "val iteration 10:loss 360.77249336242676  \n",
            "val iteration 11:loss 384.5518550872803  \n",
            "val iteration 12:loss 403.1361560821533  \n",
            "val iteration 13:loss 446.9128360748291  \n",
            "val iteration 14:loss 474.9841709136963  \n",
            "val iteration 15:loss 512.7541522979736  \n",
            "val iteration 16:loss 553.0041942596436  \n",
            "val iteration 17:loss 598.4122867584229  \n",
            "val iteration 18:loss 625.5064601898193  \n",
            "val iteration 19:loss 673.9586772918701  \n",
            "val iteration 20:loss 704.4369430541992  \n",
            "val iteration 21:loss 726.9277839660645  \n",
            "val iteration 22:loss 752.2441596984863  \n",
            "val iteration 23:loss 779.846357345581  \n",
            "val iteration 24:loss 817.7520809173584  \n",
            "val iteration 25:loss 839.247968673706  \n",
            "val iteration 26:loss 851.27028465271  \n",
            "val iteration 27:loss 868.512809753418  \n",
            "val iteration 28:loss 908.7085952758789  \n",
            "val iteration 29:loss 944.7932891845703  \n",
            "val iteration 30:loss 979.1591262817383  \n",
            "val iteration 31:loss 1020.5618743896484  \n",
            "val iteration 32:loss 1059.0304641723633  \n",
            "val iteration 33:loss 1080.0259017944336  \n",
            "val iteration 34:loss 1110.4875030517578  \n",
            "val iteration 35:loss 1123.4157733917236  \n",
            "val iteration 36:loss 1152.5517196655273  \n",
            "val iteration 37:loss 1180.1408138275146  \n",
            "val iteration 38:loss 1205.4745082855225  \n",
            "val iteration 39:loss 1239.556978225708  \n",
            "val iteration 40:loss 1268.0104942321777  \n",
            "val iteration 41:loss 1308.2194366455078  \n",
            "val iteration 42:loss 1342.539924621582  \n",
            "val iteration 43:loss 1374.8106079101562  \n",
            "val iteration 44:loss 1410.664894104004  \n",
            "val iteration 45:loss 1455.5475387573242  \n",
            "val iteration 46:loss 1492.7529640197754  \n",
            "val iteration 47:loss 1520.7514934539795  \n",
            "val iteration 48:loss 1541.4033508300781  \n",
            "val iteration 49:loss 1594.4858474731445  \n",
            "val iteration 50:loss 1618.1132793426514  \n",
            "val iteration 51:loss 1642.2319164276123  \n",
            "val iteration 52:loss 1668.7309436798096  \n",
            "val iteration 53:loss 1703.9086475372314  \n",
            "val iteration 54:loss 1754.7549800872803  \n",
            "val iteration 55:loss 1806.0395107269287  \n",
            "val iteration 56:loss 1807.198221206665  \n",
            "val Loss: 1.0245 Acc: 0.7489\n",
            "train iteration 1:loss 0.5426206588745117  \n",
            "train iteration 2:loss 0.7909021377563477  \n",
            "train iteration 3:loss 1.0723237991333008  \n",
            "train iteration 4:loss 1.360361099243164  \n",
            "train iteration 5:loss 3.191059112548828  \n",
            "train iteration 6:loss 3.4599733352661133  \n",
            "train iteration 7:loss 3.7164430618286133  \n",
            "train iteration 8:loss 3.984177589416504  \n",
            "train iteration 9:loss 4.340076446533203  \n",
            "train iteration 10:loss 5.597936630249023  \n",
            "train iteration 11:loss 5.838404655456543  \n",
            "train iteration 12:loss 6.0756120681762695  \n",
            "train iteration 13:loss 6.264925003051758  \n",
            "train iteration 14:loss 6.552607536315918  \n",
            "train iteration 15:loss 6.919866561889648  \n",
            "train iteration 16:loss 7.149662017822266  \n",
            "train iteration 17:loss 7.807360649108887  \n",
            "train iteration 18:loss 8.132607460021973  \n",
            "train iteration 19:loss 8.488752365112305  \n",
            "train iteration 20:loss 8.78691577911377  \n",
            "train iteration 21:loss 10.735772132873535  \n",
            "train iteration 22:loss 11.106680870056152  \n",
            "train iteration 23:loss 11.594736099243164  \n",
            "train iteration 24:loss 11.895545959472656  \n",
            "train iteration 25:loss 12.1264066696167  \n",
            "train iteration 26:loss 12.375494003295898  \n",
            "train iteration 27:loss 14.055277824401855  \n",
            "train iteration 28:loss 14.331562995910645  \n",
            "train iteration 29:loss 14.565234184265137  \n",
            "train iteration 30:loss 14.792970657348633  \n",
            "train iteration 31:loss 15.110334396362305  \n",
            "train iteration 32:loss 16.771902084350586  \n",
            "train iteration 33:loss 17.213521003723145  \n",
            "train iteration 34:loss 17.402474403381348  \n",
            "train iteration 35:loss 17.81693935394287  \n",
            "train iteration 36:loss 18.255277633666992  \n",
            "train iteration 37:loss 18.573856353759766  \n",
            "train iteration 38:loss 19.131667137145996  \n",
            "train iteration 39:loss 19.465500831604004  \n",
            "train iteration 40:loss 19.961021423339844  \n",
            "train iteration 41:loss 20.281095504760742  \n",
            "train iteration 42:loss 20.52293586730957  \n",
            "train iteration 43:loss 20.846840858459473  \n",
            "train iteration 44:loss 21.201913833618164  \n",
            "train iteration 45:loss 21.80528736114502  \n",
            "train iteration 46:loss 22.120004653930664  \n",
            "train iteration 47:loss 23.72596549987793  \n",
            "train iteration 48:loss 24.145377159118652  \n",
            "train iteration 49:loss 24.77410888671875  \n",
            "train iteration 50:loss 25.149699211120605  \n",
            "train iteration 51:loss 25.5202054977417  \n",
            "train iteration 52:loss 27.73675537109375  \n",
            "train iteration 53:loss 28.109904289245605  \n",
            "train iteration 54:loss 28.486756324768066  \n",
            "train iteration 55:loss 28.762226104736328  \n",
            "train iteration 56:loss 29.0955228805542  \n",
            "train iteration 57:loss 29.382930755615234  \n",
            "train iteration 58:loss 29.624972343444824  \n",
            "train iteration 59:loss 30.0325870513916  \n",
            "train iteration 60:loss 30.372431755065918  \n",
            "train iteration 61:loss 30.730796813964844  \n",
            "train iteration 62:loss 31.056049346923828  \n",
            "train iteration 63:loss 31.460761070251465  \n",
            "train iteration 64:loss 31.832627296447754  \n",
            "train iteration 65:loss 32.57646560668945  \n",
            "train iteration 66:loss 32.92474174499512  \n",
            "train iteration 67:loss 34.9291467666626  \n",
            "train iteration 68:loss 38.40244197845459  \n",
            "train iteration 69:loss 38.70986366271973  \n",
            "train iteration 70:loss 39.00890922546387  \n",
            "train iteration 71:loss 39.31103324890137  \n",
            "train iteration 72:loss 39.766902923583984  \n",
            "train iteration 73:loss 40.06415557861328  \n",
            "train iteration 74:loss 40.37172031402588  \n",
            "train iteration 75:loss 40.66184902191162  \n",
            "train iteration 76:loss 41.035125732421875  \n",
            "train iteration 77:loss 41.33310031890869  \n",
            "train iteration 78:loss 41.54485034942627  \n",
            "train iteration 79:loss 41.77959632873535  \n",
            "train iteration 80:loss 42.09862232208252  \n",
            "train iteration 81:loss 42.403971672058105  \n",
            "train iteration 82:loss 42.69960975646973  \n",
            "train iteration 83:loss 43.08769130706787  \n",
            "train iteration 84:loss 43.38627338409424  \n",
            "train iteration 85:loss 43.77135181427002  \n",
            "train iteration 86:loss 44.07861137390137  \n",
            "train iteration 87:loss 44.48153114318848  \n",
            "train iteration 88:loss 44.791786193847656  \n",
            "train iteration 89:loss 45.11692142486572  \n",
            "train iteration 90:loss 46.9199857711792  \n",
            "train iteration 91:loss 47.3594913482666  \n",
            "train iteration 92:loss 47.61241817474365  \n",
            "train iteration 93:loss 47.90136528015137  \n",
            "train iteration 94:loss 48.32180690765381  \n",
            "train iteration 95:loss 48.54144859313965  \n",
            "train iteration 96:loss 48.819703102111816  \n",
            "train iteration 97:loss 49.110246658325195  \n",
            "train iteration 98:loss 49.32207107543945  \n",
            "train iteration 99:loss 49.57338237762451  \n",
            "train iteration 100:loss 49.771822925657034  \n",
            "train Loss: 0.0078 Acc: 0.9984\n",
            "val iteration 1:loss 24.568113327026367  \n",
            "val iteration 2:loss 48.80513381958008  \n",
            "val iteration 3:loss 88.49027252197266  \n",
            "val iteration 4:loss 120.40520477294922  \n",
            "val iteration 5:loss 140.30768585205078  \n",
            "val iteration 6:loss 178.58432006835938  \n",
            "val iteration 7:loss 198.48330116271973  \n",
            "val iteration 8:loss 239.44942665100098  \n",
            "val iteration 9:loss 283.6047077178955  \n",
            "val iteration 10:loss 336.231897354126  \n",
            "val iteration 11:loss 365.621000289917  \n",
            "val iteration 12:loss 394.1887722015381  \n",
            "val iteration 13:loss 440.3717441558838  \n",
            "val iteration 14:loss 478.4908199310303  \n",
            "val iteration 15:loss 510.5131244659424  \n",
            "val iteration 16:loss 532.640323638916  \n",
            "val iteration 17:loss 560.8547019958496  \n",
            "val iteration 18:loss 598.4307441711426  \n",
            "val iteration 19:loss 630.1385116577148  \n",
            "val iteration 20:loss 660.8339080810547  \n",
            "val iteration 21:loss 671.3711967468262  \n",
            "val iteration 22:loss 693.5543746948242  \n",
            "val iteration 23:loss 731.8071823120117  \n",
            "val iteration 24:loss 766.6761322021484  \n",
            "val iteration 25:loss 797.4765701293945  \n",
            "val iteration 26:loss 819.058084487915  \n",
            "val iteration 27:loss 850.8346614837646  \n",
            "val iteration 28:loss 891.5138645172119  \n",
            "val iteration 29:loss 942.817949295044  \n",
            "val iteration 30:loss 972.5900459289551  \n",
            "val iteration 31:loss 1012.2734298706055  \n",
            "val iteration 32:loss 1037.574161529541  \n",
            "val iteration 33:loss 1078.5357704162598  \n",
            "val iteration 34:loss 1102.346046447754  \n",
            "val iteration 35:loss 1140.9535064697266  \n",
            "val iteration 36:loss 1164.1918640136719  \n",
            "val iteration 37:loss 1200.200439453125  \n",
            "val iteration 38:loss 1218.457914352417  \n",
            "val iteration 39:loss 1243.821231842041  \n",
            "val iteration 40:loss 1279.5687675476074  \n",
            "val iteration 41:loss 1347.2340965270996  \n",
            "val iteration 42:loss 1375.345069885254  \n",
            "val iteration 43:loss 1395.6212005615234  \n",
            "val iteration 44:loss 1450.345531463623  \n",
            "val iteration 45:loss 1476.3740940093994  \n",
            "val iteration 46:loss 1498.5455722808838  \n",
            "val iteration 47:loss 1520.4440593719482  \n",
            "val iteration 48:loss 1538.9841594696045  \n",
            "val iteration 49:loss 1569.7961139678955  \n",
            "val iteration 50:loss 1610.6378726959229  \n",
            "val iteration 51:loss 1639.5753345489502  \n",
            "val iteration 52:loss 1670.538682937622  \n",
            "val iteration 53:loss 1701.6369647979736  \n",
            "val iteration 54:loss 1731.8382625579834  \n",
            "val iteration 55:loss 1771.6912441253662  \n",
            "val iteration 56:loss 1781.7852067947388  \n",
            "val Loss: 1.0101 Acc: 0.7466\n",
            "Training complete in 19m 18s\n",
            "Best val Acc: 0.752834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4uo1iELLSuW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ec38529-b1ba-4082-e03b-5f62c329557f"
      },
      "source": [
        "print('Testing began....')\n",
        "best_model = cont.loadmodel(\"19model.pth\")\n",
        "google_net = models.googlenet(pretrained=True)\n",
        "google_net.to(cont.device)\n",
        "#print(best_model)\n",
        "google_net.load_state_dict(best_model)\n",
        "cont.predict(google_net)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing began....\n",
            "Iteration 1\n",
            "Iteration 2\n",
            "Iteration 3\n",
            "Iteration 4\n",
            "Iteration 5\n",
            "Iteration 6\n",
            "Iteration 7\n",
            "Iteration 8\n",
            "Iteration 9\n",
            "Iteration 10\n",
            "Iteration 11\n",
            "Iteration 12\n",
            "Iteration 13\n",
            "Iteration 14\n",
            "Iteration 15\n",
            "Iteration 16\n",
            "Iteration 17\n",
            "Iteration 18\n",
            "Iteration 19\n",
            "Iteration 20\n",
            "Iteration 21\n",
            "Iteration 22\n",
            "Iteration 23\n",
            "Iteration 24\n",
            "Iteration 25\n",
            "Iteration 26\n",
            "Iteration 27\n",
            "Iteration 28\n",
            "Iteration 29\n",
            "Iteration 30\n",
            "Iteration 31\n",
            "Iteration 32\n",
            "Iteration 33\n",
            "Iteration 34\n",
            "Iteration 35\n",
            "Iteration 36\n",
            "Iteration 37\n",
            "Iteration 38\n",
            "Iteration 39\n",
            "Iteration 40\n",
            "Iteration 41\n",
            "Iteration 42\n",
            "Iteration 43\n",
            "Iteration 44\n",
            "Iteration 45\n",
            "Iteration 46\n",
            "Iteration 47\n",
            "Iteration 48\n",
            "Iteration 49\n",
            "Iteration 50\n",
            "Iteration 51\n",
            "Iteration 52\n",
            "Iteration 53\n",
            "Iteration 54\n",
            "Iteration 55\n",
            "Iteration 56\n",
            "Iteration 57\n",
            "Iteration 58\n",
            "Iteration 59\n",
            "Iteration 60\n",
            "Iteration 61\n",
            "Iteration 62\n",
            "Iteration 63\n",
            "Iteration 64\n",
            "Iteration 65\n",
            "Iteration 66\n",
            "Iteration 67\n",
            "Iteration 68\n",
            "Iteration 69\n",
            "Iteration 70\n",
            "Iteration 71\n",
            "Iteration 72\n",
            "Iteration 73\n",
            "Iteration 74\n",
            "Iteration 75\n",
            "Iteration 76\n",
            "Iteration 77\n",
            "Iteration 78\n",
            "Iteration 79\n",
            "Iteration 80\n",
            "Iteration 81\n",
            "Iteration 82\n",
            "Iteration 83\n",
            "Iteration 84\n",
            "Iteration 85\n",
            "Iteration 86\n",
            "Iteration 87\n",
            "Iteration 88\n",
            "Iteration 89\n",
            "Iteration 90\n",
            "Iteration 91\n",
            "Iteration 92\n",
            "Iteration 93\n",
            "Iteration 94\n",
            "Iteration 95\n",
            "Iteration 96\n",
            "Iteration 97\n",
            "Iteration 98\n",
            "Iteration 99\n",
            "Iteration 100\n",
            "Iteration 101\n",
            "Iteration 102\n",
            "Iteration 103\n",
            "Iteration 104\n",
            "Iteration 105\n",
            "Iteration 106\n",
            "Iteration 107\n",
            "Iteration 108\n",
            "Iteration 109\n",
            "Iteration 110\n",
            "Iteration 111\n",
            "Iteration 112\n",
            "Iteration 113\n",
            "Iteration 114\n",
            "Iteration 115\n",
            "Iteration 116\n",
            "Iteration 117\n",
            "Iteration 118\n",
            "Iteration 119\n",
            "Iteration 120\n",
            "Iteration 121\n",
            "Iteration 122\n",
            "Iteration 123\n",
            "Iteration 124\n",
            "Iteration 125\n",
            "Iteration 126\n",
            "Iteration 127\n",
            "Iteration 128\n",
            "Iteration 129\n",
            "Iteration 130\n",
            "Iteration 131\n",
            "Iteration 132\n",
            "Iteration 133\n",
            "Iteration 134\n",
            "Iteration 135\n",
            "Iteration 136\n",
            "Iteration 137\n",
            "Iteration 138\n",
            "Iteration 139\n",
            "Iteration 140\n",
            "Iteration 141\n",
            "Iteration 142\n",
            "Iteration 143\n",
            "Iteration 144\n",
            "Iteration 145\n",
            "Iteration 146\n",
            "Iteration 147\n",
            "Iteration 148\n",
            "Iteration 149\n",
            "Iteration 150\n",
            "Iteration 151\n",
            "Iteration 152\n",
            "Iteration 153\n",
            "Iteration 154\n",
            "Iteration 155\n",
            "Iteration 156\n",
            "Iteration 157\n",
            "Iteration 158\n",
            "Iteration 159\n",
            "Iteration 160\n",
            "Iteration 161\n",
            "Iteration 162\n",
            "Iteration 163\n",
            "Iteration 164\n",
            "Iteration 165\n",
            "Iteration 166\n",
            "Iteration 167\n",
            "Iteration 168\n",
            "Iteration 169\n",
            "Iteration 170\n",
            "Iteration 171\n",
            "Iteration 172\n",
            "Iteration 173\n",
            "Iteration 174\n",
            "Iteration 175\n",
            "Iteration 176\n",
            "Iteration 177\n",
            "Iteration 178\n",
            "Iteration 179\n",
            "Iteration 180\n",
            "Iteration 181\n",
            "Iteration 182\n",
            "Iteration 183\n",
            "Iteration 184\n",
            "Iteration 185\n",
            "Iteration 186\n",
            "Iteration 187\n",
            "Iteration 188\n",
            "Iteration 189\n",
            "Iteration 190\n",
            "Iteration 191\n",
            "Iteration 192\n",
            "Iteration 193\n",
            "Iteration 194\n",
            "Iteration 195\n",
            "Iteration 196\n",
            "Iteration 197\n",
            "Iteration 198\n",
            "Iteration 199\n",
            "Iteration 200\n",
            "Iteration 201\n",
            "Iteration 202\n",
            "Iteration 203\n",
            "Iteration 204\n",
            "Iteration 205\n",
            "Iteration 206\n",
            "Iteration 207\n",
            "Iteration 208\n",
            "Iteration 209\n",
            "Iteration 210\n",
            "Iteration 211\n",
            "Iteration 212\n",
            "Iteration 213\n",
            "Iteration 214\n",
            "Iteration 215\n",
            "Iteration 216\n",
            "Iteration 217\n",
            "Iteration 218\n",
            "Iteration 219\n",
            "Iteration 220\n",
            "Iteration 221\n",
            "Iteration 222\n",
            "Iteration 223\n",
            "Iteration 224\n",
            "Iteration 225\n",
            "Iteration 226\n",
            "Iteration 227\n",
            "Iteration 228\n",
            "Iteration 229\n",
            "Iteration 230\n",
            "Iteration 231\n",
            "Iteration 232\n",
            "Iteration 233\n",
            "Iteration 234\n",
            "Iteration 235\n",
            "Iteration 236\n",
            "Iteration 237\n",
            "Iteration 238\n",
            "Iteration 239\n",
            "Iteration 240\n",
            "Iteration 241\n",
            "Iteration 242\n",
            "Iteration 243\n",
            "Iteration 244\n",
            "Iteration 245\n",
            "Iteration 246\n",
            "Iteration 247\n",
            "Iteration 248\n",
            "Iteration 249\n",
            "Iteration 250\n",
            "Iteration 251\n",
            "Iteration 252\n",
            "Accuracy of test data : 0.7150851884094018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVq46iSkMHuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYnLUlHsMJDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odpAarkjMier",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}