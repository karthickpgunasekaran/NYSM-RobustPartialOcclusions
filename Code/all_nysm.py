# -*- coding: utf-8 -*-
"""All_nysm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13uyMU8z1tAXaYadoSrTCi_cQsdSqVMAd
"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline

#dataloader file path
import sys
sys.path.insert(1, '/content/drive/My Drive/Colab Notebooks')

import time
import torch
import torch.nn as nn
import DataLoader as dataloader
from torch.autograd import Variable
from torchvision import models, transforms
from torch.utils.data import DataLoader as tdataloader

import pandas as pd
import numpy as np
import copy

#track loss and accuracies

num_epochs = 30
batch_size = 32
class_labels=196 #Subject to change
loader_works = 1 #multi-process data loading,#loader worker processes.


images_dir='/content/drive/My Drive/stanford_split/train_all/'
images_dir_test='/content/drive/My Drive/stanford_split/test_all/'

# images_dir='/content/drive/My Drive/25/train/'
# images_dir_test='/content/drive/My Drive/25/test/'


USE_GPU = True
if USE_GPU and torch.cuda.is_available():
 device = torch.device('cuda')
else:
 device = torch.device ('cpu')
print('device running on:', device)


check_point_path = '/content/drive/My Drive/stanford_split/results/checkpoint_st_split_40'
def checkpoint(model, best_loss, epoch, learning_rate):
    state = { 'model': model, 'best_loss': best_loss, 'epoch': epoch, 'rng_state': torch.get_rng_state(),'LR': learning_rate}
    # print('state',state)
    torch.save(state, check_point_path)

def loadmodel():
  # load best model weights to return
    checkpoint_best = torch.load(check_point_path)
    model = checkpoint_best['model']
    return model

tr_loss_track,val_loss_track,tr_acc_track,val_acc_track,val_acc_history = [],[], [],[],[]

def training():
  mean =  [0.5, 0.5, 0.5]
  std  = [0.5, 0.5, 0.5]

  transform ={'train':transforms.Compose( [ 
                                           transforms.RandomHorizontalFlip(),
                                           transforms.ToTensor(),
                                           transforms.Normalize(mean, std)]), 
              'val': transforms.Compose( [ 
                                          transforms.RandomHorizontalFlip(),
                                          transforms.ToTensor(),
                                          transforms.Normalize(mean, std)])}
  #Loading the dataset with train and val as keys
  #Data divided in training and val using separate script
  datasets = {}
  datasets['train'] = dataloader.ImageDataSet(images_dir,fold='train',transformation=transform['train'])
  datasets['val'] = dataloader.ImageDataSet(images_dir,fold='val',transformation=transform['val'])
  print('datasets',len(datasets['train']))
  print('datasets',len(datasets['val']))

  #Dataloader.util
  dataloaders = {}
  dataloaders['train'] = tdataloader(datasets['train'], batch_size=64, shuffle=True, num_workers=loader_works)
  dataloaders['val'] = tdataloader(datasets['val'], batch_size=batch_size, shuffle=True, num_workers=loader_works)
  print('dataloaders train ',len(dataloaders['train']))
  print('dataloaders valida',len(dataloaders['val']))

  #Defining pretrained model
  # model = models.resnet50(pretrained=True)
  model = models.vgg19(pretrained=True)
  # for param in model.parameters():
  #   param.requires_grad = False

  #Resnet
  # num_features = model.fc.in_features
  # print('model num_features',num_features)
  # model.fc = nn.Linear(num_features, class_labels)
  # model = model.to(device)

  #VGG
  model.classifier[6] = nn.Linear(4096,class_labels)
  model = model.to(device)

  # print(model.classifier)
  # print(model)

  criterian = nn.CrossEntropyLoss()
  learning_rate = 0.01
  # optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)

  #training
  start_time=time.time()
  best_loss = float('inf')
  best_model=None
  best_acc=0.0
  best_model_wts = copy.deepcopy(model.state_dict())

  for epoch in range(1,num_epochs+1):
    print('epoch {}/{}'.format(epoch,num_epochs))   

    for t in ['train','val']:
        num_correct = 0.0
        num_samples =0
        r_loss = 0.0
        running_corrects = 0

        if t == 'train':
          #training mode
          model.train()
        else:
          #evaluate model
          model.eval()

        count=0
        for data in dataloaders[t]:
            count+=1
            # data has three types files, labels and filename
            files,labels,filename = data

            files = Variable(files.to(device)) #to gpu or cpu
            labels = Variable(labels.to(device))
            
            optimizer.zero_grad() #clearning old gradient from last step

            with torch.set_grad_enabled(t == 'train'):
              pred=model(files)
                #loss computation
              loss = criterian(pred,labels)

              _,prediction =torch.max(pred,1)
              
            #backprop gradients at training time
              if t=='train':
                loss.backward()
                optimizer.step()

              # print(t +' iteration {}:loss {}  '.format(count,r_loss))
            # statistics
            r_loss += loss.item() * files.size(0)
            print(t +' iteration {}:loss {}  '.format(count,r_loss))
            running_corrects += torch.sum(prediction == labels.data)
        epoch_loss = r_loss / len(dataloaders[t].dataset)
        epoch_acc = running_corrects.double() / len(dataloaders[t].dataset)

        print('{} Loss: {:.4f} Acc: {:.4f}'.format(t, epoch_loss, epoch_acc))
        
        # print(t +' epoch {}:loss {}  '.format(epoch,r_loss))

        # deep copy the model
        if t == 'val' and epoch_acc > best_acc:
            best_acc = epoch_acc
            best_model_wts = copy.deepcopy(model.state_dict())
            checkpoint(best_model_wts, best_loss, epoch, learning_rate)
        if t == 'val':
            val_acc_history.append(epoch_acc.item())
            val_loss_track.append(epoch_loss)

        if t=='train':
          tr_loss_track.append(epoch_loss)
          tr_acc_track.append(epoch_acc.item())
       
        print()

    time_elapsed = time.time() - start_time
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    #updating best model in checkpoint
   
  return model

print('Training began....')
best_model = training()

# print(best_model)
# print(loadmodel())

def predict(model):
    torch.cuda.empty_cache()
    #For data augmentation
    transform ={'test':transforms.ToTensor()}
    datasets = {}
    datasets = dataloader.ImageDataSet(images_dir_test,fold='test',transformation=transform['test'])
    print('datasets',len(datasets))

    dataloaders = {}
    dataloaders = tdataloader(datasets, batch_size=batch_size, shuffle=True, num_workers=loader_works)
    print('dataloaders',len(dataloaders))
    # print(dataloaders)
    # print(enumerate(dataloaders))
    num_correct=0
    num_samples=0
    count=0
    for  data in dataloaders:
        count+=1
        files, labels,filename = data
        #load the data to GPU / CPU
        image_data = Variable(files.to(device))
        labels = Variable(labels.to(device))
        output = model(image_data)
        # print('output', output)
        _,prediction =torch.max(output.data,1)
        # print('filename',filename,'\nlabels',labels,'\nprediction',prediction)
        num_correct += torch.sum(prediction == labels)
        num_samples += prediction.size(0)
        print('Iteration',count)
    acc = float(num_correct) / num_samples
    print('acc',acc)

"""# New Section"""

print('Testing began....')

#loading best model from the checkpoint
# curr_model = loadmodel()
predict(best_model)

print('tr_loss_track',tr_loss_track)
print('val_loss_track',val_loss_track)
print('tr_acc_track',tr_acc_track)

# print('val_acc_track',val_acc_track)
print('val_acc_history',val_acc_history)

from matplotlib import pyplot as plt
# plt.show()
ax = plt.subplot(111)
plt.plot(tr_loss_track, c='b',label='Training loss')
plt.plot(val_loss_track, c= 'g',label='Validation loss')
plt.title('Loss')
ax.legend()
plt.savefig('/content/drive/My Drive/stanford_split/train_val.png')

from matplotlib import pyplot as plt
plt.plot(val_acc_history, c='b')
plt.xlabel('Num epochs')
plt.ylabel('Validation accuracy')
plt.savefig('/content/drive/My Drive/stanford_split/val_acc.png')

"""# New Section"""

from google.colab import drive
drive.mount('/content/drive')